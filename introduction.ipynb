{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning\n",
    "\n",
    "Machine Learning is a **subset of Artificial Intelligence (AI)** that focuses on teaching computers to **learn patterns from data** and **make predictions or decisions** without being explicitly programmed with fixed rules.\n",
    "\n",
    "üëâ Instead of writing step-by-step instructions, we provide **examples (data)**, and the algorithm learns the hidden relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## Example to Understand ML\n",
    "\n",
    "* Traditional programming:\n",
    "\n",
    "  * Rules (explicitly coded) + Data ‚Üí Output\n",
    "* Machine Learning:\n",
    "\n",
    "  * Data + Output (examples) ‚Üí Algorithm learns rules ‚Üí Predict new output\n",
    "\n",
    "‚ú® Example: Predicting house prices\n",
    "\n",
    "* Input: Size, Location, Number of rooms\n",
    "* Output: House Price\n",
    "* ML learns the mapping function:\n",
    "\n",
    "  $$\n",
    "  Price = f(Size, Location, Rooms)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "1. **Supervised Learning**\n",
    "\n",
    "   * Learn from labeled data (input + correct output given).\n",
    "   * Task: Prediction.\n",
    "   * Examples:\n",
    "\n",
    "     * Regression (predict numbers, e.g., house prices).\n",
    "     * Classification (predict categories, e.g., spam vs not spam).\n",
    "\n",
    "2. **Unsupervised Learning**\n",
    "\n",
    "   * Learn from unlabeled data (only input, no output given).\n",
    "   * Task: Discover patterns.\n",
    "   * Examples:\n",
    "\n",
    "     * Clustering (grouping customers by purchase behavior).\n",
    "     * Dimensionality reduction (compressing features for visualization).\n",
    "\n",
    "3. **Reinforcement Learning**\n",
    "\n",
    "   * Learn by interacting with the environment (trial and error).\n",
    "   * Task: Decision making.\n",
    "   * Example:\n",
    "\n",
    "     * Teaching a robot to walk.\n",
    "     * AlphaGo beating humans in Go.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components of ML\n",
    "\n",
    "1. **Dataset** ‚Üí Collection of examples (features + labels).\n",
    "2. **Model** ‚Üí Mathematical representation that makes predictions.\n",
    "3. **Training** ‚Üí Process of learning patterns (adjusting model parameters).\n",
    "4. **Evaluation** ‚Üí Measuring performance (accuracy, error, etc.).\n",
    "5. **Prediction** ‚Üí Using the trained model on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is ML important?\n",
    "\n",
    "* Handles **large, complex data** humans cannot analyze manually.\n",
    "* **Automates tasks** (spam filtering, recommendation systems, fraud detection).\n",
    "* Improves over time as it sees more data.\n",
    "\n",
    "\n",
    "## Learning approach Variants\n",
    "\n",
    "### Instance-based learning\n",
    "\n",
    "* Learns by **memorizing training examples**.\n",
    "* No explicit model is built.\n",
    "* Prediction is made by comparing a new instance with stored instances.\n",
    "* Uses a **similarity (distance) measure** to find closest examples.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* k-Nearest Neighbors (kNN)\n",
    "* Locally Weighted Regression\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Simple, flexible.\n",
    "* Works well if decision boundary is irregular.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Expensive at prediction time (must compare with many stored examples).\n",
    "* Sensitive to noise and irrelevant features.\n",
    "\n",
    "---\n",
    "\n",
    "### Model-based learning\n",
    "\n",
    "* Learns a **general model** from training data.\n",
    "* The model captures underlying relationships, then is used for prediction.\n",
    "* Parameters are estimated during training.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* Neural Networks\n",
    "* Decision Trees\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Fast prediction once model is trained.\n",
    "* Generalizes well if model is appropriate.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Training can be computationally heavy.\n",
    "* If model is too simple, it underfits; if too complex, it overfits.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Difference**\n",
    "\n",
    "* **Instance-based**: ‚ÄúRemember examples, predict by similarity.‚Äù\n",
    "* **Model-based**: ‚ÄúLearn rules (parameters), predict by applying model.‚Äù\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **List of Machine Learning Algorithms**\n",
    "\n",
    "| **Category**                 | **Sub-type**                 | **Algorithms**                                                                                                                                                                                                                                                                                                               |\n",
    "| ---------------------------- | ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Supervised Learning**      | **Regression**               | Linear Regression, Polynomial Regression, Ridge, Lasso, Elastic Net, SVR, Decision Tree Regression, Random Forest Regression, Gradient Boosting (XGBoost, LightGBM, CatBoost), kNN Regression, Bayesian Regression, Neural Networks                                                                                          |\n",
    "|                              | **Classification**           | Logistic Regression, kNN, SVM, Decision Trees (CART, ID3, C4.5), Random Forest, Gradient Boosting (XGBoost, LightGBM, CatBoost), Naive Bayes (Gaussian, Multinomial, Bernoulli), Perceptron, Multi-layer Perceptrons, Ensemble Methods (Bagging, Stacking, Voting), Probabilistic Graphical Models (Bayesian Networks, CRFs) |\n",
    "| **Unsupervised Learning**    | **Clustering**               | k-Means, Hierarchical Clustering, DBSCAN, OPTICS, Gaussian Mixture Models, Mean-Shift, Spectral Clustering, BIRCH, Affinity Propagation                                                                                                                                                                                      |\n",
    "|                              | **Dimensionality Reduction** | PCA, Kernel PCA, ICA, SVD, Factor Analysis, t-SNE, UMAP, Autoencoders                                                                                                                                                                                                                                                        |\n",
    "|                              | **Association Rules**        | Apriori, Eclat, FP-Growth                                                                                                                                                                                                                                                                                                    |\n",
    "|                              | **Density Estimation**       | KDE, Expectation-Maximization (EM), Hidden Markov Models (unsupervised setting)                                                                                                                                                                                                                                              |\n",
    "| **Semi-Supervised Learning** | ‚Äî                            | Self-training, Co-training, Label Propagation/Spreading, Semi-supervised SVM, Graph-based methods, Semi-supervised Deep Learning (Consistency Regularization, Pseudo-labeling)                                                                                                                                               |\n",
    "| **Reinforcement Learning**   | **Value-based**              | Q-Learning, SARSA, Deep Q-Networks (DQN)                                                                                                                                                                                                                                                                                     |\n",
    "|                              | **Policy-based**             | Policy Gradient (REINFORCE), Actor‚ÄìCritic (A2C, A3C), Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO)                                                                                                                                                                                            |\n",
    "|                              | **Model-based / Advanced**   | DDPG, TD3, SAC, Monte Carlo Tree Search, Multi-agent RL                                                                                                                                                                                                                                                                      |\n",
    "| **Other Methods**            | **Ensemble Methods**         | Bagging, Boosting (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost), Stacking, Blending, Voting Classifier                                                                                                                                                                                                          |\n",
    "|                              | **Probabilistic / Bayesian** | Naive Bayes, Bayesian Networks, Gaussian Processes, HMMs, Markov Random Fields                                                                                                                                                                                                                                               |\n",
    "|                              | **Deep Learning**            | Feedforward NN, CNN, RNN, LSTM, GRU, Transformers (BERT, GPT), Variational Autoencoders (VAE), Generative Adversarial Networks (GANs)                                                                                                                                                                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{grid} 1 1 2 3\n",
    ":gutter: 3\n",
    "\n",
    ":::{grid-item-card} \n",
    ":link: machine_learning/supervised_learning/Linear_Regression/overview\n",
    ":link-type: doc\n",
    ":class-header: bg-grid-header\n",
    ":class-body: grid-center bg-grid-body\n",
    "\n",
    "\n",
    "<span class=\"grid-title\">Linear Regression</span>\n",
    "^^^\n",
    "\n",
    "The branch of mathematics that measures the likelihood of events occurring, ranging from impossible (0) to certain (1)\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
