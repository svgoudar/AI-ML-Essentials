Traceback (most recent call last):
  File "C:\Users\sangouda\AppData\Local\Programs\Python\Python312\Lib\site-packages\jupyter_cache\executors\utils.py", line 58, in single_nb_execution
    executenb(
  File "C:\Users\sangouda\AppData\Local\Programs\Python\Python312\Lib\site-packages\nbclient\client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sangouda\AppData\Local\Programs\Python\Python312\Lib\site-packages\nbclient\util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sangouda\AppData\Local\Programs\Python\Python312\Lib\site-packages\nbclient\util.py", line 62, in just_run
    return loop.run_until_complete(coro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\sangouda\AppData\Local\Programs\Python\Python312\Lib\asyncio\base_events.py", line 664, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "C:\Users\sangouda\AppData\Local\Programs\Python\Python312\Lib\site-packages\nbclient\client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "C:\Users\sangouda\AppData\Local\Programs\Python\Python312\Lib\site-packages\nbclient\client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "C:\Users\sangouda\AppData\Local\Programs\Python\Python312\Lib\site-packages\nbclient\client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Generate synthetic linear data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term
X_b = np.c_[np.ones((100, 1)), X]

# OLS closed-form solution
theta_ols = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# Gradient Descent
eta = 0.1
n_iterations = 50
m = len(X_b)
theta = np.random.randn(2, 1)  # random init
theta_path = [theta]

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
    theta_path.append(theta)

theta_path = np.array(theta_path)

# Compute cost function grid
theta0_vals = np.linspace(-1, 8, 100)
theta1_vals = np.linspace(1, 5, 100)
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)

cost = np.zeros(T0.shape)
for i in range(T0.shape[0]):
    for j in range(T0.shape[1]):
        t = np.array([[T0[i, j]], [T1[i, j]]])
        cost[i, j] = np.mean((X_b.dot(t) - y) ** 2)

# ==============================
# Combined subplot figure
# ==============================
fig = plt.figure(figsize=(16, 8))

# --- Subplot 1: Regression line ---
ax1 = fig.add_subplot(231)
ax1.scatter(X, y, alpha=0.6)
ax1.plot(X, X_b.dot(theta_ols), "r-", label="OLS Solution")
ax1.plot(X, X_b.dot(theta), "g--", label="GD Solution (50 iters)")
ax1.set_xlabel("X")
ax1.set_ylabel("y")
ax1.set_title("OLS vs GD (Regression Line)")
ax1.legend()

# --- Subplot 2: Contour cost function ---
ax2 = fig.add_subplot(232)
CS = ax2.contour(T0, T1, cost, levels=np.logspace(-1, 3, 20), cmap="jet")
ax2.plot(theta_path[:, 0], theta_path[:, 1], "yo-", label="GD Path")
ax2.plot(theta_ols[0, 0], theta_ols[1, 0], "r*", markersize=15, label="OLS")
ax2.set_xlabel(r"$\theta_0$")
ax2.set_ylabel(r"$\theta_1$")
ax2.set_title("GD Path on Cost Function Contours")
ax2.legend()

# --- Subplot 3: 3D cost surface ---
ax3 = fig.add_subplot(233, projection="3d")
ax3.plot_surface(T0, T1, cost, cmap="viridis", alpha=0.8)
ax3.plot(theta_path[:, 0], theta_path[:, 1],
         [np.mean((X_b.dot(t.reshape(-1, 1)) - y) ** 2) for t in theta_path],
         "yo-", markersize=5, label="GD Path")
ax3.scatter(theta_ols[0, 0], theta_ols[1, 0],
            np.mean((X_b.dot(theta_ols) - y) ** 2),
            c="red", marker="*", s=200, label="OLS")
ax3.set_xlabel(r"$\theta_0$")
ax3.set_ylabel(r"$\theta_1$")
ax3.set_zlabel("Cost (MSE)")
ax3.set_title("Cost Surface with GD vs OLS")
ax3.legend()

import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic linear data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term
X_b = np.c_[np.ones((100, 1)), X]

# OLS closed-form solution
theta_ols = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# Gradient Descent
eta = 0.1
n_iterations = 50
m = len(X_b)
theta = np.random.randn(2, 1)  # random init
theta_path = [theta]

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
    theta_path.append(theta)

theta_path = np.array(theta_path)

# Compute cost function grid
theta0_vals = np.linspace(-1, 8, 100)
theta1_vals = np.linspace(1, 5, 100)
T0, T1 = np.meshgrid(theta0_vals, theta1_vals)

cost = np.zeros(T0.shape)
for i in range(T0.shape[0]):
    for j in range(T0.shape[1]):
        t = np.array([[T0[i, j]], [T1[i, j]]])
        cost[i, j] = np.mean((X_b.dot(t) - y) ** 2)

# 2D Contour Plot

ax4 = fig.add_subplot(234)
ax4.contour(T0, T1, cost, levels=50, cmap="viridis")
ax4.clabel(contours, inline=True, fontsize=8)

# Gradient Descent path
ax4.plot(theta_path[:, 0], theta_path[:, 1], "yo-", markersize=5, label="GD Path")

# OLS solution
ax4.scatter(theta_ols[0, 0], theta_ols[1, 0], c="red", marker="*", s=200, label="OLS Solution")

ax4.set_xlabel(r"$\theta_0$")
ax4.set_ylabel(r"$\theta_1$")
ax4.set_title("OLS vs Gradient Descent on Cost Function (2D Contours)")
ax4.legend()


import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic linear data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Add bias term
X_b = np.c_[np.ones((100, 1)), X]

# OLS closed-form solution
theta_ols = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# Gradient Descent
eta = 0.1
n_iterations = 50
m = len(X_b)
theta = np.random.randn(2, 1)  # random init
theta_path = [theta]
cost_values = []

for iteration in range(n_iterations):
    cost = np.mean((X_b.dot(theta) - y) ** 2)
    cost_values.append(cost)
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
    theta_path.append(theta)

theta_path = np.array(theta_path)

# Final OLS cost
ols_cost = np.mean((X_b.dot(theta_ols) - y) ** 2)

# Plot Cost vs Iterations (Gradient Descent)
ax5 = fig.add_subplot(235)
ax5.plot(range(len(cost_values)), cost_values, "bo-", label="Gradient Descent Path")
ax5.axhline(y=ols_cost, color="red", linestyle="--", label="OLS Cost (Closed Form)")
ax5.set_xlabel("Iterations")
ax5.set_ylabel("Cost (MSE)")
ax5.set_title("Cost Function Convergence: Gradient Descent vs OLS")
ax5.legend()


# Plot Theta path vs OLS
ax6 = fig.add_subplot(236)
ax6.plot(theta_path[:, 0], theta_path[:, 1], "yo-", markersize=5, label="GD Path (Î¸0, Î¸1)")
ax6.scatter(theta_ols[0, 0], theta_ols[1, 0], c="red", marker="*", s=200, label="OLS Solution")
ax6.set_xlabel(r"$\theta_0$")
ax6.set_ylabel(r"$\theta_1$")
ax6.set_title("Theta Path in Gradient Descent vs OLS Solution")
ax6.legend()



plt.tight_layout()
plt.show()

------------------

[31m---------------------------------------------------------------------------[39m
[31mNameError[39m                                 Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[2][39m[32m, line 124[39m
[32m    122[39m ax4 = fig.add_subplot([32m234[39m)
[32m    123[39m ax4.contour(T0, T1, cost, levels=[32m50[39m, cmap=[33m"[39m[33mviridis[39m[33m"[39m)
[32m--> [39m[32m124[39m ax4.clabel([43mcontours[49m, inline=[38;5;28;01mTrue[39;00m, fontsize=[32m8[39m)
[32m    126[39m [38;5;66;03m# Gradient Descent path[39;00m
[32m    127[39m ax4.plot(theta_path[:, [32m0[39m], theta_path[:, [32m1[39m], [33m"[39m[33myo-[39m[33m"[39m, markersize=[32m5[39m, label=[33m"[39m[33mGD Path[39m[33m"[39m)

[31mNameError[39m: name 'contours' is not defined
NameError: name 'contours' is not defined

