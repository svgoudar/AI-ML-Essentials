{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e9bdc4",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "Stopwords are **common words** in a language that usually **don‚Äôt carry significant meaning** in text analysis.\n",
    "\n",
    "Examples in English:\n",
    "`[\"is\", \"am\", \"are\", \"the\", \"in\", \"on\", \"at\", \"a\", \"of\", \"this\", \"that\"]`\n",
    "\n",
    "üëâ In the sentence:\n",
    "`\"The cat is on the mat.\"`\n",
    "The important words are: `[\"cat\", \"mat\"]`\n",
    "Words like `\"the\", \"is\", \"on\"` are stopwords.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Remove Stopwords?\n",
    "\n",
    "* They **don‚Äôt add much meaning** to most NLP tasks (like classification, topic modeling).\n",
    "* They **increase dataset size & computation** without improving accuracy.\n",
    "* Removing them helps models focus on meaningful words.\n",
    "\n",
    "‚ö†Ô∏è **BUT**: In some tasks (e.g., translation, question answering), stopwords **must be kept**, because they affect grammar and meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How to Remove Stopwords?\n",
    "\n",
    "### 1. **Using NLTK**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords (only once)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "text = \"This is a simple example showing the removal of stopwords in NLP.\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [w for w in words if w.lower() not in stopwords.words(\"english\")]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"After Stopword Removal:\", filtered_words)\n",
    "```\n",
    "\n",
    "‚úÖ **Output**\n",
    "\n",
    "```\n",
    "Original: ['This', 'is', 'a', 'simple', 'example', 'showing', 'the', 'removal', 'of', 'stopwords', 'in', 'NLP', '.']\n",
    "After Stopword Removal: ['simple', 'example', 'showing', 'removal', 'stopwords', 'NLP']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Using spaCy**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a simple example showing the removal of stopwords in NLP.\")\n",
    "\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "print(filtered_words)\n",
    "```\n",
    "\n",
    "‚úÖ Output:\n",
    "\n",
    "```\n",
    "['simple', 'example', 'showing', 'removal', 'stopwords', 'NLP']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Using Custom Stopwords List**\n",
    "\n",
    "You can add/remove words from the default list:\n",
    "\n",
    "```python\n",
    "custom_stopwords = set(stopwords.words(\"english\"))\n",
    "custom_stopwords.update([\"example\", \"showing\"])  # add extra words\n",
    "\n",
    "filtered_words = [w for w in words if w.lower() not in custom_stopwords]\n",
    "print(filtered_words)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* **Stopwords** = common words with little meaning.\n",
    "* Removing them helps simplify text and improves performance in many NLP tasks.\n",
    "* Different libraries (NLTK, spaCy, sklearn) provide stopword lists.\n",
    "* You can always create a **custom list** depending on your dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a486a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sangouda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sangouda\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a great movie and I really enjoyed it!\n",
      "Tokenized Words: ['This', 'is', 'a', 'great', 'movie', 'and', 'I', 'really', 'enjoyed', 'it', '!']\n",
      "After Stopword Removal: ['great', 'movie', 'really', 'enjoyed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required resources (run once)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Example text\n",
    "text = \"This is a great movie and I really enjoyed it!\"\n",
    "\n",
    "# Tokenize text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [w for w in words if w.lower() not in stop_words and w.isalpha()]\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Tokenized Words:\", words)\n",
    "print(\"After Stopword Removal:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00fa398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
