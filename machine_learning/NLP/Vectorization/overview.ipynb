{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b50e89e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "# Vectorization\n",
    "\n",
    "## **1. Bag of Words (BoW)**\n",
    "\n",
    "* **Concept:** Represents a text as a **vector of word counts**. Ignores grammar and word order.\n",
    "* **Steps:**\n",
    "\n",
    "  1. Build a vocabulary of all unique words across the corpus.\n",
    "  2. Count the frequency of each word in every document.\n",
    "* **Pros:** Simple, easy to implement.\n",
    "* **Cons:** Ignores context and semantics, sparse vectors.\n",
    "\n",
    "**Example:**\n",
    "Corpus: \\[\"I love NLP\", \"NLP is amazing\"]\n",
    "BoW vectors:\n",
    "\n",
    "* Doc1: `[1, 0, 1, 0, 1]`\n",
    "* Doc2: `[0, 1, 1, 1, 0]`\n",
    "\n",
    "---\n",
    "\n",
    "## **2. TF (Term Frequency)**\n",
    "\n",
    "* **Concept:** Represents words by their frequency in a document.\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  TF(word) = \\frac{\\text{Number of times word appears in document}}{\\text{Total number of words in document}}\n",
    "  $$\n",
    "* **Pros:** Captures importance of words relative to the document.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "\n",
    "* **Concept:** Adjusts term frequency by how rare a word is across all documents. Rare words get more weight.\n",
    "* **Formula:**\n",
    "\n",
    "  $$\n",
    "  TFIDF(word) = TF(word) \\times \\log\\frac{N}{DF(word)}\n",
    "  $$\n",
    "\n",
    "  * $N$ = Total number of documents\n",
    "  * $DF(word)$ = Number of documents containing the word\n",
    "* **Pros:** Reduces importance of common words like \"the\", \"is\".\n",
    "* **Use:** Widely used in text classification and information retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Word Embeddings**\n",
    "\n",
    "* **Concept:** Dense vector representations capturing **semantic meaning** of words.\n",
    "* **Techniques:**\n",
    "\n",
    "  * **Word2Vec:** Predicts a word from its context (skip-gram or CBOW).\n",
    "  * **GloVe:** Uses global word co-occurrence matrix.\n",
    "  * **FastText:** Captures subword information for rare words.\n",
    "* **Pros:** Captures meaning, similar words have close vectors.\n",
    "* **Cons:** Pretrained embeddings may not always fit your corpus.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Contextualized Embeddings (Transformers)**\n",
    "\n",
    "* **Concept:** Word representation depends on **context in the sentence**.\n",
    "* **Models:** BERT, GPT, RoBERTa, XLNet, etc.\n",
    "* **Pros:** Handles polysemy (words with multiple meanings), state-of-the-art performance.\n",
    "* **Cons:** Computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. One-Hot Encoding**\n",
    "\n",
    "* **Concept:** Represent each word as a **binary vector** with a 1 at the word’s index in the vocabulary.\n",
    "* **Pros:** Simple, easy to implement.\n",
    "* **Cons:** Very sparse, does not capture meaning or similarity.\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison Summary**\n",
    "\n",
    "| Technique             | Sparse/Dense | Captures Semantics | Context Awareness |\n",
    "| --------------------- | ------------ | ------------------ | ----------------- |\n",
    "| Bag of Words          | Sparse       | ❌                  | ❌                 |\n",
    "| TF                    | Sparse       | ❌                  | ❌                 |\n",
    "| TF-IDF                | Sparse       | ❌                  | ❌                 |\n",
    "| Word Embeddings       | Dense        | ✅                  | ❌                 |\n",
    "| Contextual Embeddings | Dense        | ✅                  | ✅                 |\n",
    "| One-Hot Encoding      | Sparse       | ❌                  | ❌                 |\n",
    "\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
