{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad6cbd7",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering is an **unsupervised machine learning algorithm** used to group similar data points into clusters.\n",
    "\n",
    "* Unlike K-Means, you **don’t need to specify the number of clusters upfront**.\n",
    "* It builds a **hierarchy of clusters**, often visualized as a **dendrogram**.\n",
    "\n",
    "There are **two main types**:\n",
    "\n",
    "1. **Agglomerative (Bottom-Up)** – most common\n",
    "\n",
    "   * Start with each data point as its own cluster.\n",
    "   * Iteratively merge the **two closest clusters** until all points are in one cluster.\n",
    "\n",
    "2. **Divisive (Top-Down)**\n",
    "\n",
    "   * Start with all points in one cluster.\n",
    "   * Recursively split clusters until each cluster contains a single point.\n",
    "\n",
    "> Agglomerative is far more commonly used in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## **Workflow of Agglomerative Clustering**\n",
    "\n",
    "1. **Compute Distance Matrix**\n",
    "\n",
    "   * Calculate pairwise distances between all points using a metric (Euclidean, Manhattan, Cosine, etc.).\n",
    "\n",
    "2. **Linkage Criteria**\n",
    "\n",
    "   * Decide how to measure distance between clusters:\n",
    "\n",
    "     * **Single linkage:** distance between closest points in clusters\n",
    "     * **Complete linkage:** distance between farthest points in clusters\n",
    "     * **Average linkage:** average distance between points in clusters\n",
    "     * **Ward’s method:** minimize variance within merged clusters\n",
    "\n",
    "3. **Merge Clusters Iteratively**\n",
    "\n",
    "   * Merge the two clusters with the smallest distance according to the chosen linkage.\n",
    "   * Update the distance matrix after each merge.\n",
    "\n",
    "4. **Build the Dendrogram**\n",
    "\n",
    "   * Dendrogram is a tree-like diagram showing cluster merges at different distances.\n",
    "   * The **height of the merge** represents distance between clusters.\n",
    "\n",
    "5. **Cut the Dendrogram to Form Clusters**\n",
    "\n",
    "   * Decide the number of clusters by **choosing a height** to cut the dendrogram.\n",
    "   * Clusters below that cut are considered separate.\n",
    "\n",
    "---\n",
    "\n",
    "## **Distance Metrics and Linkage Methods**\n",
    "\n",
    "| Metric      | Description                 |\n",
    "| ----------- | --------------------------- |\n",
    "| Euclidean   | Straight-line distance      |\n",
    "| Manhattan   | Sum of absolute differences |\n",
    "| Cosine      | Angle between vectors       |\n",
    "| Correlation | 1 - correlation coefficient |\n",
    "\n",
    "| Linkage  | Description                                        |\n",
    "| -------- | -------------------------------------------------- |\n",
    "| Single   | Minimum distance between points of clusters        |\n",
    "| Complete | Maximum distance between points of clusters        |\n",
    "| Average  | Average distance between all points                |\n",
    "| Ward     | Merge clusters to minimize within-cluster variance |\n",
    "\n",
    "> **Ward linkage + Euclidean distance** is commonly used because it tends to produce clusters of similar size and reduces variance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Hierarchical Clustering**\n",
    "\n",
    "* No need to pre-specify the number of clusters.\n",
    "* Produces **dendrogram**, which is interpretable.\n",
    "* Can capture **nested clusters** (hierarchical relationships).\n",
    "\n",
    "---\n",
    "\n",
    "## **Limitations**\n",
    "\n",
    "* Computationally expensive for large datasets (O(n²) time and memory).\n",
    "* Sensitive to **noise and outliers**.\n",
    "* Choice of distance metric and linkage affects results.\n",
    "\n",
    "---\n",
    "\n",
    "## **Intuition**\n",
    "\n",
    "* Think of **data points as leaves of a tree**:\n",
    "\n",
    "  * Agglomerative HC gradually merges leaves into branches → branches into bigger branches → one tree.\n",
    "* Cutting the tree at a certain height gives you **clusters**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252df13",
   "metadata": {},
   "source": [
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
