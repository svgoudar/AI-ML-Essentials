{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d914d46e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "The **main hyperparameter in K-Means is `k` (number of clusters)**. Choosing the right value of `k` is critical because it directly affects model performance. There are also auxiliary parameters like `init`, `max_iter`, and `n_init`.\n",
    "\n",
    "#### 1. Choosing `k`\n",
    "\n",
    "* **Elbow Method**: Plot the cost function (Within-Cluster-Sum-of-Squares, WCSS) against different values of `k`. The \"elbow point\" indicates a good trade-off between variance explained and complexity.\n",
    "* **Silhouette Score**: Measures cohesion (how close points are in a cluster) vs separation (how far clusters are apart). Higher silhouette = better cluster quality.\n",
    "* **Gap Statistic**: Compares WCSS of actual clustering with WCSS of randomly generated data.\n",
    "\n",
    "#### 2. Initialization method (`init`)\n",
    "\n",
    "* **k-means++** (default): Ensures initial centroids are well spread out, improving convergence.\n",
    "* **Random**: Risk of poor local minima, but with `n_init > 1`, mitigated.\n",
    "\n",
    "#### 3. Number of runs (`n_init`)\n",
    "\n",
    "* Run clustering multiple times with different centroid seeds, then choose the best outcome. Higher `n_init` reduces sensitivity to bad initialization.\n",
    "\n",
    "#### 4. Maximum iterations (`max_iter`)\n",
    "\n",
    "* Ensures convergence. Usually defaults (like 300) are sufficient, but can be tuned for speed vs stability.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling Overfitting and Underfitting in K-Means\n",
    "\n",
    "Though clustering is unsupervised (no labels), the concepts of underfitting/overfitting still apply in terms of **cluster quality**.\n",
    "\n",
    "#### Underfitting (too simple clusters)\n",
    "\n",
    "* **Cause**:\n",
    "\n",
    "  * Choosing too few clusters (`k` too small).\n",
    "  * Poor initialization.\n",
    "* **Symptoms**:\n",
    "\n",
    "  * High WCSS (large distances within clusters).\n",
    "  * Low silhouette score.\n",
    "* **Fix**:\n",
    "\n",
    "  * Increase `k`.\n",
    "  * Use `k-means++` instead of random init.\n",
    "  * Increase `n_init`.\n",
    "\n",
    "#### Overfitting (too many clusters)\n",
    "\n",
    "* **Cause**:\n",
    "\n",
    "  * Choosing `k` too large.\n",
    "* **Symptoms**:\n",
    "\n",
    "  * Very low WCSS but clusters don’t generalize (each point may form its own cluster).\n",
    "  * Silhouette score decreases after some point.\n",
    "* **Fix**:\n",
    "\n",
    "  * Use elbow method or silhouette score to cap `k`.\n",
    "  * Regularize by preferring simpler solutions (Occam’s razor).\n",
    "\n",
    "---\n",
    "\n",
    "**In short**\n",
    "\n",
    "* **Tune `k`** carefully using elbow, silhouette, or gap statistic.\n",
    "* **Use `k-means++` and `n_init > 10`** for stability.\n",
    "* **Balance `k`** to avoid under/overfitting: too low = coarse clusters, too high = fragmented clusters.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
