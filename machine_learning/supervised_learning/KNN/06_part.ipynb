{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1b0855",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9042f",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "Actually, **K-Nearest Neighbors (KNN) doesn’t explicitly have a “cost function” like linear regression or SVM**, because it is a **non-parametric, instance-based learning algorithm**. But we can still think about a related concept that measures “how well KNN is performing.”\n",
    "\n",
    "---\n",
    "\n",
    "## **1. KNN is Lazy Learning**\n",
    "\n",
    "* KNN **does not train a model**.\n",
    "* There are **no parameters like weights** to optimize via a cost function.\n",
    "* All computation happens at prediction time: distances are measured, neighbors are selected, and votes/averages are computed.\n",
    "\n",
    "So unlike linear regression (minimizing squared error) or logistic regression (maximizing likelihood), **KNN does not have a formal cost function during training**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Implicit “Cost” at Prediction**\n",
    "\n",
    "We can think about KNN performance in terms of **prediction error**:\n",
    "\n",
    "### **A. Classification**\n",
    "\n",
    "* The “cost” is related to **misclassification**:\n",
    "\n",
    "$$\n",
    "\\text{Error rate} = \\frac{\\text{Number of misclassified points}}{\\text{Total points}}\n",
    "$$\n",
    "\n",
    "* Alternatively, **weighted misclassification** can be used if neighbors contribute differently (e.g., closer neighbors count more).\n",
    "\n",
    "### **B. Regression**\n",
    "\n",
    "* The “cost” is related to the **difference between predicted and true values**:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "* Here, $\\hat{y}_i$ is the KNN prediction (mean of neighbors).\n",
    "* So for regression, you can think of **MSE or MAE** as the “implicit cost” that KNN is minimizing by choosing neighbors wisely.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Choosing k as Implicit Optimization**\n",
    "\n",
    "* Selecting **k** can be seen as **optimizing the model to minimize prediction error**.\n",
    "* Small $k$ → low bias, high variance → sensitive to noise.\n",
    "* Large $k$ → high bias, low variance → smoother prediction.\n",
    "* Cross-validation is used to find **the k that minimizes validation error**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Optional Weighted Cost Function**\n",
    "\n",
    "Some KNN variants use **distance-weighted predictions**:\n",
    "\n",
    "* **Classification**: closer neighbors get more weight in majority vote.\n",
    "* **Regression**: prediction is weighted average:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{\\sum_{i=1}^k w_i y_i}{\\sum_{i=1}^k w_i}, \\quad w_i = \\frac{1}{d(x, x_i)}\n",
    "$$\n",
    "\n",
    "* Here, $d(x, x_i)$ is the distance to neighbor $i$.\n",
    "* This is effectively **minimizing weighted prediction error**.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Aspect                    | KNN Behavior                                             |\n",
    "| ------------------------- | -------------------------------------------------------- |\n",
    "| Traditional cost function | None (lazy learner)                                      |\n",
    "| Implicit “cost”           | Misclassification (classification), MSE/MAE (regression) |\n",
    "| Optimization              | Choice of k and distance weighting                       |\n",
    "| Goal                      | Minimize prediction error                                |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
