{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e89466",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "KNN has **no training phase in the usual sense**, but it still has **hyperparameters** that control how predictions are made:\n",
    "\n",
    "1. **`k` (number of neighbors)** – how many nearby points influence the prediction\n",
    "2. **Distance metric** – how we measure “closeness” between points\n",
    "\n",
    "   * Euclidean, Manhattan, Minkowski, Hamming, etc.\n",
    "3. **Weights of neighbors** – whether all neighbors contribute equally or closer ones count more\n",
    "\n",
    "   * `uniform`: all neighbors have equal weight\n",
    "   * `distance`: closer neighbors have higher influence\n",
    "\n",
    "Hyperparameter tuning is the process of **finding the combination of these hyperparameters that minimizes error** (or maximizes accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Key Hyperparameters**\n",
    "\n",
    "| Hyperparameter    | Description                            | Effect on model                                                                    |\n",
    "| ----------------- | -------------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| `n_neighbors (k)` | Number of nearest neighbors            | Small k → noisy predictions, overfit <br> Large k → smoother predictions, underfit |\n",
    "| `metric`          | Distance metric to calculate closeness | Changes neighbor selection → affects predictions                                   |\n",
    "| `weights`         | Weighting of neighbors                 | Can improve performance by prioritizing closer points                              |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Methods for Hyperparameter Tuning**\n",
    "\n",
    "### **A. Manual Search**\n",
    "\n",
    "* Try different values of `k` (e.g., 1,3,5,…,15)\n",
    "* Evaluate performance on a validation set\n",
    "* Choose the `k` giving the **best accuracy / lowest error**\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Grid Search**\n",
    "\n",
    "* Explore a **grid of hyperparameter combinations**:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3,5,7,9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", grid.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **C. Randomized Search**\n",
    "\n",
    "* Instead of trying all combinations, sample **random combinations** (faster for large grids)\n",
    "* Works similarly to GridSearchCV but more efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### **D. Cross-Validation**\n",
    "\n",
    "* Always combine tuning with **cross-validation** to avoid overfitting\n",
    "* Use **k-fold CV** (e.g., k=5) to evaluate each hyperparameter setting.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Intuition Behind Tuning `k`**\n",
    "\n",
    "* **Small `k`**:\n",
    "\n",
    "  * Captures local patterns\n",
    "  * Sensitive to noise → may misclassify outliers\n",
    "* **Large `k`**:\n",
    "\n",
    "  * Smooths predictions\n",
    "  * Ignores local patterns → may underfit\n",
    "\n",
    "Optimal `k` is usually found by **experimenting with validation scores or silhouette scores (for clustering)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Weighted vs Uniform Neighbors**\n",
    "\n",
    "* **Uniform**: all neighbors contribute equally\n",
    "* **Distance**: closer neighbors contribute more → often improves accuracy in noisy datasets\n",
    "\n",
    "**Intuition:** nearer neighbors are more likely to be similar, so weighting helps KNN “trust” the right points.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary Workflow**\n",
    "\n",
    "1. Choose a **range of hyperparameters** (`k`, `metric`, `weights`)\n",
    "2. Split data (train/validation or use cross-validation)\n",
    "3. Evaluate performance for each combination\n",
    "4. Select **best combination**\n",
    "5. Retrain KNN on the full training set with these hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f581179",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
