{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25714567",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09105373",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "\n",
    "## 1. **Nearby points have similar labels (locality assumption)**\n",
    "\n",
    "* Core assumption:\n",
    "\n",
    "  > *If two points are close in feature space, they are likely to have the same class (classification) or similar values (regression).*\n",
    "* This is basically saying the data is **locally smooth**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **The chosen distance metric is meaningful**\n",
    "\n",
    "* KNN assumes the **distance function** (Euclidean, Manhattan, cosine, etc.) correctly reflects **similarity**.\n",
    "* If features are on different scales (e.g., height in cm vs income in lakhs), distance won’t make sense unless you **normalize/standardize**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Features are equally important (unless weighted)**\n",
    "\n",
    "* Default KNN assumes every feature contributes equally to distance.\n",
    "* If one feature is irrelevant or has high variance, it can distort distances.\n",
    "* Sometimes solved with **feature scaling**, **feature selection**, or **distance weighting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Decision boundary is locally smooth**\n",
    "\n",
    "* KNN assumes the **decision boundary** between classes is not too irregular, so local neighborhoods give meaningful predictions.\n",
    "* With noisy data or highly overlapping classes, this assumption breaks.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **No strong independence assumption (unlike Naive Bayes)**\n",
    "\n",
    "* Unlike probabilistic models, KNN does not assume feature independence.\n",
    "* But it does assume **“closeness in feature space = similarity in outcome”**.\n",
    "* This assumption may fail in **high-dimensional spaces** (curse of dimensionality).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Data distribution is representative & dense**\n",
    "\n",
    "* KNN works best when:\n",
    "\n",
    "  * Training data covers the space well (dense enough).\n",
    "  * Otherwise, new points may fall in **empty regions**, making prediction unreliable.\n",
    "* This means:\n",
    "\n",
    "  > KNN assumes **training data is large, clean, and representative of the population**.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary**\n",
    "\n",
    "KNN assumes:\n",
    "\n",
    "1. **Local smoothness** → neighbors have similar outcomes.\n",
    "2. **Distance metric is meaningful**.\n",
    "3. **Features are scaled & equally important** (unless weighted).\n",
    "4. **Decision boundary is not too complex**.\n",
    "5. **Sufficient data density** for reliable neighborhoods.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
