{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af30febe",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebbf5c",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "## **1. Data Preparation**\n",
    "\n",
    "Before applying KNN:\n",
    "\n",
    "* **Collect data:** You need labeled training data for classification or regression.\n",
    "* **Feature scaling:** KNN relies on distance metrics, so features should be on the same scale. Use:\n",
    "\n",
    "  * Min-Max Scaling\n",
    "  * Standardization (Z-score)\n",
    "\n",
    "**Why scaling matters:** Without scaling, a feature with a larger range will dominate the distance calculation.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Choose the Distance Metric**\n",
    "\n",
    "Decide how to measure \"closeness\" between points. Common choices:\n",
    "\n",
    "| Metric    | Formula (2D example)                    | When to Use                      |   |             |   |                                        |\n",
    "| --------- | --------------------------------------- | -------------------------------- | - | ----------- | - | -------------------------------------- |\n",
    "| Euclidean | $\\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}$  | Most common, continuous features |   |             |   |                                        |\n",
    "| Manhattan | (                                       | x\\_1 - y\\_1                      | + | x\\_2 - y\\_2 | ) | Grid-like distances, discrete features |\n",
    "| Minkowski | Generalization of Euclidean & Manhattan | Flexibility with parameter `p`   |   |             |   |                                        |\n",
    "| Hamming   | Counts different features               | Categorical variables            |   |             |   |                                        |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Select `k`**\n",
    "\n",
    "Decide how many neighbors to consider:\n",
    "\n",
    "* **Small `k`** → sensitive to noise, high variance (overfitting)\n",
    "* **Large `k`** → smooths boundaries, may underfit\n",
    "* Common practice: test multiple odd `k` values using cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Compute Distances**\n",
    "\n",
    "For each new data point $x_{\\text{new}}$:\n",
    "\n",
    "1. Calculate the distance to every point in the training set.\n",
    "2. Store these distances in a sorted list.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Identify Nearest Neighbors**\n",
    "\n",
    "* Pick the top `k` closest points from the sorted distance list.\n",
    "* These points “vote” for the label (classification) or contribute to the average (regression).\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Aggregate Neighbor Information**\n",
    "\n",
    "* **Classification:** Majority vote determines the predicted class.\n",
    "\n",
    "  * Optional: weighted vote (closer neighbors count more).\n",
    "* **Regression:** Take the mean (or weighted mean) of neighbors’ values.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Assign the Label or Value**\n",
    "\n",
    "* Output the predicted class or numerical value for $x_{\\text{new}}$.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Evaluate the Model**\n",
    "\n",
    "* Use performance metrics:\n",
    "\n",
    "  * Classification: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n",
    "  * Regression: MSE, RMSE, MAE, R²\n",
    "\n",
    "* Optionally, tune `k` and/or distance metric to improve results.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. **Prepare data** → 2. **Scale features** → 3. **Select k & distance metric** →\n",
    "2. **Compute distances** → 5. **Find nearest neighbors** → 6. **Aggregate results** → 7. **Predict output** → 8. **Evaluate & tune**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
