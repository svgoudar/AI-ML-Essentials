{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9391a791",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "A decision tree can solve both **classification** and **regression** problems.\n",
    "\n",
    "- **Entropy**\n",
    "- **Gini Index**\n",
    "- **Information Gain**\n",
    "\n",
    "We’ll also briefly differentiate between the two major types of decision tree algorithms:\n",
    "\n",
    "1. **ID3 (Iterative Dichotomiser 3):** Allows multiple splits at a node.\n",
    "2. **CART (Classification and Regression Tree):** Restricts splits to binary decisions (used in libraries like `sklearn`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition: Decision Trees as Conditional Statements**\n",
    "\n",
    "Think of decision trees as an extension of simple conditional statements. For example, in Python:\n",
    "\n",
    "```python\n",
    "age = 14\n",
    "if age <= 14:\n",
    "    print(\"The person is in school.\")\n",
    "elif 15 <= age <= 21:\n",
    "    print(\"The person may be in college.\")\n",
    "else:\n",
    "    print(\"The person has passed college.\")\n",
    "```\n",
    "\n",
    "This logic resembles how a decision tree splits data:\n",
    "\n",
    "1. Start with a **root node** (e.g., `age <= 14`).\n",
    "2. Branch into outcomes (e.g., **Yes** or **No**).\n",
    "3. Continue splitting based on new conditions until reaching a pure classification (e.g., \"School,\" \"College\").\n",
    "\n",
    "---\n",
    "\n",
    "### **Constructing a Decision Tree**\n",
    "\n",
    "Let’s explore how a decision tree is built using a **real-world dataset**. Imagine a dataset with these features:\n",
    "\n",
    "- **Outlook** (Sunny, Overcast, Rainy)\n",
    "- **Temperature**\n",
    "- **Humidity**\n",
    "- **Wind**\n",
    "Our goal: Predict whether someone will **Play Tennis**.\n",
    "\n",
    "#### Step 1: Selecting the Root Node\n",
    "\n",
    "- Start by evaluating each feature. For instance, take **Outlook**.\n",
    "- Count the occurrences:\n",
    "  - **Sunny:** 2 Yes, 3 No\n",
    "  - **Overcast:** 4 Yes, 0 No (Pure split)\n",
    "  - **Rainy:** 3 Yes, 2 No\n",
    "- Notice that **Sunny** and **Rainy** are **impure splits** (contain both Yes and No), while **Overcast** is a **pure split**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Metrics for Splitting**\n",
    "\n",
    "When splitting nodes, we use metrics like:\n",
    "\n",
    "1. **Entropy:** Measures the impurity in the dataset.\n",
    "2. **Information Gain:** Determines how much uncertainty is reduced after a split.\n",
    "3. **Gini Index:** Another measure of impurity, often used in CART.\n",
    "\n",
    "#### Example: Calculating Entropy\n",
    "\n",
    "For the root node:\n",
    "$\n",
    "\\text{Entropy} = - \\sum_{i=1}^{n} p_i \\cdot \\log_2(p_i)\n",
    "$\n",
    "Where$ p_i$ is the proportion of each class (Yes/No).\n",
    "\n",
    "#### Example: Information Gain\n",
    "\n",
    "$\n",
    "\\text{Information Gain} = \\text{Entropy (Parent)} - \\text{Weighted Average Entropy (Child Nodes)}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Splitting Impure Nodes**\n",
    "\n",
    "For impure splits like **Sunny (2 Yes, 3 No)**:\n",
    "\n",
    "1. Consider another feature (e.g., **Temperature**).\n",
    "2. Split further until achieving pure nodes or acceptable impurity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion and Next Steps**\n",
    "\n",
    "This process continues until:\n",
    "\n",
    "- The tree reaches pure splits.\n",
    "- Or further splits don’t improve the model significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04589a5b",
   "metadata": {},
   "source": [
    "A **Decision Tree** is a **supervised learning algorithm** used for both **classification** (predicting categories) and **regression** (predicting continuous values).\n",
    "It works like a flowchart:\n",
    "\n",
    "* Each **internal node** represents a condition (feature test).\n",
    "* Each **branch** represents the outcome of the condition.\n",
    "* Each **leaf node** gives the final prediction (class label or numeric value).\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Imagine you’re deciding whether to play outside:\n",
    "\n",
    "* If it’s sunny → Yes\n",
    "* If it’s rainy → Check if you have an umbrella → Yes or No\n",
    "\n",
    "This is exactly how decision trees work: **split data into smaller subsets based on conditions** until the subsets are “pure” (contain mostly one label).\n",
    "\n",
    "---\n",
    "\n",
    "## Structure of a Decision Tree\n",
    "\n",
    "* **Root Node** → the first question/condition (best split).\n",
    "* **Decision Nodes** → intermediate questions.\n",
    "* **Leaf Nodes** → final output (prediction).\n",
    "\n",
    "Example (Classification):\n",
    "\n",
    "```\n",
    "         Weather?\n",
    "        /       \\\n",
    "     Sunny      Rainy\n",
    "     /   \\        \\\n",
    "  Hot    Cold    Umbrella?\n",
    "   |       |     /       \\\n",
    " Play   Don’t Play   Play   Don’t Play\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How Does the Tree Decide Where to Split\n",
    "\n",
    "At each step, the algorithm chooses the **best feature and threshold** to split the data.\n",
    "It uses **impurity measures** to decide:\n",
    "\n",
    "### For Classification:\n",
    "\n",
    "* **Gini Impurity** (most common in CART)\n",
    "* **Entropy / Information Gain** (used in ID3, C4.5)\n",
    "\n",
    "👉 Lower impurity = better split.\n",
    "\n",
    "### For Regression:\n",
    "\n",
    "* **Mean Squared Error (MSE)**\n",
    "* **Mean Absolute Error (MAE)**\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "✅ Easy to interpret and visualize\n",
    "✅ Handles both classification & regression\n",
    "✅ Works with both numerical and categorical data\n",
    "✅ No need for feature scaling\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "❌ Can overfit (very deep trees memorize training data)\n",
    "❌ Sensitive to small changes in data\n",
    "❌ Greedy splitting (locally optimal, not always globally optimal)\n",
    "\n",
    "👉 To fix these, we use **Ensembles** like **Random Forests** and **Gradient Boosted Trees**.\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow of a Decision Tree**\n",
    "\n",
    "1. Start with the root node (entire dataset).\n",
    "2. Choose the **best feature** to split on (using impurity measure).\n",
    "3. Split the dataset into subsets.\n",
    "4. Repeat recursively for each subset.\n",
    "5. Stop when:\n",
    "\n",
    "   * All points belong to the same class (pure leaf).\n",
    "   * Max depth reached.\n",
    "   * Minimum number of samples per leaf reached.\n",
    "\n",
    "---\n",
    "\n",
    "**Example Use Cases**\n",
    "\n",
    "* **Classification:** Spam detection, disease diagnosis, loan approval.\n",
    "* **Regression:** Predicting house prices, sales forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "A **Decision Tree** is like repeatedly asking **“the best question”** until you reach a clear decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab716f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
