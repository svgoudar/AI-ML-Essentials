{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9391a791",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "A decision tree can solve both **classification** and **regression** problems.\n",
    "\n",
    "- **Entropy**\n",
    "- **Gini Index**\n",
    "- **Information Gain**\n",
    "\n",
    "Weâ€™ll also briefly differentiate between the two major types of decision tree algorithms:\n",
    "\n",
    "1. **ID3 (Iterative Dichotomiser 3):** Allows multiple splits at a node.\n",
    "2. **CART (Classification and Regression Tree):** Restricts splits to binary decisions (used in libraries like `sklearn`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition: Decision Trees as Conditional Statements**\n",
    "\n",
    "Think of decision trees as an extension of simple conditional statements. For example, in Python:\n",
    "\n",
    "```python\n",
    "age = 14\n",
    "if age <= 14:\n",
    "    print(\"The person is in school.\")\n",
    "elif 15 <= age <= 21:\n",
    "    print(\"The person may be in college.\")\n",
    "else:\n",
    "    print(\"The person has passed college.\")\n",
    "```\n",
    "\n",
    "This logic resembles how a decision tree splits data:\n",
    "\n",
    "1. Start with a **root node** (e.g., `age <= 14`).\n",
    "2. Branch into outcomes (e.g., **Yes** or **No**).\n",
    "3. Continue splitting based on new conditions until reaching a pure classification (e.g., \"School,\" \"College\").\n",
    "\n",
    "---\n",
    "\n",
    "### **Constructing a Decision Tree**\n",
    "\n",
    "Letâ€™s explore how a decision tree is built using a **real-world dataset**. Imagine a dataset with these features:\n",
    "\n",
    "- **Outlook** (Sunny, Overcast, Rainy)\n",
    "- **Temperature**\n",
    "- **Humidity**\n",
    "- **Wind**\n",
    "Our goal: Predict whether someone will **Play Tennis**.\n",
    "\n",
    "#### Step 1: Selecting the Root Node\n",
    "\n",
    "- Start by evaluating each feature. For instance, take **Outlook**.\n",
    "- Count the occurrences:\n",
    "  - **Sunny:** 2 Yes, 3 No\n",
    "  - **Overcast:** 4 Yes, 0 No (Pure split)\n",
    "  - **Rainy:** 3 Yes, 2 No\n",
    "- Notice that **Sunny** and **Rainy** are **impure splits** (contain both Yes and No), while **Overcast** is a **pure split**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Metrics for Splitting**\n",
    "\n",
    "When splitting nodes, we use metrics like:\n",
    "\n",
    "1. **Entropy:** Measures the impurity in the dataset.\n",
    "2. **Information Gain:** Determines how much uncertainty is reduced after a split.\n",
    "3. **Gini Index:** Another measure of impurity, often used in CART.\n",
    "\n",
    "#### Example: Calculating Entropy\n",
    "\n",
    "For the root node:\n",
    "$\n",
    "\\text{Entropy} = - \\sum_{i=1}^{n} p_i \\cdot \\log_2(p_i)\n",
    "$\n",
    "Where$ p_i$ is the proportion of each class (Yes/No).\n",
    "\n",
    "#### Example: Information Gain\n",
    "\n",
    "$\n",
    "\\text{Information Gain} = \\text{Entropy (Parent)} - \\text{Weighted Average Entropy (Child Nodes)}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Splitting Impure Nodes**\n",
    "\n",
    "For impure splits like **Sunny (2 Yes, 3 No)**:\n",
    "\n",
    "1. Consider another feature (e.g., **Temperature**).\n",
    "2. Split further until achieving pure nodes or acceptable impurity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion and Next Steps**\n",
    "\n",
    "This process continues until:\n",
    "\n",
    "- The tree reaches pure splits.\n",
    "- Or further splits donâ€™t improve the model significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04589a5b",
   "metadata": {},
   "source": [
    "A **Decision Tree** is a **supervised learning algorithm** used for both **classification** (predicting categories) and **regression** (predicting continuous values).\n",
    "It works like a flowchart:\n",
    "\n",
    "* Each **internal node** represents a condition (feature test).\n",
    "* Each **branch** represents the outcome of the condition.\n",
    "* Each **leaf node** gives the final prediction (class label or numeric value).\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Imagine youâ€™re deciding whether to play outside:\n",
    "\n",
    "* If itâ€™s sunny â†’ Yes\n",
    "* If itâ€™s rainy â†’ Check if you have an umbrella â†’ Yes or No\n",
    "\n",
    "This is exactly how decision trees work: **split data into smaller subsets based on conditions** until the subsets are â€œpureâ€ (contain mostly one label).\n",
    "\n",
    "---\n",
    "\n",
    "## Structure of a Decision Tree\n",
    "\n",
    "* **Root Node** â†’ the first question/condition (best split).\n",
    "* **Decision Nodes** â†’ intermediate questions.\n",
    "* **Leaf Nodes** â†’ final output (prediction).\n",
    "\n",
    "Example (Classification):\n",
    "\n",
    "```\n",
    "         Weather?\n",
    "        /       \\\n",
    "     Sunny      Rainy\n",
    "     /   \\        \\\n",
    "  Hot    Cold    Umbrella?\n",
    "   |       |     /       \\\n",
    " Play   Donâ€™t Play   Play   Donâ€™t Play\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How Does the Tree Decide Where to Split\n",
    "\n",
    "At each step, the algorithm chooses the **best feature and threshold** to split the data.\n",
    "It uses **impurity measures** to decide:\n",
    "\n",
    "### For Classification:\n",
    "\n",
    "* **Gini Impurity** (most common in CART)\n",
    "* **Entropy / Information Gain** (used in ID3, C4.5)\n",
    "\n",
    "ðŸ‘‰ Lower impurity = better split.\n",
    "\n",
    "### For Regression:\n",
    "\n",
    "* **Mean Squared Error (MSE)**\n",
    "* **Mean Absolute Error (MAE)**\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "âœ… Easy to interpret and visualize\n",
    "âœ… Handles both classification & regression\n",
    "âœ… Works with both numerical and categorical data\n",
    "âœ… No need for feature scaling\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "âŒ Can overfit (very deep trees memorize training data)\n",
    "âŒ Sensitive to small changes in data\n",
    "âŒ Greedy splitting (locally optimal, not always globally optimal)\n",
    "\n",
    "ðŸ‘‰ To fix these, we use **Ensembles** like **Random Forests** and **Gradient Boosted Trees**.\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow of a Decision Tree**\n",
    "\n",
    "1. Start with the root node (entire dataset).\n",
    "2. Choose the **best feature** to split on (using impurity measure).\n",
    "3. Split the dataset into subsets.\n",
    "4. Repeat recursively for each subset.\n",
    "5. Stop when:\n",
    "\n",
    "   * All points belong to the same class (pure leaf).\n",
    "   * Max depth reached.\n",
    "   * Minimum number of samples per leaf reached.\n",
    "\n",
    "---\n",
    "\n",
    "**Example Use Cases**\n",
    "\n",
    "* **Classification:** Spam detection, disease diagnosis, loan approval.\n",
    "* **Regression:** Predicting house prices, sales forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "A **Decision Tree** is like repeatedly asking **â€œthe best questionâ€** until you reach a clear decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab716f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
