{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0923576",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bc161",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "## **1. Problem Understanding & Data Preparation**\n",
    "\n",
    "1. **Define the task:**\n",
    "\n",
    "   * Classification → predict class labels\n",
    "   * Regression → predict continuous values\n",
    "\n",
    "2. **Collect & clean data:**\n",
    "\n",
    "   * Handle missing values (RF can sometimes handle missing values internally)\n",
    "   * Encode categorical features if needed\n",
    "\n",
    "3. **Split dataset:**\n",
    "\n",
    "   * Usually into **train and test sets**\n",
    "   * Optional: cross-validation for hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Feature Selection (Optional)**\n",
    "\n",
    "* Random Forest is robust to irrelevant features because it **selects random subsets of features** at each split.\n",
    "* Still, removing completely useless features can improve efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Bootstrap Sampling (Bagging)**\n",
    "\n",
    "* For each tree in the forest:\n",
    "\n",
    "  1. Randomly sample **N observations with replacement** from the training set.\n",
    "  2. This sample is called a **bootstrapped dataset**.\n",
    "\n",
    "**Effect:** Each tree sees slightly different data → introduces diversity among trees.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Tree Building (Individual Decision Trees)**\n",
    "\n",
    "* For each tree:\n",
    "\n",
    "  1. Start at the root node.\n",
    "  2. At each split, **select a random subset of features**.\n",
    "  3. Choose the best split based on:\n",
    "\n",
    "     * **Regression:** Variance reduction / MSE\n",
    "     * **Classification:** Gini impurity or entropy\n",
    "  4. Repeat recursively until stopping criteria:\n",
    "\n",
    "     * Maximum depth reached (`max_depth`)\n",
    "     * Minimum samples per leaf (`min_samples_leaf`)\n",
    "     * All leaves are pure\n",
    "\n",
    "**Note:** Trees are usually grown deep → individual trees may overfit.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Repeat for All Trees**\n",
    "\n",
    "* Build **n\\_estimators** trees independently using their own bootstrapped samples.\n",
    "* Each tree is slightly different due to **data sampling + random feature selection**.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Aggregation of Predictions (Ensembling)**\n",
    "\n",
    "* After all trees are trained, make predictions on **new/unseen data**:\n",
    "\n",
    "  * **Regression:** Average predictions from all trees.\n",
    "  * **Classification:** Majority vote across all trees.\n",
    "\n",
    "**Effect:**\n",
    "\n",
    "* Reduces variance → predictions are more stable than a single tree.\n",
    "* Robust to noise → errors from individual trees are averaged out.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Model Evaluation**\n",
    "\n",
    "* Evaluate performance on **test/validation set**:\n",
    "\n",
    "  * **Regression:** R², RMSE, MAE\n",
    "  * **Classification:** Accuracy, Precision, Recall, F1-score, ROC-AUC\n",
    "\n",
    "* Optional: **Out-of-Bag (OOB) Error**\n",
    "\n",
    "  * Since each tree sees only \\~63% of data (bootstrap sampling), remaining 37% can act as a **validation set** → gives an unbiased error estimate without separate test set.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Hyperparameter Tuning**\n",
    "\n",
    "* Common parameters to tune:\n",
    "\n",
    "  * `n_estimators` → number of trees\n",
    "  * `max_depth` → maximum depth of trees\n",
    "  * `min_samples_split` / `min_samples_leaf` → control complexity\n",
    "  * `max_features` → number of features considered at each split\n",
    "\n",
    "* Use **GridSearchCV or RandomizedSearchCV** for optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Feature Importance (Optional)**\n",
    "\n",
    "* Random Forest can compute **feature importance scores**:\n",
    "\n",
    "  * Measures how much each feature reduces impurity across all trees\n",
    "  * Helps in **interpreting the model**\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Deployment**\n",
    "\n",
    "* Once trained and validated, the Random Forest model can be deployed to **predict unseen data**.\n",
    "* Predictions are **aggregated outputs** of all the trees.\n",
    "\n",
    "---\n",
    "\n",
    "### **Workflow Summary Diagram (Mental Picture)**\n",
    "\n",
    "1. **Input Data** → clean & split →\n",
    "2. **Bootstrap Samples** → multiple trees trained independently →\n",
    "3. **Random Feature Selection at Each Split** → tree grows deep →\n",
    "4. **Aggregate Predictions** → final ensemble output →\n",
    "5. **Evaluate & Tune Hyperparameters** → deploy model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf9d6d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
