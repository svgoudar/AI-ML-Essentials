{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3a9419",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd33a16",
   "metadata": {},
   "source": [
    "## Hyper-paramter Tuning Intiution\n",
    "\n",
    "Perfect üëç let‚Äôs go deeper into the **mathematical intuition behind the key hyperparameters of SVC**.\n",
    "\n",
    "We‚Äôll focus on the three most important ones: **C, Œ≥ (gamma), and kernel**.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective Function of SVC\n",
    "\n",
    "The **primal optimization problem** of SVM is:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^N \\xi_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i (w^T \\phi(x_i) + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $w$: weight vector\n",
    "* $b$: bias term\n",
    "* $\\xi_i$: slack variables (allow misclassifications)\n",
    "* $C$: **regularization parameter** (controls penalty for misclassifications)\n",
    "* $\\phi(x)$: feature mapping (depends on kernel)\n",
    "\n",
    "---\n",
    "\n",
    "### Role of **C** (Regularization)\n",
    "\n",
    "From the objective function:\n",
    "\n",
    "* The term $\\frac{1}{2} \\|w\\|^2$ ‚Üí tries to maximize margin.\n",
    "* The term $C \\sum \\xi_i$ ‚Üí penalizes misclassifications.\n",
    "\n",
    "üëâ Intuition:\n",
    "\n",
    "* **Small C** ‚Üí margin maximization dominates (tolerates some errors).\n",
    "\n",
    "  * Simpler decision boundary.\n",
    "  * Prevents overfitting.\n",
    "* **Large C** ‚Üí error penalty dominates (forces correct classification of training data).\n",
    "\n",
    "  * Narrow margin.\n",
    "  * Risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Role of **Œ≥ (Gamma)** in RBF Kernel\n",
    "\n",
    "The **RBF kernel** is:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)\n",
    "$$\n",
    "\n",
    "üëâ Interpretation:\n",
    "\n",
    "* If $\\|x_i - x_j\\|$ is small ‚Üí similarity close to 1.\n",
    "\n",
    "* If $\\|x_i - x_j\\|$ is large ‚Üí similarity close to 0.\n",
    "\n",
    "* $\\gamma$ controls the **decay rate** of similarity.\n",
    "\n",
    "* **Small Œ≥:**\n",
    "\n",
    "  * Kernel is smoother, points far apart are still considered similar.\n",
    "  * Leads to a smooth, less complex decision boundary.\n",
    "\n",
    "* **Large Œ≥:**\n",
    "\n",
    "  * Kernel is sharper, only very close neighbors are considered similar.\n",
    "  * Leads to highly complex decision boundary (can overfit).\n",
    "\n",
    "---\n",
    "\n",
    "### Role of **Kernel Choice**\n",
    "\n",
    "The kernel defines $\\phi(x)$, the transformation of data:\n",
    "\n",
    "* **Linear kernel:**\n",
    "\n",
    "  $$\n",
    "  K(x_i, x_j) = x_i^T x_j\n",
    "  $$\n",
    "\n",
    "  ‚Üí Works well if data is linearly separable.\n",
    "\n",
    "* **Polynomial kernel:**\n",
    "\n",
    "  $$\n",
    "  K(x_i, x_j) = (x_i^T x_j + c)^d\n",
    "  $$\n",
    "\n",
    "  ‚Üí Captures polynomial relationships; degree $d$ is a hyperparameter.\n",
    "\n",
    "* **RBF kernel:**\n",
    "\n",
    "  $$\n",
    "  K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)\n",
    "  $$\n",
    "\n",
    "  ‚Üí Very flexible, maps data to infinite-dimensional feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### How C and Œ≥ Interact\n",
    "\n",
    "* **High C + High Œ≥:**\n",
    "\n",
    "  * Very complex model, tries to classify everything correctly.\n",
    "  * Risk of overfitting.\n",
    "\n",
    "* **Low C + Low Œ≥:**\n",
    "\n",
    "  * Very smooth decision boundary, high bias.\n",
    "  * Risk of underfitting.\n",
    "\n",
    "* **Balanced values:**\n",
    "\n",
    "  * Trade-off between margin size, misclassification, and flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "### Decision Function\n",
    "\n",
    "The final decision function of SVC is:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}\\Big(\\sum_{i=1}^N \\alpha_i y_i K(x_i, x) + b\\Big)\n",
    "$$\n",
    "\n",
    "* $\\alpha_i$: learned weights (nonzero only for support vectors).\n",
    "* $K(x_i, x)$: similarity function (depends on Œ≥ and kernel).\n",
    "* $C$: controls how many support vectors exist (larger C ‚Üí more support vectors).\n",
    "\n",
    "---\n",
    "\n",
    "**Summary (Mathematical Intuition):**\n",
    "\n",
    "* **C** ‚Üí controls penalty on misclassified points ($\\xi_i$).\n",
    "* **Œ≥** ‚Üí controls how similarity decays in RBF kernel.\n",
    "* **Kernel** ‚Üí defines feature space transformation.\n",
    "* Together, they shape the **decision boundary**: wide vs narrow, smooth vs complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ddfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Base SVC model\n",
    "svc = SVC()\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=svc,\n",
    "    param_grid=param_grid,\n",
    "    refit=True,        # keep best model\n",
    "    cv=5,              # 5-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1          # use all CPUs\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Use best model for predictions\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
