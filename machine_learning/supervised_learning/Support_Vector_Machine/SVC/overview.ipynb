{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bda911",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bff0f8",
   "metadata": {},
   "source": [
    "## SVC\n",
    "\n",
    "Support Vector Classifier (SVC) is the classification version of Support Vector Machines.\n",
    "\n",
    "**Core idea:**\n",
    "\n",
    "* Data points are separated by a hyperplane.\n",
    "* The algorithm chooses the hyperplane that **maximizes the margin** (distance to nearest data points).\n",
    "* The nearest data points on each side are the **support vectors**.\n",
    "\n",
    "**Mathematical form:**\n",
    "For input $x$, prediction is:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}(w^T x + b)\n",
    "$$\n",
    "\n",
    "where $w$ and $b$ define the separating hyperplane.\n",
    "\n",
    "**Soft margin:**\n",
    "\n",
    "* Perfect separation may not be possible.\n",
    "* Slack variables $\\xi_i$ allow misclassifications.\n",
    "* Trade-off controlled by **regularization parameter $C$**:\n",
    "\n",
    "  * Large $C$: less tolerance for errors (narrow margin).\n",
    "  * Small $C$: more tolerance, wider margin, better generalization.\n",
    "\n",
    "**Non-linear boundaries:**\n",
    "\n",
    "* Kernel trick replaces dot product with kernel function $K(x_i, x)$.\n",
    "* Common kernels: linear, polynomial, RBF (Gaussian).\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* SVC = SVM applied to classification.\n",
    "* Works well with high-dimensional data.\n",
    "* Requires scaling of features.\n",
    "* Sensitive to $C$, kernel, and kernel parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=6)\n",
    "\n",
    "# Fit SVM classifier\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot data points\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=50, edgecolors='k')\n",
    "\n",
    "# Plot decision boundary\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.7, linestyles=['--', '-', '--'])\n",
    "\n",
    "# Plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=120, linewidth=2, facecolors='none', edgecolors='k', label='Support Vectors')\n",
    "\n",
    "plt.title('SVM Decision Boundary and Support Vectors')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e62332",
   "metadata": {},
   "source": [
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
