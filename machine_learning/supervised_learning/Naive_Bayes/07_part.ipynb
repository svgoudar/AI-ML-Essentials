{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47295c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce272ea",
   "metadata": {},
   "source": [
    "# Variants of NB\n",
    "\n",
    "## 1. **Gaussian Naïve Bayes**\n",
    "\n",
    "* Assumes that **continuous features** follow a **normal (Gaussian) distribution** within each class.\n",
    "* Likelihood:\n",
    "\n",
    "  $$\n",
    "  P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)\n",
    "  $$\n",
    "\n",
    "  * $\\mu_{y,i}$: mean of feature $i$ for class $y$\n",
    "  * $\\sigma_{y,i}^2$: variance\n",
    "* **Use case**: Continuous numeric data (e.g., medical measurements, sensor data).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Multinomial Naïve Bayes**\n",
    "\n",
    "* Assumes features are **discrete counts** (e.g., word counts in text).\n",
    "* Likelihood:\n",
    "\n",
    "  $$\n",
    "  P(x \\mid y) = \\frac{( \\sum_i x_i )!}{\\prod_i x_i!} \\prod_{i=1}^n P(x_i \\mid y)^{x_i}\n",
    "  $$\n",
    "* **Use case**: Text classification (spam detection, sentiment analysis) with **bag-of-words** or **TF-IDF counts**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Bernoulli Naïve Bayes**\n",
    "\n",
    "* Features are **binary** (0 or 1: present/absent).\n",
    "* Likelihood:\n",
    "\n",
    "  $$\n",
    "  P(x_i \\mid y) = P_{i,y}^{x_i} (1-P_{i,y})^{1-x_i}\n",
    "  $$\n",
    "* **Use case**: Text classification where only word presence matters (not frequency).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Complement Naïve Bayes**\n",
    "\n",
    "* A variation of Multinomial NB designed for **imbalanced datasets**.\n",
    "* Uses **complement of each class** to estimate likelihoods, reducing bias toward majority class.\n",
    "* **Use case**: Text classification with severe class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Categorical Naïve Bayes** (aka Multivariate Bernoulli NB in sklearn ≥0.20)\n",
    "\n",
    "* Handles **categorical features with multiple categories** (not just binary).\n",
    "* Uses category probabilities per feature.\n",
    "* **Use case**: Datasets with categorical variables (e.g., “color = red/green/blue”).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Kernel Density Estimation (KDE) Naïve Bayes**\n",
    "\n",
    "* Instead of assuming Gaussian distribution, estimates feature likelihoods using **non-parametric density estimation**.\n",
    "* More flexible but computationally heavier.\n",
    "* **Use case**: Continuous features that are not Gaussian-shaped.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Variant            | Data Type   | Distribution Assumption        | Common Use Case              |\n",
    "| ------------------ | ----------- | ------------------------------ | ---------------------------- |\n",
    "| **Gaussian NB**    | Continuous  | Normal (Gaussian)              | Medical, sensor data         |\n",
    "| **Multinomial NB** | Count-based | Multinomial                    | Text (word counts, TF-IDF)   |\n",
    "| **Bernoulli NB**   | Binary      | Bernoulli                      | Text (word presence/absence) |\n",
    "| **Complement NB**  | Count-based | Multinomial (complement class) | Imbalanced text datasets     |\n",
    "| **Categorical NB** | Categorical | Categorical distribution       | Tabular categorical data     |\n",
    "| **KDE NB**         | Continuous  | Non-parametric (KDE)           | Complex continuous features  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791c5497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GaussianNB': 0.9777777777777777,\n",
       " 'MultinomialNB': 1.0,\n",
       " 'BernoulliNB': 1.0,\n",
       " 'ComplementNB': 1.0,\n",
       " 'CategoricalNB': 0.85}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration of Naive Bayes Variants in scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, fetch_20newsgroups, make_classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, CategoricalNB, ComplementNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. Gaussian Naive Bayes (Iris dataset)\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "results['GaussianNB'] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 2. Multinomial Naive Bayes (text classification)\n",
    "docs = [\"I love Python\", \"Python is great for machine learning\", \"I dislike bugs\", \"Bugs are annoying\"]\n",
    "labels = [1, 1, 0, 0]  # 1=positive, 0=negative\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "results['MultinomialNB'] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 3. Bernoulli Naive Bayes (binary text presence/absence)\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "results['BernoulliNB'] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 4. Complement Naive Bayes (good for imbalanced data)\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X_train, y_train)\n",
    "y_pred = cnb.predict(X_test)\n",
    "results['ComplementNB'] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 5. Categorical Naive Bayes (on synthetic categorical dataset)\n",
    "# Generate categorical-like features (values 0-3)\n",
    "X, y = make_classification(n_samples=200, n_features=3, n_informative=3, n_redundant=0, random_state=42)\n",
    "X = np.digitize(X, bins=[-1, 0, 1, 2])  # discretize features into bins (categories)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "catnb = CategoricalNB()\n",
    "catnb.fit(X_train, y_train)\n",
    "y_pred = catnb.predict(X_test)\n",
    "results['CategoricalNB'] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90cdc43",
   "metadata": {},
   "source": [
    "| Variant           | Use Case                                           | Dataset Used                  | Accuracy  |\n",
    "| ----------------- | -------------------------------------------------- | ----------------------------- | --------- |\n",
    "| **GaussianNB**    | Continuous features (normally distributed)         | Iris dataset                  | **97.8%** |\n",
    "| **MultinomialNB** | Discrete counts (text classification, word counts) | Small text dataset            | **100%**  |\n",
    "| **BernoulliNB**   | Binary features (word presence/absence)            | Same text dataset             | **100%**  |\n",
    "| **ComplementNB**  | Handles imbalanced text data better                | Same text dataset             | **100%**  |\n",
    "| **CategoricalNB** | Purely categorical data                            | Synthetic categorical dataset | **85%**   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
