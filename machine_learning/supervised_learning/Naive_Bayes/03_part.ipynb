{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a64d16",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec9046",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Why do we need probability in ML?\n",
    "\n",
    "Machine learning is about **making predictions under uncertainty**. For example:\n",
    "\n",
    "* Email classification: ‚ÄúIs this spam or not?‚Äù\n",
    "* Medical diagnosis: ‚ÄúDoes the patient have disease X?‚Äù\n",
    "* Sentiment analysis: ‚ÄúIs the review positive, neutral, or negative?‚Äù\n",
    "\n",
    "We want to estimate the **probability of each class** given the observed features.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "P(y \\mid X) \\quad \\text{where } X = (x_1, x_2, \\dots, x_n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Independent vs Dependent events\n",
    "\n",
    "* **Independent events**: rolling a dice ‚Äî the outcome of roll 1 does not affect roll 2.\n",
    "\n",
    "  $$\n",
    "  P(A \\cap B) = P(A) \\cdot P(B)\n",
    "  $$\n",
    "\n",
    "* **Dependent events**: drawing marbles without replacement. The probability of the second draw **changes** depending on the first.\n",
    "\n",
    "  $$\n",
    "  P(A \\cap B) = P(A) \\cdot P(B \\mid A)\n",
    "  $$\n",
    "\n",
    "üëâ This dependency idea is the foundation for **conditional probability**.\n",
    "\n",
    "---\n",
    "\n",
    "## Conditional Probability\n",
    "\n",
    "Definition:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Intuition: ‚ÄúWhat is the probability of $A$ happening if I already know that $B$ happened?‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "Using conditional probability both ways:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $P(A)$ = prior (belief about $A$ before seeing data)\n",
    "* $P(B \\mid A)$ = likelihood (how compatible evidence $B$ is with $A$)\n",
    "* $P(B)$ = marginal probability (normalizing constant)\n",
    "* $P(A \\mid B)$ = posterior (updated belief after seeing evidence)\n",
    "\n",
    "This is the **Bayesian update rule**.\n",
    "\n",
    "---\n",
    "\n",
    "## Applying Bayes to ML\n",
    "\n",
    "We want:\n",
    "\n",
    "$$\n",
    "P(y \\mid x_1, x_2, \\dots, x_n)\n",
    "$$\n",
    "\n",
    "By Bayes theorem:\n",
    "\n",
    "$$\n",
    "P(y \\mid X) = \\frac{P(y) \\cdot P(x_1, x_2, \\dots, x_n \\mid y)}{P(x_1, x_2, \\dots, x_n)}\n",
    "$$\n",
    "\n",
    "The denominator is **the same for all classes**, so we only care about the numerator.\n",
    "\n",
    "---\n",
    "\n",
    "## The Na√Øve Assumption\n",
    "\n",
    "Problem: computing $P(x_1, x_2, \\dots, x_n \\mid y)$ is complex because features may be dependent.\n",
    "\n",
    "**Na√Øve Bayes assumes conditional independence:**\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\dots, x_n \\mid y) \\approx \\prod_{i=1}^n P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "This gives:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y \\; P(y) \\prod_{i=1}^n P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "That‚Äôs the **Na√Øve Bayes classifier**.\n",
    "\n",
    "---\n",
    "\n",
    "## Interpreting the Formula\n",
    "\n",
    "* $P(y)$: Prior probability of the class\n",
    "* $P(x_i \\mid y)$: Likelihood of feature $x_i$ under class $y$\n",
    "* $\\prod_i$: Combine all feature evidence\n",
    "* $\\arg\\max_y$: Choose the class with the highest posterior probability\n",
    "\n",
    "---\n",
    "\n",
    "## Worked Example (Play Tennis üå§Ô∏èüéæ)\n",
    "\n",
    "Dataset (simplified):\n",
    "\n",
    "* Features: **Outlook, Temperature**\n",
    "* Target: **Play = Yes/No**\n",
    "\n",
    "Say the test instance is:\n",
    "`Outlook = Sunny, Temperature = Hot`\n",
    "\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "P(\\text{Yes} \\mid Sunny, Hot) \\propto P(\\text{Yes}) \\cdot P(Sunny \\mid Yes) \\cdot P(Hot \\mid Yes)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No} \\mid Sunny, Hot) \\propto P(\\text{No}) \\cdot P(Sunny \\mid No) \\cdot P(Hot \\mid No)\n",
    "$$\n",
    "\n",
    "* From counts in dataset:\n",
    "\n",
    "  * $P(\\text{Yes}) = \\tfrac{9}{14}$, $P(\\text{No}) = \\tfrac{5}{14}$\n",
    "  * $P(Sunny \\mid Yes) = \\tfrac{2}{9}$, $P(Hot \\mid Yes) = \\tfrac{2}{9}$\n",
    "  * $P(Sunny \\mid No) = \\tfrac{3}{5}$, $P(Hot \\mid No) = \\tfrac{2}{5}$\n",
    "\n",
    "Plug in:\n",
    "\n",
    "$$\n",
    "P(\\text{Yes} \\mid Sunny, Hot) \\propto \\tfrac{9}{14} \\cdot \\tfrac{2}{9} \\cdot \\tfrac{2}{9} \\approx 0.031\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No} \\mid Sunny, Hot) \\propto \\tfrac{5}{14} \\cdot \\tfrac{3}{5} \\cdot \\tfrac{2}{5} \\approx 0.085\n",
    "$$\n",
    "\n",
    "Normalize:\n",
    "\n",
    "* $P(\\text{Yes} \\mid Sunny, Hot) = 0.27$\n",
    "* $P(\\text{No} \\mid Sunny, Hot) = 0.73$\n",
    "\n",
    "‚úÖ Prediction: **No (won‚Äôt play tennis)**\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "* **Na√Øve Bayes = Bayes theorem + independence assumption**\n",
    "* Works well when features are weakly correlated\n",
    "* Very fast, good for text classification (spam filtering, sentiment analysis)\n",
    "* Outputs **probabilities**, not just class labels"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
