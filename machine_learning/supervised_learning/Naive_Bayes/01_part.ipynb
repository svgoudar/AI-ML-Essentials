{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f541223c",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe16eb2",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Feature Independence\n",
    "\n",
    "* **Assumption:** Features are independent given the class.\n",
    "* **Reality:** Features are often correlated.\n",
    "\n",
    "  * Example: In spam emails, the words *“lottery”* and *“win”* appear together frequently. Independence is false.\n",
    "* **Why it still works:** The product of probabilities often still yields a **reasonable ranking of classes**, even if absolute probabilities are wrong. Classification only needs the **highest posterior**, not exact values.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Equal Contribution of Features\n",
    "\n",
    "* **Assumption:** Each feature contributes equally to the prediction.\n",
    "* **Reality:** Some features dominate.\n",
    "\n",
    "  * Example: In medical diagnosis, *“tumor detected in MRI”* is far stronger than *“slight fever”*.\n",
    "* **Why it still works:** In high-dimensional settings (like text classification), many weak but independent-ish signals combine to give strong results.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Distribution of Features\n",
    "\n",
    "* **Assumption:**\n",
    "\n",
    "  * Gaussian NB → features are normally distributed.\n",
    "  * Multinomial NB → word counts follow multinomial distribution.\n",
    "  * Bernoulli NB → features are binary indicators.\n",
    "* **Reality:** Data distributions often deviate.\n",
    "\n",
    "  * Example: Continuous features may be skewed, not Gaussian.\n",
    "* **Why it still works:** As long as the assumed distribution is a rough approximation, the decision boundary can still separate classes effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. No Zero Probability\n",
    "\n",
    "* **Assumption:** Every feature-class combination has a nonzero probability.\n",
    "* **Reality:** Some words/values may not appear in training.\n",
    "\n",
    "  * Example: If “Bitcoin” never appeared in spam training data, then $P(\\text{Bitcoin}|\\text{spam}) = 0$.\n",
    "* **Why it still works:** With **Laplace (add-one) smoothing**, we avoid zeros and keep predictions stable.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insight:**\n",
    "Even though independence and distribution assumptions are false in practice, Naive Bayes still works well when:\n",
    "\n",
    "* Features provide **enough weak evidence**.\n",
    "* The goal is classification, not perfect probability estimation.\n",
    "* Data is **high-dimensional and sparse** (like text).\n",
    "\n",
    "❌ It fails when:\n",
    "\n",
    "* Strong feature correlations matter.\n",
    "* Precise probability estimates are required (not just classification).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
