{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c921d8af",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae538db",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "Naive Bayes is a **\"vote counting\" machine** powered by probability.\n",
    "\n",
    "* You look at the features of a new sample (like words in an email).\n",
    "* For each possible class (e.g., *spam* vs *ham*), you ask:\n",
    "  *“If this sample belonged to this class, how likely would I see these features?”*\n",
    "* Multiply those likelihoods by how common that class is overall (*prior*).\n",
    "* Whichever class gives the highest probability wins.\n",
    "\n",
    "👉 Even if features are correlated, it still works because it **compares relative evidence**, not exact truth.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Intuition\n",
    "\n",
    "Bayes’ theorem:\n",
    "\n",
    "$$\n",
    "P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "We only care about the class with the **maximum posterior probability**, so denominator $P(X)$ can be ignored:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y P(X|y) \\cdot P(y)\n",
    "$$\n",
    "\n",
    "Now apply the **naive assumption** (feature independence):\n",
    "\n",
    "$$\n",
    "P(X|y) = P(x_1, x_2, …, x_n | y) \\approx \\prod_{i=1}^n P(x_i | y)\n",
    "$$\n",
    "\n",
    "So the classifier becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^n P(x_i | y)\n",
    "$$\n",
    "\n",
    "* $P(y)$ → how frequent the class is (prior).\n",
    "* $P(x_i|y)$ → how often feature $x_i$ appears in class $y$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Example Intuition with Math**\n",
    "\n",
    "Suppose we want to classify an email with the words: *“win lottery”*.\n",
    "\n",
    "From training data:\n",
    "\n",
    "* $P(\\text{spam}) = 0.4$, $P(\\text{ham}) = 0.6$.\n",
    "* $P(\\text{win}|\\text{spam}) = 0.8$, $P(\\text{win}|\\text{ham}) = 0.1$.\n",
    "* $P(\\text{lottery}|\\text{spam}) = 0.7$, $P(\\text{lottery}|\\text{ham}) = 0.05$.\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "P(\\text{spam}|\\text{“win lottery”}) \\propto 0.4 \\cdot 0.8 \\cdot 0.7 = 0.224\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{ham}|\\text{“win lottery”}) \\propto 0.6 \\cdot 0.1 \\cdot 0.05 = 0.003\n",
    "$$\n",
    "\n",
    "📌 Prediction: **Spam**, because 0.224 > 0.003.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Why it works despite being naive**\n",
    "\n",
    "* Even if \"win\" and \"lottery\" are correlated, multiplying probabilities still boosts the correct class compared to the wrong one.\n",
    "* The absolute values may be wrong, but **relative comparison is good enough for classification**.\n",
    "\n",
    "---\n",
    "\n",
    "⚡ **Summary:**\n",
    "\n",
    "* **Intuition:** Pick the class that best explains the observed features.\n",
    "* **Math intuition:** Bayes’ theorem + independence assumption → product of simple probabilities.\n",
    "* **Outcome:** Fast, effective classifier, especially for text/NLP.\n",
    "\n",
    "Would you like me to also make a **visual probability tree diagram** to show the intuition graphically?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
