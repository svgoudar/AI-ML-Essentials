{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f205a8",
   "metadata": {},
   "source": [
    " ```{contents}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272655ed",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost (**Extreme Gradient Boosting**) is an **optimized distributed gradient boosting library** designed for efficiency, scalability, and model performance. It is one of the most widely used ML algorithms in competitions (e.g., Kaggle) and industry because it consistently delivers **state-of-the-art results** on structured/tabular data.\n",
    "\n",
    "\n",
    "## 1. How XGBoost Works\n",
    "\n",
    "At its core, XGBoost is an **ensemble method** that builds multiple decision trees in sequence. Each new tree corrects the mistakes of the previous ones by focusing on the residual errors.\n",
    "\n",
    "* Start with an initial prediction (often the mean for regression, log-odds for classification).\n",
    "* At each iteration, compute **gradients (errors)** and optionally **Hessians (curvature information)**.\n",
    "* Fit a new decision tree to these gradients.\n",
    "* Update the predictions by adding the contribution of the new tree, scaled by the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Objective Function\n",
    "\n",
    "XGBoost optimizes a **regularized objective function**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $l$ = loss function (e.g., squared error, log loss).\n",
    "* $\\hat{y}_i$ = predicted value for sample $i$.\n",
    "* $\\Omega(f_k) = \\gamma T + \\tfrac{1}{2}\\lambda \\|w\\|^2$ is the regularization term for tree $f_k$, where:\n",
    "\n",
    "  * $T$ = number of leaves.\n",
    "  * $w$ = leaf weights.\n",
    "  * $\\gamma, \\lambda$ = regularization parameters.\n",
    "\n",
    "This penalizes overly complex trees, reducing **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key Innovations in XGBoost\n",
    "\n",
    "### Algorithmic\n",
    "\n",
    "* **Second-order optimization**: Uses both gradient and Hessian for faster convergence.\n",
    "* **Regularization (L1 & L2)**: Prevents overfitting (unlike vanilla Gradient Boosting).\n",
    "* **Sparsity-aware split finding**: Handles missing values automatically.\n",
    "* **Weighted quantile sketch**: Efficiently finds split points for weighted data.\n",
    "\n",
    "### System-level\n",
    "\n",
    "* **Parallelization**: Builds tree branches in parallel.\n",
    "* **Out-of-core computation**: Processes datasets too large for memory.\n",
    "* **Cache optimization**: Improves CPU utilization.\n",
    "* **Distributed training**: Works across multiple machines.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hyperparameters\n",
    "\n",
    "* **Learning rate ($\\eta$)**: Shrinks contribution of each tree.\n",
    "* **n\\_estimators**: Number of boosting rounds (trees).\n",
    "* **max\\_depth**: Maximum depth of each tree. Controls model complexity.\n",
    "* **subsample**: Fraction of training samples used per tree.\n",
    "* **colsample\\_bytree**: Fraction of features used per tree.\n",
    "* **lambda (L2)**, **alpha (L1)**: Regularization strength.\n",
    "* **gamma**: Minimum loss reduction required to make a split.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Strengths\n",
    "\n",
    "* High accuracy.\n",
    "* Robust to missing data.\n",
    "* Flexible (supports regression, classification, ranking).\n",
    "* Feature importance ranking.\n",
    "* Scalable to millions of samples.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Limitations\n",
    "\n",
    "* Slower to train compared to linear models.\n",
    "* Can overfit if not tuned properly.\n",
    "* Less interpretable than simpler models.\n",
    "* Not ideal for unstructured data (images, text) â†’ deep learning is better.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Use Cases\n",
    "\n",
    "* **Classification**: fraud detection, churn prediction, medical diagnosis.\n",
    "* **Regression**: house price prediction, risk modeling.\n",
    "* **Ranking**: search engines, recommender systems.\n",
    "* **Feature selection**: via feature importance.\n",
    "\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
