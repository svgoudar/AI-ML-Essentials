{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caa17d5",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assumptions\n",
    "\n",
    "## 1. **Weak Learners Should Perform Slightly Better than Random**\n",
    "\n",
    "* The base learners (often shallow decision trees, called *decision stumps*) should have an accuracy just above random guessing:\n",
    "\n",
    "  * For binary classification → slightly better than 50% accuracy.\n",
    "* Boosting works by combining many weak rules, so if each base learner is no better than chance, AdaBoost fails.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Additive Model of Errors**\n",
    "\n",
    "* AdaBoost assumes that errors from weak learners can be combined and corrected sequentially.\n",
    "* Misclassified samples get higher weights → future weak learners focus on them.\n",
    "* This assumes misclassification can be *reduced step-by-step* instead of being random noise.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Data is (Relatively) Clean**\n",
    "\n",
    "* AdaBoost is **sensitive to noisy data and outliers**, because:\n",
    "\n",
    "  * Misclassified points get higher weights repeatedly.\n",
    "  * Outliers that are impossible to classify correctly receive disproportionate focus.\n",
    "* Implicit assumption: dataset has low noise and few extreme outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Feature Independence Isn’t Required (Unlike Naive Bayes)**\n",
    "\n",
    "* AdaBoost does **not assume independence of features**.\n",
    "* It can handle correlated features, but redundant features may make training inefficient.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Sufficient Number of Weak Learners**\n",
    "\n",
    "* Boosting assumes that with **enough iterations (weak learners)**, the combined strong learner will converge to a low-error classifier.\n",
    "* Too few learners → underfitting; too many learners → risk of overfitting (though AdaBoost is surprisingly resistant to overfitting on clean data).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Weak Learners Should Be Simple**\n",
    "\n",
    "* Base learners should be simple (e.g., decision stumps or very shallow trees).\n",
    "* If base learners are too complex (deep trees), boosting loses meaning (becomes just an ensemble of strong models).\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "AdaBoost works best under these assumptions:\n",
    "\n",
    "* Weak learners perform slightly better than chance.\n",
    "* Errors can be sequentially corrected.\n",
    "* Data is relatively clean (not dominated by noise or outliers).\n",
    "* Enough learners are combined to reduce bias.\n",
    "* Base learners are simple and diverse.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
