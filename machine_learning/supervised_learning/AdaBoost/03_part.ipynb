{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caa17d5",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Workflow**\n",
    "\n",
    "1. **Initialize Sample Weights**\n",
    "\n",
    "   * All data points start with equal weights (e.g., 1/N for N points).\n",
    "\n",
    "2. **Train Weak Learner (Decision Tree Stump)**\n",
    "\n",
    "   * Select the best stump based on metrics like Entropy or Gini Impurity.\n",
    "\n",
    "3. **Calculate Total Error (TE)**\n",
    "\n",
    "   * TE = fraction of misclassified points by the stump.\n",
    "\n",
    "4. **Compute Stump Weight (α)**\n",
    "\n",
    "   * $\\alpha = \\frac{1}{2} \\ln\\frac{1 - TE}{TE}$\n",
    "   * Higher α → better performing weak learner; α is the weight in the final prediction.\n",
    "\n",
    "5. **Update Data Point Weights**\n",
    "\n",
    "   * Correctly classified points → **decrease weight**: multiply by $e^{-α}$\n",
    "   * Misclassified points → **increase weight**: multiply by $e^{α}$\n",
    "\n",
    "6. **Normalize Weights & Assign Bins**\n",
    "\n",
    "   * Weights normalized to sum ≈ 1; assigned proportional bins (misclassified points get larger bins).\n",
    "\n",
    "7. **Select Data Points for Next Learner**\n",
    "\n",
    "   * Random values between 0-1 determine selection; misclassified points are more likely to be chosen.\n",
    "\n",
    "8. **Iterate**\n",
    "\n",
    "   * Repeat training, weight updates, and selection for a predefined number of iterations, generating multiple weak learners.\n",
    "\n",
    "9. **Final Prediction**\n",
    "\n",
    "   * Weighted sum of predictions: $f = \\alpha_1 M_1 + \\alpha_2 M_2 + ... + \\alpha_n M_n$\n",
    "   * Class with highest total weighted score is chosen as the final output.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
