{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631596a0",
   "metadata": {},
   "source": [
    "# One-vs-Rest (OVR) Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "## **1. The Problem**\n",
    "\n",
    "* Standard **logistic regression** handles **binary classification**: two classes only.\n",
    "* Many real-world problems are **multi-class** (3 or more classes), e.g., classifying animals as Cat, Dog, or Rabbit.\n",
    "* We need a way to extend logistic regression to handle multiple classes.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. One-vs-Rest (OVR) Strategy**\n",
    "\n",
    "OVR (also called One-vs-All) converts a **multi-class problem into multiple binary classification problems**:\n",
    "\n",
    "1. Suppose there are $K$ classes: $C_1, C_2, ..., C_K$.\n",
    "2. For each class $C_k$, train a **binary logistic regression classifier**:\n",
    "\n",
    "   * Treat $C_k$ as the **positive class** (1).\n",
    "   * Treat all other classes as **negative class** (0).\n",
    "\n",
    "**Example with 3 classes (Cat, Dog, Rabbit):**\n",
    "\n",
    "| Classifier | Positive | Negative    |\n",
    "| ---------- | -------- | ----------- |\n",
    "| M1         | Cat      | Dog, Rabbit |\n",
    "| M2         | Dog      | Cat, Rabbit |\n",
    "| M3         | Rabbit   | Cat, Dog    |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Training Phase**\n",
    "\n",
    "* Each binary model $M_k$ is trained **independently**.\n",
    "* Input features remain the same for all models.\n",
    "* Use **one-hot encoding** for outputs:\n",
    "\n",
    "| Class  | One-hot  |\n",
    "| ------ | -------- |\n",
    "| Cat    | \\[1,0,0] |\n",
    "| Dog    | \\[0,1,0] |\n",
    "| Rabbit | \\[0,0,1] |\n",
    "\n",
    "* Each model predicts the probability that a sample belongs to its respective class.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Prediction Phase**\n",
    "\n",
    "For a new data point:\n",
    "\n",
    "1. Pass the input to **all K models**.\n",
    "2. Each model outputs a **probability** that the point belongs to its positive class.\n",
    "3. Example probabilities:\n",
    "\n",
    "| Model | Probability |\n",
    "| ----- | ----------- |\n",
    "| M1    | 0.25        |\n",
    "| M2    | 0.20        |\n",
    "| M3    | 0.55        |\n",
    "\n",
    "4. **Choose the class with the highest probability** → here, **Rabbit (class 3)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Advantages of OVR**\n",
    "\n",
    "* Simple to implement.\n",
    "* Works with any binary classifier (logistic regression, SVM, etc.).\n",
    "* Efficient when the number of classes is not very large.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Disadvantages**\n",
    "\n",
    "* Probabilities from different classifiers may not be **well-calibrated**.\n",
    "* Can be biased if one class is much smaller than the “rest.”\n",
    "* Less accurate than One-vs-One in some cases.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Summary**\n",
    "\n",
    "OVR Logistic Regression works by:\n",
    "\n",
    "1. Splitting a multi-class problem into **K binary problems**.\n",
    "2. Training a separate logistic regression for each class.\n",
    "3. Predicting the class with the **highest probability** across all models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827e3b7",
   "metadata": {},
   "source": [
    "## Example Problem Statement\n",
    "\n",
    "**Problem:**\n",
    "You are building a model to classify types of fruits based on two features:\n",
    "\n",
    "* **f1** = Weight (grams)\n",
    "* **f2** = Color Score (0–10 scale)\n",
    "\n",
    "**Classes:**\n",
    "\n",
    "1. Apple\n",
    "2. Banana\n",
    "3. Cherry\n",
    "\n",
    "**Training Data:**\n",
    "\n",
    "| Fruit  | f1 (Weight) | f2 (Color Score) |\n",
    "| ------ | ----------- | ---------------- |\n",
    "| Apple  | 150         | 8                |\n",
    "| Apple  | 170         | 7                |\n",
    "| Banana | 120         | 4                |\n",
    "| Banana | 130         | 5                |\n",
    "| Cherry | 10          | 9                |\n",
    "| Cherry | 15          | 8                |\n",
    "\n",
    "We want to predict the **fruit type** given `f1` and `f2`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: One-vs-Rest (OVR) Setup**\n",
    "\n",
    "We have **3 classes**, so we create **3 binary classifiers**:\n",
    "\n",
    "1. **M1 (Apple vs Rest):**\n",
    "\n",
    "   * Positive: Apple\n",
    "   * Negative: Banana, Cherry\n",
    "\n",
    "2. **M2 (Banana vs Rest):**\n",
    "\n",
    "   * Positive: Banana\n",
    "   * Negative: Apple, Cherry\n",
    "\n",
    "3. **M3 (Cherry vs Rest):**\n",
    "\n",
    "   * Positive: Cherry\n",
    "   * Negative: Apple, Banana\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: One-hot Encoding of Target**\n",
    "\n",
    "| Fruit  | One-hot (Apple, Banana, Cherry) |\n",
    "| ------ | ------------------------------- |\n",
    "| Apple  | \\[1, 0, 0]                      |\n",
    "| Banana | \\[0, 1, 0]                      |\n",
    "| Cherry | \\[0, 0, 1]                      |\n",
    "\n",
    "* Each classifier uses its corresponding column as the **target**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Training Binary Models**\n",
    "\n",
    "* Each binary logistic regression model is trained independently:\n",
    "\n",
    "  * Input: `[f1, f2]`\n",
    "  * Output: probability of being the **positive class**\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Prediction Example**\n",
    "\n",
    "**Test data:**\n",
    "\n",
    "* f1 = 140, f2 = 6\n",
    "\n",
    "**Step 4a: Predict probabilities using each classifier**\n",
    "\n",
    "| Model | Class  | Probability |\n",
    "| ----- | ------ | ----------- |\n",
    "| M1    | Apple  | 0.4         |\n",
    "| M2    | Banana | 0.35        |\n",
    "| M3    | Cherry | 0.25        |\n",
    "\n",
    "**Step 4b: Choose class with highest probability**\n",
    "\n",
    "* Max probability = 0.4 → **Apple**\n",
    "\n",
    "So the predicted class is **Apple**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Summary**\n",
    "\n",
    "* OVR breaks multi-class classification into **multiple binary logistic regressions**.\n",
    "* Each model outputs a probability for its class.\n",
    "* **Final prediction** = class with **highest probability**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc6d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data: [140   6]\n",
      "Predicted Probabilities: [5.19151260e-01 4.80777962e-01 7.07778904e-05]\n",
      "Predicted Class: Apple\n",
      "\n",
      "Test Data: [12  8]\n",
      "Predicted Probabilities: [3.41751899e-22 6.85793166e-02 9.31420683e-01]\n",
      "Predicted Class: Cherry\n",
      "\n",
      "Test Data: [125   5]\n",
      "Predicted Probabilities: [3.84060828e-03 9.95473944e-01 6.85447453e-04]\n",
      "Predicted Class: Banana\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Sample data (Fruit dataset)\n",
    "X = np.array([\n",
    "    [150, 8],   # Apple\n",
    "    [170, 7],   # Apple\n",
    "    [120, 4],   # Banana\n",
    "    [130, 5],   # Banana\n",
    "    [10, 9],    # Cherry\n",
    "    [15, 8]     # Cherry\n",
    "])\n",
    "\n",
    "y = np.array(['Apple', 'Apple', 'Banana', 'Banana', 'Cherry', 'Cherry'])\n",
    "\n",
    "# Encode labels to integers\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  # Apple=0, Banana=1, Cherry=2\n",
    "\n",
    "# Create OVR Logistic Regression model\n",
    "model = LogisticRegression(multi_class='ovr', solver='lbfgs')\n",
    "model.fit(X, y_encoded)\n",
    "\n",
    "# Test data\n",
    "X_test = np.array([\n",
    "    [140, 6],  # Expected: Apple\n",
    "    [12, 8],   # Expected: Cherry\n",
    "    [125, 5]   # Expected: Banana\n",
    "])\n",
    "\n",
    "# Predict probabilities for each class\n",
    "probs = model.predict_proba(X_test)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predicted labels back to original class names\n",
    "predicted_classes = le.inverse_transform(predictions)\n",
    "\n",
    "# Print results\n",
    "for i, x in enumerate(X_test):\n",
    "    print(f\"Test Data: {x}\")\n",
    "    print(f\"Predicted Probabilities: {probs[i]}\")\n",
    "    print(f\"Predicted Class: {predicted_classes[i]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
