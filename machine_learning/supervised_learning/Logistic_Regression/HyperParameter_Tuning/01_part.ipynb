{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ada2dd",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68f1b0",
   "metadata": {},
   "source": [
    "# Evaluation Metrics \n",
    "\n",
    "## `accuracy_score`\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "* Measures the **overall proportion of correct predictions**.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total predictions}}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = [0, 0, 1, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 1]\n",
    "\n",
    "accuracy_score(y_true, y_pred)  # Output: 0.8\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* 80% of predictions are correct.\n",
    "* **Limitation:** For **imbalanced datasets**, accuracy can be misleading.\n",
    "\n",
    "  * Example: If 90% of samples are class 0, predicting everything as 0 gives 90% accuracy, but the minority class is completely ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## `confusion_matrix`\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "* Shows the **count of true vs predicted labels**.\n",
    "* For binary classification:\n",
    "\n",
    "|             | Predicted 0    | Predicted 1    |\n",
    "| ----------- | -------------- | -------------- |\n",
    "| True 0 (TN) | True Negative  | False Positive |\n",
    "| True 1 (TP) | False Negative | True Positive  |\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [0, 0, 1, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 1]\n",
    "\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# Output:\n",
    "# [[2 0]\n",
    "#  [1 2]]\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* TN = 2 → True 0 predicted correctly\n",
    "* FP = 0 → No 0 predicted incorrectly\n",
    "* FN = 1 → One 1 predicted incorrectly\n",
    "* TP = 2 → Two 1 predicted correctly\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* Helps visualize **errors by class**\n",
    "* Essential for **imbalanced datasets**, as accuracy alone may be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "## `classification_report`\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "* Provides **precision, recall, F1-score, and support** for each class.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = [0, 0, 1, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 1]\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.67      1.00      0.80         2\n",
    "           1       1.00      0.67      0.80         3\n",
    "\n",
    "    accuracy                           0.80         5\n",
    "   macro avg       0.83      0.83      0.80         5\n",
    "weighted avg       0.87      0.80      0.80         5\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "| Metric        | Meaning                                                                                                       |\n",
    "| ------------- | ------------------------------------------------------------------------------------------------------------- |\n",
    "| **Precision** | Out of all predicted as class X, how many were actually X. <br> High precision → few false positives.         |\n",
    "| **Recall**    | Out of all actual class X samples, how many were correctly predicted. <br> High recall → few false negatives. |\n",
    "| **F1-score**  | Harmonic mean of precision and recall. Balances the two.                                                      |\n",
    "| **Support**   | Number of true samples for each class.                                                                        |\n",
    "\n",
    "**Imbalanced datasets:**\n",
    "\n",
    "* F1-score is more informative than accuracy.\n",
    "* Weighted or macro averages help summarize overall performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table for Quick Reference**\n",
    "\n",
    "| Metric                  | Best Used For                         | Interpretation in Imbalanced Data           |\n",
    "| ----------------------- | ------------------------------------- | ------------------------------------------- |\n",
    "| `accuracy_score`        | Overall correctness                   | Can be misleading if classes are imbalanced |\n",
    "| `confusion_matrix`      | Counts of TP, TN, FP, FN              | Shows where the model is failing            |\n",
    "| `classification_report` | Precision, Recall, F1-score per class | Gives balanced evaluation across classes    |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
