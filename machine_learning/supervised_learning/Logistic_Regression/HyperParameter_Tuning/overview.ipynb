{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98edc681",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c58c6f",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "##  What are Hyperparameters\n",
    "\n",
    "* In machine learning, **hyperparameters** are settings you define **before training** a model.\n",
    "* They are **not learned from data** (unlike model weights/coefficients).\n",
    "* Proper tuning of hyperparameters can **improve model performance** and **prevent overfitting/underfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Hyperparameters in Logistic Regression\n",
    "\n",
    "1. **Regularization Parameter (C)**\n",
    "\n",
    "* **Definition:** Controls the strength of regularization (penalty for large coefficients).\n",
    "* In scikit-learn, `C` is the **inverse of regularization strength**:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{\\lambda}\n",
    "$$\n",
    "\n",
    "* Smaller `C` → **stronger regularization** → reduces overfitting.\n",
    "* Larger `C` → weaker regularization → may overfit on training data.\n",
    "* Regularization helps prevent the model from giving too much weight to a single feature.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Penalty Type (`penalty`)**\n",
    "\n",
    "* Determines the type of regularization used. Common options:\n",
    "\n",
    "  * `'l2'` → Ridge regularization (squared magnitude of coefficients)\n",
    "  * `'l1'` → Lasso regularization (absolute value of coefficients, encourages sparsity)\n",
    "  * `'elasticnet'` → Combination of L1 and L2 (requires `solver='saga'`)\n",
    "\n",
    "---\n",
    "\n",
    "3. **Solver (`solver`)**\n",
    "\n",
    "* Optimization algorithm used to fit the model. Common options:\n",
    "\n",
    "  * `'lbfgs'` → Good default for small datasets, supports L2\n",
    "  * `'liblinear'` → Good for small datasets, supports L1\n",
    "  * `'saga'` → Supports L1, L2, and elasticnet, scalable for large datasets\n",
    "* Choice of solver may depend on dataset size and penalty type.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Maximum Iterations (`max_iter`)**\n",
    "\n",
    "* Maximum number of iterations for the solver to converge.\n",
    "* Default is usually 100; if the model does not converge, you can **increase this number**.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Class Weight (`class_weight`)**\n",
    "\n",
    "* Used for **imbalanced datasets**.\n",
    "* Options:\n",
    "\n",
    "  * `None` → all classes are treated equally\n",
    "  * `'balanced'` → weights inversely proportional to class frequency\n",
    "\n",
    "---\n",
    "\n",
    "## 3. How to Tune Hyperparameters\n",
    "\n",
    "1. **Grid Search**\n",
    "\n",
    "   * Try all possible combinations of hyperparameters in a predefined grid.\n",
    "   * Example: tune `C = [0.01, 0.1, 1, 10]` and `penalty = ['l1','l2']`.\n",
    "2. **Randomized Search**\n",
    "\n",
    "   * Randomly select hyperparameter combinations, useful when the grid is large.\n",
    "3. **Cross-Validation**\n",
    "\n",
    "   * Split training data into multiple folds.\n",
    "   * Evaluate hyperparameter combinations using **average validation performance**.\n",
    "   * Prevents overfitting to a single train-test split.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Example in Python**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306364d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* **Hyperparameter tuning** improves model performance.\n",
    "* Important hyperparameters in Logistic Regression:\n",
    "\n",
    "  * `C` → regularization strength\n",
    "  * `penalty` → L1/L2\n",
    "  * `solver` → optimization algorithm\n",
    "  * `class_weight` → handle imbalanced datasets\n",
    "* Use **GridSearchCV** or **RandomizedSearchCV** with **cross-validation** to find the best combination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced9d6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "Best Cross-Validated Accuracy: 0.9523809523809523\n",
      "Test Set Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load Iris dataset (multi-class example)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create logistic regression model\n",
    "logreg = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=500)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],          # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],          # L1 or L2 regularization\n",
    "    'class_weight': [None, 'balanced'] # Handle imbalanced data\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Test set evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689df953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores for each fold: [0.61016949 0.64864865 0.7037037  0.74509804 0.6557377 ]\n",
      "Mean F1-score: 0.672671517602299\n",
      "\n",
      "Precision for each fold: [0.47368421 0.75       0.57575758 0.63333333 0.51282051]\n",
      "Mean Precision: 0.5891191264875475\n",
      "\n",
      "Recall for each fold: [0.85714286 0.57142857 0.9047619  0.9047619  0.90909091]\n",
      "Mean Recall: 0.8294372294372293\n"
     ]
    }
   ],
   "source": [
    "# Demonstration: Logistic Regression on an imbalanced dataset using StratifiedKFold\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "# Step 1: Create imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, \n",
    "                           n_informative=3, n_redundant=0, n_classes=2, \n",
    "                           weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Step 2: Create Logistic Regression model with class_weight='balanced'\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "\n",
    "# Step 3: Define StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 4: Evaluate using cross-validation with F1-score, Precision, Recall\n",
    "f1_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(f1_score))\n",
    "precision_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(precision_score))\n",
    "recall_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(recall_score))\n",
    "\n",
    "# Step 5: Print results\n",
    "print(\"F1-scores for each fold:\", f1_scores)\n",
    "print(\"Mean F1-score:\", f1_scores.mean())\n",
    "print(\"\\nPrecision for each fold:\", precision_scores)\n",
    "print(\"Mean Precision:\", precision_scores.mean())\n",
    "print(\"\\nRecall for each fold:\", recall_scores)\n",
    "print(\"Mean Recall:\", recall_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ab5af",
   "metadata": {},
   "source": [
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
