{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f0513b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67941868",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "### 1. Start with a naive model\n",
    "\n",
    "* Begin with a constant prediction (e.g., mean of $y$ in regression, log-odds in classification).\n",
    "* This is a crude first guess.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Look at the errors\n",
    "\n",
    "* Compute the **residuals**:\n",
    "\n",
    "  * In regression: difference between predicted and actual.\n",
    "  * In classification: negative gradient of loss (pseudo-residuals).\n",
    "* These residuals show **what the model has not yet explained**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Fit a weak learner to the residuals\n",
    "\n",
    "* Train a small tree on the residuals.\n",
    "* The tree asks: *“What patterns in the features can explain the mistakes so far?”*\n",
    "* Each learner focuses only on what is left unexplained.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Take a small corrective step\n",
    "\n",
    "* Instead of fully trusting the new learner, we add it in **with a small weight** (learning rate).\n",
    "* This is analogous to taking a small step in the negative gradient direction when minimizing a loss function.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Repeat\n",
    "\n",
    "* Again compute residuals, fit another tree, update predictions.\n",
    "* Each iteration **nudges the model closer to the target**.\n",
    "* Over many iterations, small corrections accumulate into a strong predictor.\n",
    "\n",
    "---\n",
    "\n",
    "### Why “gradient”?\n",
    "\n",
    "* The **loss function** defines an error surface.\n",
    "* Instead of computing residuals directly, gradient boosting fits learners to the **negative gradient of the loss** at each step.\n",
    "* This is identical to doing gradient descent, except instead of updating a parameter vector, we update a function (the predictor).\n",
    "\n",
    "---\n",
    "\n",
    "### Geometric intuition\n",
    "\n",
    "* Imagine trying to descend a mountain blindfolded.\n",
    "* You feel the slope (gradient) under your feet, then take a step in the downhill direction.\n",
    "* In boosting, each weak learner is like a tiny step downhill.\n",
    "* The sequence of learners traces a path toward the minimum of the loss function.\n",
    "\n",
    "---\n",
    "### Example intuition (squared error regression)\n",
    "\n",
    "* Step 1: Predict mean of $y$. Residuals = $y - \\hat{y}$.\n",
    "* Step 2: Fit tree to residuals. It learns structure in what was missed.\n",
    "* Step 3: Add this tree’s prediction (scaled by learning rate).\n",
    "* Step 4: Now residuals are smaller, new tree corrects again.\n",
    "* Step 5: Repeat until residuals are close to random noise.\n",
    "\n",
    "---\n",
    "\n",
    "### Key mental model\n",
    "\n",
    "* Gradient boosting is **“sequential residual correction guided by gradients.”**\n",
    "* Each learner = a correction.\n",
    "* Learning rate = trust in each correction.\n",
    "* Many small corrections = strong final model.\n",
    "\n",
    "\n",
    "Mathematical intuition of **Gradient Boosting**: it is **gradient descent in function space**. Instead of minimizing loss by adjusting numeric parameters, we minimize it by iteratively adding functions.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Intiution\n",
    "\n",
    "### 1. Problem setup\n",
    "\n",
    "We want to approximate a function $F(x)$ that predicts $y$ from input $x$.\n",
    "We define a differentiable loss function:\n",
    "\n",
    "$$\n",
    "L(y, F(x))\n",
    "$$\n",
    "\n",
    "and aim to minimize the total loss over training data:\n",
    "\n",
    "$$\n",
    "\\min_F \\; \\sum_{i=1}^n L(y_i, F(x_i))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Function space gradient descent\n",
    "\n",
    "In standard gradient descent, you update parameters:\n",
    "\n",
    "$$\n",
    "\\theta_{m} = \\theta_{m-1} - \\eta \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "Gradient Boosting does the same, but in **function space**:\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) - \\nu \\cdot \\nabla_F J(F)(x)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $\\nabla_F J(F)(x)$ = derivative of the loss wrt prediction $F(x)$.\n",
    "* This derivative is the **negative gradient** at each data point → called pseudo-residuals.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Pseudo-residuals\n",
    "\n",
    "For each sample $i$, at stage $m$:\n",
    "\n",
    "$$\n",
    "r_{im} = -\\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)}\n",
    "$$\n",
    "\n",
    "These $r_{im}$ are the directions in which the loss decreases most steeply.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Weak learner approximation\n",
    "\n",
    "We cannot directly set $F(x) = F_{m-1}(x) + r_{im}$. Instead, we **fit a weak learner** $h_m(x)$ (small decision tree) to approximate these residuals.\n",
    "\n",
    "$$\n",
    "h_m(x) \\approx r_{im}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Line search\n",
    "\n",
    "To find how much of this weak learner to add, solve for:\n",
    "\n",
    "$$\n",
    "\\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^n L\\big(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Update rule\n",
    "\n",
    "Finally, update the model:\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\nu \\cdot \\gamma_m h_m(x)\n",
    "$$\n",
    "\n",
    "* $\\nu$ = learning rate (step size).\n",
    "* $\\gamma_m$ = optimal multiplier for the learner.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Iterative effect\n",
    "\n",
    "* Each $h_m(x)$ is a **step in function space** in the negative gradient direction.\n",
    "* The sum of many such steps produces a strong predictor:\n",
    "\n",
    "$$\n",
    "F_M(x) = F_0(x) + \\nu \\sum_{m=1}^M \\gamma_m h_m(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Concrete examples of gradients\n",
    "\n",
    "* **Regression (MSE loss)**:\n",
    "\n",
    "  $$\n",
    "  L(y, F(x)) = (y - F(x))^2\n",
    "  $$\n",
    "\n",
    "  Gradient:\n",
    "\n",
    "  $$\n",
    "  r_{im} = y_i - F_{m-1}(x_i) \\quad \\text{(residuals)}\n",
    "  $$\n",
    "\n",
    "* **Classification (logistic loss)**:\n",
    "\n",
    "  $$\n",
    "  L(y, F(x)) = \\log\\big(1 + e^{-yF(x)}\\big), \\quad y \\in \\{-1, +1\\}\n",
    "  $$\n",
    "\n",
    "  Gradient:\n",
    "\n",
    "  $$\n",
    "  r_{im} = \\frac{y_i}{1 + e^{y_i F_{m-1}(x_i)}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition summary\n",
    "\n",
    "* At each stage, compute the gradient of the loss wrt predictions.\n",
    "* Approximate this gradient with a weak learner.\n",
    "* Add it in with small weight.\n",
    "* Repeat → function gradually descends toward minimum of the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd44b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
