{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a645cd5",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning in Linear Regression\n",
    "\n",
    "In **ordinary least squares (OLS) linear regression**, there are actually **no hyperparameters to tune** â€” the coefficients are directly computed by minimizing the sum of squared errors.\n",
    "\n",
    "But when we apply **regularization techniques** (Ridge, Lasso, ElasticNet), hyperparameters come into play.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameters in Linear Regression Variants\n",
    "\n",
    "### Ridge Regression (L2 Regularization)\n",
    "\n",
    "* **Hyperparameter:** `Î±` (sometimes called Î»).\n",
    "* Controls the penalty on large coefficients.\n",
    "\n",
    "  * `Î± = 0` â†’ ordinary least squares (no penalty).\n",
    "  * Large `Î±` â†’ coefficients shrink towards zero but never exactly zero.\n",
    "* Effect: reduces variance, prevents overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Lasso Regression (L1 Regularization)\n",
    "\n",
    "* **Hyperparameter:** `Î±`.\n",
    "* Penalizes absolute values of coefficients.\n",
    "* Large `Î±` â†’ many coefficients become **exactly zero** â†’ feature selection.\n",
    "* Effect: simpler, more interpretable model.\n",
    "\n",
    "---\n",
    "\n",
    "### ElasticNet (Combination of L1 & L2)\n",
    "\n",
    "* **Hyperparameters:**\n",
    "\n",
    "  * `Î±` â†’ overall penalty strength.\n",
    "  * `l1_ratio` â†’ balance between L1 (Lasso) and L2 (Ridge).\n",
    "\n",
    "    * `l1_ratio = 0` â†’ pure Ridge.\n",
    "    * `l1_ratio = 1` â†’ pure Lasso.\n",
    "    * `0 < l1_ratio < 1` â†’ mixture.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Hyperparameter Tuning is Needed?\n",
    "\n",
    "* If `Î±` is too small â†’ model behaves like OLS, may **overfit**.\n",
    "* If `Î±` is too large â†’ coefficients shrink too much, model may **underfit**.\n",
    "* Proper tuning finds a balance.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Tune Hyperparameters?\n",
    "\n",
    "We use **Cross-Validation (CV)** to find the best values:\n",
    "\n",
    "1. **Grid Search CV**\n",
    "\n",
    "   * Try different values of `Î±` (and `l1_ratio` for ElasticNet).\n",
    "   * Example: test `Î± = [0.01, 0.1, 1, 10, 100]`.\n",
    "   * Train model on folds, pick the one with best average CV score.\n",
    "\n",
    "2. **Randomized Search CV**\n",
    "\n",
    "   * Randomly sample hyperparameters from distributions.\n",
    "   * More efficient for large search spaces.\n",
    "\n",
    "3. **Bayesian Optimization** (advanced)\n",
    "\n",
    "   * Uses past evaluation results to choose next hyperparameter values intelligently.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Example (Python, Scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example: Ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key takeaway:**\n",
    "\n",
    "* OLS â†’ no hyperparameters.\n",
    "* Ridge, Lasso, ElasticNet â†’ hyperparameters (`Î±`, `l1_ratio`).\n",
    "* Tune them using **cross-validation** to balance bias and variance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
