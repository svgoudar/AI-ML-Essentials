{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a645cd5",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning in Linear Regression\n",
    "\n",
    "In **ordinary least squares (OLS) linear regression**, there are actually **no hyperparameters to tune** — the coefficients are directly computed by minimizing the sum of squared errors.\n",
    "\n",
    "But when we apply **regularization techniques** (Ridge, Lasso, ElasticNet), hyperparameters come into play.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameters in Linear Regression Variants\n",
    "\n",
    "### Ridge Regression (L2 Regularization)\n",
    "\n",
    "* **Hyperparameter:** `α` (sometimes called λ).\n",
    "* Controls the penalty on large coefficients.\n",
    "\n",
    "  * `α = 0` → ordinary least squares (no penalty).\n",
    "  * Large `α` → coefficients shrink towards zero but never exactly zero.\n",
    "* Effect: reduces variance, prevents overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Lasso Regression (L1 Regularization)\n",
    "\n",
    "* **Hyperparameter:** `α`.\n",
    "* Penalizes absolute values of coefficients.\n",
    "* Large `α` → many coefficients become **exactly zero** → feature selection.\n",
    "* Effect: simpler, more interpretable model.\n",
    "\n",
    "---\n",
    "\n",
    "### ElasticNet (Combination of L1 & L2)\n",
    "\n",
    "* **Hyperparameters:**\n",
    "\n",
    "  * `α` → overall penalty strength.\n",
    "  * `l1_ratio` → balance between L1 (Lasso) and L2 (Ridge).\n",
    "\n",
    "    * `l1_ratio = 0` → pure Ridge.\n",
    "    * `l1_ratio = 1` → pure Lasso.\n",
    "    * `0 < l1_ratio < 1` → mixture.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Hyperparameter Tuning is Needed?\n",
    "\n",
    "* If `α` is too small → model behaves like OLS, may **overfit**.\n",
    "* If `α` is too large → coefficients shrink too much, model may **underfit**.\n",
    "* Proper tuning finds a balance.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Tune Hyperparameters?\n",
    "\n",
    "We use **Cross-Validation (CV)** to find the best values:\n",
    "\n",
    "1. **Grid Search CV**\n",
    "\n",
    "   * Try different values of `α` (and `l1_ratio` for ElasticNet).\n",
    "   * Example: test `α = [0.01, 0.1, 1, 10, 100]`.\n",
    "   * Train model on folds, pick the one with best average CV score.\n",
    "\n",
    "2. **Randomized Search CV**\n",
    "\n",
    "   * Randomly sample hyperparameters from distributions.\n",
    "   * More efficient for large search spaces.\n",
    "\n",
    "3. **Bayesian Optimization** (advanced)\n",
    "\n",
    "   * Uses past evaluation results to choose next hyperparameter values intelligently.\n",
    "\n",
    "---\n",
    "\n",
    "## Example (Python, Scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example: Ridge\n",
    "ridge = Ridge()\n",
    "\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key takeaway:**\n",
    "\n",
    "* OLS → no hyperparameters.\n",
    "* Ridge, Lasso, ElasticNet → hyperparameters (`α`, `l1_ratio`).\n",
    "* Tune them using **cross-validation** to balance bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7ae55",
   "metadata": {},
   "source": [
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
