{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9490ad6",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01255b",
   "metadata": {},
   "source": [
    "\n",
    "# Cross-Validation\n",
    "\n",
    "## Why do we need Cross-Validation\n",
    "\n",
    "* When training ML models, we split data into:\n",
    "\n",
    "  * **Training set** ‚Üí model learns patterns.\n",
    "  * **Validation set** ‚Üí used for hyperparameter tuning.\n",
    "  * **Test set** ‚Üí unseen data, used only at the end to check performance.\n",
    "\n",
    "If we simply split data once (say 70% train, 30% test), the result depends on the random split.\n",
    "üëâ Cross-validation helps us get a **more reliable performance estimate** by training and validating on multiple splits.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Cross-Validation\n",
    "\n",
    "### 1. **Leave-One-Out CV (LOOCV)**\n",
    "\n",
    "* Take one data point as validation, rest as training.\n",
    "* Repeat for every data point.\n",
    "* Accuracy = average of all experiments.\n",
    "\n",
    "‚úÖ Advantage: maximum use of training data.\n",
    "‚ùå Disadvantages:\n",
    "\n",
    "* Computationally expensive (n experiments for n records).\n",
    "* Prone to overfitting (since validation set = 1 record).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Leave-P-Out CV**\n",
    "\n",
    "* Instead of leaving 1 record, leave *p* records as validation.\n",
    "* Train on the rest.\n",
    "* Repeat for all possible combinations.\n",
    "\n",
    "‚úÖ More flexible than LOOCV.\n",
    "‚ùå Impractical for large datasets (combinatorial explosion).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **K-Fold Cross-Validation**\n",
    "\n",
    "* Split dataset into *k* equal folds.\n",
    "* Train on *k-1* folds, validate on the remaining fold.\n",
    "* Repeat *k* times (each fold used once as validation).\n",
    "* Final score = average of all folds.\n",
    "\n",
    "‚úÖ Balance between efficiency and reliability.\n",
    "‚úÖ Most common method in ML.\n",
    "‚ùå Validation set may not preserve class distribution (in classification).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Stratified K-Fold CV**\n",
    "\n",
    "* Same as K-Fold, but ensures **class distribution** is preserved in each fold.\n",
    "* Example: if dataset has 60% positive and 40% negative labels ‚Üí each fold keeps \\~60:40 ratio.\n",
    "\n",
    "‚úÖ Very important for classification tasks with **imbalanced data**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Time Series Cross-Validation**\n",
    "\n",
    "* In time series, order matters ‚Üí can‚Äôt randomly shuffle.\n",
    "* Train on past data, validate on future data.\n",
    "* Example:\n",
    "\n",
    "  * Train = Day 1‚Äì100, Validate = Day 101‚Äì120\n",
    "  * Train = Day 1‚Äì120, Validate = Day 121‚Äì140\n",
    "\n",
    "‚úÖ Used in forecasting, stock prediction, sentiment analysis.\n",
    "‚ùå Training size keeps growing ‚Üí computationally heavier.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Summary Table\n",
    "\n",
    "| CV Type               | Works Best For            | Pros                   | Cons                             |\n",
    "| --------------------- | ------------------------- | ---------------------- | -------------------------------- |\n",
    "| **Hold-Out**          | Large datasets            | Simple, fast           | High variance, depends on split  |\n",
    "| **LOOCV**             | Very small data           | Uses max training data | Very slow, overfitting           |\n",
    "| **Leave-P-Out**       | Small datasets            | Flexible               | Impractical for big data         |\n",
    "| **K-Fold**            | General ML tasks          | Reliable, efficient    | Random split may cause imbalance |\n",
    "| **Stratified K-Fold** | Classification            | Maintains class ratios | Slightly slower                  |\n",
    "| **Time Series CV**    | Forecasting/temporal data | Respects time order    | Increasing training size         |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Cross-Validation?\n",
    "\n",
    "* Provides **stable estimate** of performance.\n",
    "* Helps avoid **overfitting** by testing model on multiple validation sets.\n",
    "* Essential for **hyperparameter tuning** (GridSearchCV, RandomizedSearchCV, Bayesian Optimization).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
