{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{grid} 1 1 2 3\n",
    ":gutter: 3\n",
    "\n",
    ":::{grid-item-card} \n",
    ":link: machine_learning/supervised_learning/Linear_Regression/overview\n",
    ":link-type: doc\n",
    ":class-header: bg-grid-header\n",
    ":class-body: grid-center bg-grid-body\n",
    "\n",
    "<span class=\"grid-title\">Linear Regression</span>\n",
    "^^^\n",
    "\n",
    "Predicts a continuous outcome by modeling the relationship between input features and the target as a straight line.\n",
    ":::\n",
    "\n",
    ":::{grid-item-card} \n",
    ":link: machine_learning/supervised_learning/Logistic_Regression/overview\n",
    ":link-type: doc\n",
    ":class-header: bg-grid-header\n",
    ":class-body: grid-center bg-grid-body\n",
    "\n",
    "<span class=\"grid-title\">Logistic Regression</span>\n",
    "^^^\n",
    "\n",
    "Predicts the probability of a categorical outcome (usually binary) using a logistic (sigmoid) function.\n",
    ":::\n",
    "\n",
    ":::{grid-item-card} \n",
    ":link: machine_learning/supervised_learning/Support_Vector_Machine/overview\n",
    ":link-type: doc\n",
    ":class-header: bg-grid-header\n",
    ":class-body: grid-center bg-grid-body\n",
    "\n",
    "<span class=\"grid-title\">Support Vector Machine</span>\n",
    "^^^\n",
    "\n",
    "Finds the optimal boundary (hyperplane) that best separates classes in the feature space for classification tasks.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{dropdown} Click here for Contents\n",
    "```{contents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is a **subset of Artificial Intelligence (AI)** that focuses on teaching computers to **learn patterns from data** and **make predictions or decisions** without being explicitly programmed with fixed rules.\n",
    "\n",
    "üëâ Instead of writing step-by-step instructions, we provide **examples (data)**, and the algorithm learns the hidden relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## Example to Understand ML\n",
    "\n",
    "* Traditional programming:\n",
    "\n",
    "  * Rules (explicitly coded) + Data ‚Üí Output\n",
    "* Machine Learning:\n",
    "\n",
    "  * Data + Output (examples) ‚Üí Algorithm learns rules ‚Üí Predict new output\n",
    "\n",
    "‚ú® Example: Predicting house prices\n",
    "\n",
    "* Input: Size, Location, Number of rooms\n",
    "* Output: House Price\n",
    "* ML learns the mapping function:\n",
    "\n",
    "  $$\n",
    "  Price = f(Size, Location, Rooms)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "1. **Supervised Learning**\n",
    "\n",
    "   * Learn from labeled data (input + correct output given).\n",
    "   * Task: Prediction.\n",
    "   * Examples:\n",
    "\n",
    "     * Regression (predict numbers, e.g., house prices).\n",
    "     * Classification (predict categories, e.g., spam vs not spam).\n",
    "\n",
    "2. **Unsupervised Learning**\n",
    "\n",
    "   * Learn from unlabeled data (only input, no output given).\n",
    "   * Task: Discover patterns.\n",
    "   * Examples:\n",
    "\n",
    "     * Clustering (grouping customers by purchase behavior).\n",
    "     * Dimensionality reduction (compressing features for visualization).\n",
    "\n",
    "3. **Reinforcement Learning**\n",
    "\n",
    "   * Learn by interacting with the environment (trial and error).\n",
    "   * Task: Decision making.\n",
    "   * Example:\n",
    "\n",
    "     * Teaching a robot to walk.\n",
    "     * AlphaGo beating humans in Go.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Key Components of ML\n",
    "\n",
    "1. **Dataset** ‚Üí Collection of examples (features + labels).\n",
    "2. **Model** ‚Üí Mathematical representation that makes predictions.\n",
    "3. **Training** ‚Üí Process of learning patterns (adjusting model parameters).\n",
    "4. **Evaluation** ‚Üí Measuring performance (accuracy, error, etc.).\n",
    "5. **Prediction** ‚Üí Using the trained model on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is ML important?\n",
    "\n",
    "* Handles **large, complex data** humans cannot analyze manually.\n",
    "* **Automates tasks** (spam filtering, recommendation systems, fraud detection).\n",
    "* Improves over time as it sees more data.\n",
    "\n",
    "\n",
    "## Learning approach Variants\n",
    "\n",
    "### Instance-based learning\n",
    "\n",
    "* Learns by **memorizing training examples**.\n",
    "* No explicit model is built.\n",
    "* Prediction is made by comparing a new instance with stored instances.\n",
    "* Uses a **similarity (distance) measure** to find closest examples.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* k-Nearest Neighbors (kNN)\n",
    "* Locally Weighted Regression\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Simple, flexible.\n",
    "* Works well if decision boundary is irregular.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Expensive at prediction time (must compare with many stored examples).\n",
    "* Sensitive to noise and irrelevant features.\n",
    "\n",
    "---\n",
    "\n",
    "### Model-based learning\n",
    "\n",
    "* Learns a **general model** from training data.\n",
    "* The model captures underlying relationships, then is used for prediction.\n",
    "* Parameters are estimated during training.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* Neural Networks\n",
    "* Decision Trees\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Fast prediction once model is trained.\n",
    "* Generalizes well if model is appropriate.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Training can be computationally heavy.\n",
    "* If model is too simple, it underfits; if too complex, it overfits.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Difference**\n",
    "\n",
    "* **Instance-based**: ‚ÄúRemember examples, predict by similarity.‚Äù\n",
    "* **Model-based**: ‚ÄúLearn rules (parameters), predict by applying model.‚Äù\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **List of Machine Learning Algorithms**\n",
    "\n",
    "| **Category**                 | **Sub-type**                 | **Algorithms**                                                                                                                                                                                                                                                                                                               |\n",
    "| ---------------------------- | ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Supervised Learning**      | **Regression**               | Linear Regression, Polynomial Regression, Ridge, Lasso, Elastic Net, SVR, Decision Tree Regression, Random Forest Regression, Gradient Boosting (XGBoost, LightGBM, CatBoost), kNN Regression, Bayesian Regression, Neural Networks                                                                                          |\n",
    "|                              | **Classification**           | Logistic Regression, kNN, SVM, Decision Trees (CART, ID3, C4.5), Random Forest, Gradient Boosting (XGBoost, LightGBM, CatBoost), Naive Bayes (Gaussian, Multinomial, Bernoulli), Perceptron, Multi-layer Perceptrons, Ensemble Methods (Bagging, Stacking, Voting), Probabilistic Graphical Models (Bayesian Networks, CRFs) |\n",
    "| **Unsupervised Learning**    | **Clustering**               | k-Means, Hierarchical Clustering, DBSCAN, OPTICS, Gaussian Mixture Models, Mean-Shift, Spectral Clustering, BIRCH, Affinity Propagation                                                                                                                                                                                      |\n",
    "|                              | **Dimensionality Reduction** | PCA, Kernel PCA, ICA, SVD, Factor Analysis, t-SNE, UMAP, Autoencoders                                                                                                                                                                                                                                                        |\n",
    "|                              | **Association Rules**        | Apriori, Eclat, FP-Growth                                                                                                                                                                                                                                                                                                    |\n",
    "|                              | **Density Estimation**       | KDE, Expectation-Maximization (EM), Hidden Markov Models (unsupervised setting)                                                                                                                                                                                                                                              |\n",
    "| **Semi-Supervised Learning** | ‚Äî                            | Self-training, Co-training, Label Propagation/Spreading, Semi-supervised SVM, Graph-based methods, Semi-supervised Deep Learning (Consistency Regularization, Pseudo-labeling)                                                                                                                                               |\n",
    "| **Reinforcement Learning**   | **Value-based**              | Q-Learning, SARSA, Deep Q-Networks (DQN)                                                                                                                                                                                                                                                                                     |\n",
    "|                              | **Policy-based**             | Policy Gradient (REINFORCE), Actor‚ÄìCritic (A2C, A3C), Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO)                                                                                                                                                                                            |\n",
    "|                              | **Model-based / Advanced**   | DDPG, TD3, SAC, Monte Carlo Tree Search, Multi-agent RL                                                                                                                                                                                                                                                                      |\n",
    "| **Other Methods**            | **Ensemble Methods**         | Bagging, Boosting (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost), Stacking, Blending, Voting Classifier                                                                                                                                                                                                          |\n",
    "|                              | **Probabilistic / Bayesian** | Naive Bayes, Bayesian Networks, Gaussian Processes, HMMs, Markov Random Fields                                                                                                                                                                                                                                               |\n",
    "|                              | **Deep Learning**            | Feedforward NN, CNN, RNN, LSTM, GRU, Transformers (BERT, GPT), Variational Autoencoders (VAE), Generative Adversarial Networks (GANs)                                                                                                                                                                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Common ML Pitfalls & How to Prevent Them\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Leakage\n",
    "\n",
    "* **What it is:** Information from test/future data sneaks into training.\n",
    "* **Example:** Scaling before splitting, or using ‚Äúfuture‚Äù features.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Always split before preprocessing.\n",
    "  * Use scikit-learn **pipelines**.\n",
    "  * In time-series, only use **past data** for training.\n",
    "\n",
    "Got it üëç ‚Äî let‚Äôs go deep into **Data Leakage** because it‚Äôs one of the trickiest yet most common mistakes in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Data leakage happens when **information that would not be available at prediction time** is used (directly or indirectly) during training.\n",
    "\n",
    "üëâ This gives the model **unfair hints**, making it look very accurate on validation/test data but fail on real-world unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why It‚Äôs Dangerous\n",
    "\n",
    "* Inflates model performance (fake high accuracy).\n",
    "* Leads to overconfidence in the model.\n",
    "* Deployment disaster: model fails when such information isn‚Äôt available.\n",
    "\n",
    "It‚Äôs like *cheating in an exam with leaked answers* ‚Üí perfect marks in practice, but no real skill.\n",
    "\n",
    "---\n",
    "\n",
    "#### Types of Data Leakage\n",
    "\n",
    "##### A. Target Leakage\n",
    "\n",
    "* Features include data that would only be available *after* the prediction is made.\n",
    "* Example:\n",
    "\n",
    "  * Predicting if a patient has diabetes.\n",
    "  * Including ‚Äúinsulin prescribed‚Äù as a feature.\n",
    "  * Problem: prescription decisions depend on knowing the patient has diabetes.\n",
    "\n",
    "---\n",
    "\n",
    "##### B. Train-Test Contamination\n",
    "\n",
    "* Test data information accidentally influences training.\n",
    "* Example:\n",
    "\n",
    "  * Scaling or feature selection done **before splitting** dataset into train/test.\n",
    "  * The test data indirectly shapes the training process.\n",
    "\n",
    "---\n",
    "\n",
    "##### C. Temporal Leakage\n",
    "\n",
    "* In time-series data, using **future information** to predict the past.\n",
    "* Example:\n",
    "\n",
    "  * Predicting stock price at $t$.\n",
    "  * Accidentally including features from $t+1$ or later.\n",
    "\n",
    "---\n",
    "\n",
    "##### D. Indirect / Proxy Leakage\n",
    "\n",
    "* When a feature is a disguised form of the target.\n",
    "* Example:\n",
    "\n",
    "  * Predicting whether a customer churns.\n",
    "  * Including ‚Äúlast month‚Äôs customer support ticket closure‚Äù ‚Üí which directly correlates with churn.\n",
    "\n",
    "---\n",
    "\n",
    "#### Causes of Data Leakage\n",
    "\n",
    "* Preprocessing the entire dataset before splitting.\n",
    "* Poor feature engineering (using outcome-related variables).\n",
    "* Mismanaged cross-validation (e.g., same patient‚Äôs data across train & test).\n",
    "* Temporal misalignment in time-series datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### Real-World Examples\n",
    "\n",
    "* **Healthcare:** Using \"hospital billing code\" as a feature when predicting disease ‚Üí billing code assigned *after* diagnosis.\n",
    "* **Finance:** Predicting loan defaults using ‚Äúlate payment flag‚Äù ‚Üí this flag only appears after default happens.\n",
    "* **E-commerce:** Predicting purchase likelihood using ‚Äúdiscount applied‚Äù ‚Üí but discount decisions happen *after* purchase intent.\n",
    "\n",
    "---\n",
    "\n",
    "#### How to Detect Data Leakage\n",
    "\n",
    "* Too-good-to-be-true model performance.\n",
    "* Validation accuracy much higher than real-world deployment.\n",
    "* Suspicious features that seem too correlated with the target.\n",
    "* Leakage found in **feature importance** analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### How to Prevent Data Leakage\n",
    "\n",
    "* Best Practices:\n",
    "\n",
    "1. **Split first, preprocess later**\n",
    "\n",
    "   * Do train/test split before scaling, imputing, or feature selection.\n",
    "2. **Pipelines**\n",
    "\n",
    "   * Use sklearn `Pipeline` to ensure preprocessing happens separately for train/test.\n",
    "3. **Audit features**\n",
    "\n",
    "   * Check: *Would I have this feature at prediction time?*\n",
    "4. **Careful with time-series**\n",
    "\n",
    "   * Always split chronologically, not randomly.\n",
    "5. **Cross-validation grouping**\n",
    "\n",
    "   * Ensure related samples (same patient, same user) are not split across train/test.\n",
    "6. **Domain expertise**\n",
    "\n",
    "   * Work with subject experts to identify hidden leakage features.\n",
    "\n",
    "---\n",
    "\n",
    "#### Analogy\n",
    "\n",
    "* Training with leakage = **student cheating with leaked exam answers**.\n",
    "* Deployment = **real exam without leaks** ‚Üí the student (model) fails badly.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Data leakage = using future or unavailable information in training.\n",
    "It‚Äôs subtle, dangerous, and often the reason behind ‚Äúamazing models that collapse in production.‚Äù\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Overfitting\n",
    "\n",
    "* **What it is:** Model memorizes noise in training data ‚Üí poor generalization.\n",
    "* **Example:** Deep tree that perfectly fits training but fails on test.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Use **regularization** (L1, L2, dropout).\n",
    "  * Collect more data.\n",
    "  * Use **cross-validation**.\n",
    "  * Prune complexity (e.g., max depth in decision trees).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Underfitting\n",
    "\n",
    "* **What it is:** Model too simple ‚Üí misses important patterns.\n",
    "* **Example:** Using linear regression on complex nonlinear data.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Use more expressive models.\n",
    "  * Add features or polynomial terms.\n",
    "  * Reduce regularization strength.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Class Imbalance\n",
    "\n",
    "* **What it is:** One class dominates (e.g., 99% normal, 1% fraud).\n",
    "* **Example:** Classifier predicts ‚Äúnormal‚Äù always ‚Üí high accuracy but useless.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Resample (oversample minority, undersample majority).\n",
    "  * Use **SMOTE** (synthetic data generation).\n",
    "  * Choose **balanced metrics** (F1, ROC-AUC, Precision-Recall).\n",
    "  * Apply **class weights** in algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Data Drift & Concept Drift\n",
    "\n",
    "* **What it is:** Data or relationships change over time.\n",
    "* **Example:** Customer behavior before vs after COVID.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Monitor model performance regularly.\n",
    "  * Retrain periodically.\n",
    "  * Use **online learning** for streaming data.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Multicollinearity\n",
    "\n",
    "* **What it is:** Features highly correlated ‚Üí unstable coefficients.\n",
    "* **Example:** Predicting salary with both ‚Äúyears of experience‚Äù and ‚Äúmonths of experience‚Äù.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Remove redundant features.\n",
    "  * Use **regularization (Ridge/Lasso)**.\n",
    "  * Apply **PCA** for dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Curse of Dimensionality\n",
    "\n",
    "* **What it is:** As features grow, data becomes sparse ‚Üí distance metrics fail.\n",
    "* **Example:** kNN performs poorly in 1000 dimensions.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Use **feature selection**.\n",
    "  * Apply dimensionality reduction (PCA, t-SNE, UMAP).\n",
    "  * Gather more data.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Sampling Bias\n",
    "\n",
    "* **What it is:** Training data doesn‚Äôt represent real-world distribution.\n",
    "* **Example:** Training only on urban customers ‚Üí fails on rural customers.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Ensure **stratified sampling**.\n",
    "  * Collect **representative datasets**.\n",
    "  * Be cautious with web-scraped or convenience samples.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Scaling & Normalization Issues\n",
    "\n",
    "* **What it is:** Using features with different scales can mislead algorithms.\n",
    "* **Example:** kNN treating ‚Äúincome (\\$)‚Äù as more important than ‚Äúage (years)‚Äù.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Normalize/standardize features.\n",
    "  * Use pipelines to prevent leakage.\n",
    "  * Choose scale-invariant models if possible (trees).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Evaluation Pitfalls\n",
    "\n",
    "* **What it is:** Using the wrong metric for the problem.\n",
    "* **Example:** Accuracy in fraud detection (useless if data is imbalanced).\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Choose metrics suited to task (F1 for imbalance, RMSE for regression).\n",
    "  * Use **cross-validation**.\n",
    "  * Avoid test set reuse (keep a final hold-out set).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
