{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{grid} 1 1 2 2\n",
    ":gutter: 3\n",
    "\n",
    ":::{grid-item-card}\n",
    ":class-header: bg-grid-header\n",
    ":class-body: grid-center bg-grid-body\n",
    "\n",
    "<span class=\"grid-title\">Supervised ‚Äì Regression</span>\n",
    "^^^\n",
    "\n",
    "* [Linear Regression](machine_learning/supervised_learning/Linear_Regression/overview)\n",
    "* [Support Vector Machine](machine_learning/supervised_learning/Support_Vector_Machine/overview)\n",
    "* [SVR](machine_learning/supervised_learning/Support_Vector_Machine/SVR/overview)\n",
    "* [KNN](machine_learning/supervised_learning/KNN/overview)\n",
    "* [Decision Tree](machine_learning/supervised_learning/Decision_Tree/overview)\n",
    "* [DTC](machine_learning/supervised_learning/Decision_Tree/DTR/overview)\n",
    "* [Random Forest](machine_learning/supervised_learning/Random_Forest/overview)\n",
    "* [RFR](machine_learning/supervised_learning/Random_Forest/RFR)\n",
    "* [AdaBoost](machine_learning/supervised_learning/AdaBoost/overview)\n",
    "* [AdaBoost Regression](machine_learning/supervised_learning/AdaBoost/06_part)\n",
    "* [Gradient Boosting](machine_learning/supervised_learning/Gradient_boosting/overview)\n",
    "* [Gradient Boosting Regressor](machine_learning/supervised_learning/Gradient_boosting/05_part)\n",
    "* [XGBoost](machine_learning/supervised_learning/XGBoost/overview)\n",
    "* [XGBoost Regressor](machine_learning/supervised_learning/XGBoost/04_part)\n",
    "  \n",
    ":::\n",
    "\n",
    ":::{grid-item-card}\n",
    ":class-header: bg-grid-header\n",
    ":class-body: grid-center bg-grid-body\n",
    "\n",
    "\n",
    "<span class=\"grid-title\">Supervised ‚Äì Classification</span>\n",
    "^^^\n",
    "\n",
    "* [Logistic Regression](machine_learning/supervised_learning/Logistic_Regression/overview)\n",
    "* [Support Vector Machine](machine_learning/supervised_learning/Support_Vector_Machine/overview)\n",
    "* [SVC](machine_learning/supervised_learning/Support_Vector_Machine/SVC/overview)\n",
    "* [Naive Bayes](machine_learning/supervised_learning/Naive_Bayes/overview)\n",
    "* [KNN](machine_learning/supervised_learning/KNN/overview)\n",
    "* [Decision Tree](machine_learning/supervised_learning/Decision_Tree/overview)\n",
    "* [DTC](machine_learning/supervised_learning/Decision_Tree/DTC/overview)\n",
    "* [Random Forest](machine_learning/supervised_learning/Random_Forest/overview)\n",
    "* [RFC](machine_learning/supervised_learning/Random_Forest/RFC)\n",
    "* [AdaBoost](machine_learning/supervised_learning/AdaBoost/overview)\n",
    "* [AdaBoost Classification](machine_learning/supervised_learning/AdaBoost/08_part)\n",
    "* [Gradient Boosting](machine_learning/supervised_learning/Gradient_boosting/overview)\n",
    "* [Gradient Boosting Classifier](machine_learning/supervised_learning/Gradient_boosting/04_part)\n",
    "* [XGBoost](machine_learning/supervised_learning/XGBoost/overview)\n",
    "* [XGBoost Classifier](machine_learning/supervised_learning/XGBoost/05_part)\n",
    "\n",
    ":::\n",
    "\n",
    ":::{grid-item-card}\n",
    ":class-header: bg-grid-header\n",
    ":class-body: grid-center bg-grid-body\n",
    "\n",
    "<span class=\"grid-title\">Unsupervised ‚Äì Clustering</span>\n",
    "^^^\n",
    "* [K-Means](machine_learning/unsupervised_learning/K_Means/overview)\n",
    "* [Hierarchical Clustering](machine_learning/unsupervised_learning/Hierarchical_Clustering/overview)\n",
    "* [DBSCAN](machine_learning/unsupervised_learning/DBSCAN/overview)\n",
    "* [Anamoly Detection](machine_learning/unsupervised_learning/Anamoly_Detection/overview)\n",
    ":::\n",
    "\n",
    ":::{grid-item-card}\n",
    ":class-header: bg-grid-header\n",
    ":class-body: grid-center bg-grid-body\n",
    "\n",
    "<span class=\"grid-title\">Unsupervised ‚Äì Dimensionality Reduction</span>\n",
    "^^^\n",
    "* [Principal Component Analysis (PCA)](machine_learning/unsupervised_learning/PCA/overview)\n",
    "* [t-SNE](machine_learning/unsupervised_learning/T_SNE/overview)\n",
    "\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{dropdown} Click here for Contents\n",
    "```{contents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is a **subset of Artificial Intelligence (AI)** that focuses on teaching computers to **learn patterns from data** and **make predictions or decisions** without being explicitly programmed with fixed rules.\n",
    "\n",
    "üëâ Instead of writing step-by-step instructions, we provide **examples (data)**, and the algorithm learns the hidden relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## Example to Understand ML\n",
    "\n",
    "* Traditional programming:\n",
    "\n",
    "  * Rules (explicitly coded) + Data ‚Üí Output\n",
    "* Machine Learning:\n",
    "\n",
    "  * Data + Output (examples) ‚Üí Algorithm learns rules ‚Üí Predict new output\n",
    "\n",
    "‚ú® Example: Predicting house prices\n",
    "\n",
    "* Input: Size, Location, Number of rooms\n",
    "* Output: House Price\n",
    "* ML learns the mapping function:\n",
    "\n",
    "  $$\n",
    "  Price = f(Size, Location, Rooms)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "1. **Supervised Learning**\n",
    "\n",
    "   * Learn from labeled data (input + correct output given).\n",
    "   * Task: Prediction.\n",
    "   * Examples:\n",
    "\n",
    "     * Regression (predict numbers, e.g., house prices).\n",
    "     * Classification (predict categories, e.g., spam vs not spam).\n",
    "\n",
    "2. **Unsupervised Learning**\n",
    "\n",
    "   * Learn from unlabeled data (only input, no output given).\n",
    "   * Task: Discover patterns.\n",
    "   * Examples:\n",
    "\n",
    "     * Clustering (grouping customers by purchase behavior).\n",
    "     * Dimensionality reduction (compressing features for visualization).\n",
    "\n",
    "3. **Reinforcement Learning**\n",
    "\n",
    "   * Learn by interacting with the environment (trial and error).\n",
    "   * Task: Decision making.\n",
    "   * Example:\n",
    "\n",
    "     * Teaching a robot to walk.\n",
    "     * AlphaGo beating humans in Go.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Key Components of ML\n",
    "\n",
    "1. **Dataset** ‚Üí Collection of examples (features + labels).\n",
    "2. **Model** ‚Üí Mathematical representation that makes predictions.\n",
    "3. **Training** ‚Üí Process of learning patterns (adjusting model parameters).\n",
    "4. **Evaluation** ‚Üí Measuring performance (accuracy, error, etc.).\n",
    "5. **Prediction** ‚Üí Using the trained model on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is ML important?\n",
    "\n",
    "* Handles **large, complex data** humans cannot analyze manually.\n",
    "* **Automates tasks** (spam filtering, recommendation systems, fraud detection).\n",
    "* Improves over time as it sees more data.\n",
    "\n",
    "\n",
    "## Learning approach Variants\n",
    "\n",
    "### Instance-based learning\n",
    "\n",
    "* Learns by **memorizing training examples**.\n",
    "* No explicit model is built.\n",
    "* Prediction is made by comparing a new instance with stored instances.\n",
    "* Uses a **similarity (distance) measure** to find closest examples.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* k-Nearest Neighbors (kNN)\n",
    "* Locally Weighted Regression\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Simple, flexible.\n",
    "* Works well if decision boundary is irregular.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Expensive at prediction time (must compare with many stored examples).\n",
    "* Sensitive to noise and irrelevant features.\n",
    "\n",
    "---\n",
    "\n",
    "### Model-based learning\n",
    "\n",
    "* Learns a **general model** from training data.\n",
    "* The model captures underlying relationships, then is used for prediction.\n",
    "* Parameters are estimated during training.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* Neural Networks\n",
    "* Decision Trees\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "* Fast prediction once model is trained.\n",
    "* Generalizes well if model is appropriate.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Training can be computationally heavy.\n",
    "* If model is too simple, it underfits; if too complex, it overfits.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Difference**\n",
    "\n",
    "* **Instance-based**: ‚ÄúRemember examples, predict by similarity.‚Äù\n",
    "* **Model-based**: ‚ÄúLearn rules (parameters), predict by applying model.‚Äù\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **List of Machine Learning Algorithms**\n",
    "\n",
    "| **Category**                 | **Sub-type**                 | **Algorithms**                                                                                                                                                                                                                                                                                                               |\n",
    "| ---------------------------- | ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Supervised Learning**      | **Regression**               | Linear Regression, Polynomial Regression, Ridge, Lasso, Elastic Net, SVR, Decision Tree Regression, Random Forest Regression, Gradient Boosting (XGBoost, LightGBM, CatBoost), kNN Regression, Bayesian Regression, Neural Networks                                                                                          |\n",
    "|                              | **Classification**           | Logistic Regression, kNN, SVM, Decision Trees (CART, ID3, C4.5), Random Forest, Gradient Boosting (XGBoost, LightGBM, CatBoost), Naive Bayes (Gaussian, Multinomial, Bernoulli), Perceptron, Multi-layer Perceptrons, Ensemble Methods (Bagging, Stacking, Voting), Probabilistic Graphical Models (Bayesian Networks, CRFs) |\n",
    "| **Unsupervised Learning**    | **Clustering**               | k-Means, Hierarchical Clustering, DBSCAN, OPTICS, Gaussian Mixture Models, Mean-Shift, Spectral Clustering, BIRCH, Affinity Propagation                                                                                                                                                                                      |\n",
    "|                              | **Dimensionality Reduction** | PCA, Kernel PCA, ICA, SVD, Factor Analysis, t-SNE, UMAP, Autoencoders                                                                                                                                                                                                                                                        |\n",
    "|                              | **Association Rules**        | Apriori, Eclat, FP-Growth                                                                                                                                                                                                                                                                                                    |\n",
    "|                              | **Density Estimation**       | KDE, Expectation-Maximization (EM), Hidden Markov Models (unsupervised setting)                                                                                                                                                                                                                                              |\n",
    "| **Semi-Supervised Learning** | ‚Äî                            | Self-training, Co-training, Label Propagation/Spreading, Semi-supervised SVM, Graph-based methods, Semi-supervised Deep Learning (Consistency Regularization, Pseudo-labeling)                                                                                                                                               |\n",
    "| **Reinforcement Learning**   | **Value-based**              | Q-Learning, SARSA, Deep Q-Networks (DQN)                                                                                                                                                                                                                                                                                     |\n",
    "|                              | **Policy-based**             | Policy Gradient (REINFORCE), Actor‚ÄìCritic (A2C, A3C), Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO)                                                                                                                                                                                            |\n",
    "|                              | **Model-based / Advanced**   | DDPG, TD3, SAC, Monte Carlo Tree Search, Multi-agent RL                                                                                                                                                                                                                                                                      |\n",
    "| **Other Methods**            | **Ensemble Methods**         | Bagging, Boosting (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost), Stacking, Blending, Voting Classifier                                                                                                                                                                                                          |\n",
    "|                              | **Probabilistic / Bayesian** | Naive Bayes, Bayesian Networks, Gaussian Processes, HMMs, Markov Random Fields                                                                                                                                                                                                                                               |\n",
    "|                              | **Deep Learning**            | Feedforward NN, CNN, RNN, LSTM, GRU, Transformers (BERT, GPT), Variational Autoencoders (VAE), Generative Adversarial Networks (GANs)                                                                                                                                                                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Common ML Pitfalls & How to Prevent Them\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Leakage\n",
    "\n",
    "* **What it is:** Information from test/future data sneaks into training.\n",
    "* **Example:** Scaling before splitting, or using ‚Äúfuture‚Äù features.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Always split before preprocessing.\n",
    "  * Use scikit-learn **pipelines**.\n",
    "  * In time-series, only use **past data** for training.\n",
    "\n",
    "Got it üëç ‚Äî let‚Äôs go deep into **Data Leakage** because it‚Äôs one of the trickiest yet most common mistakes in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Data leakage happens when **information that would not be available at prediction time** is used (directly or indirectly) during training.\n",
    "\n",
    "üëâ This gives the model **unfair hints**, making it look very accurate on validation/test data but fail on real-world unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why It‚Äôs Dangerous\n",
    "\n",
    "* Inflates model performance (fake high accuracy).\n",
    "* Leads to overconfidence in the model.\n",
    "* Deployment disaster: model fails when such information isn‚Äôt available.\n",
    "\n",
    "It‚Äôs like *cheating in an exam with leaked answers* ‚Üí perfect marks in practice, but no real skill.\n",
    "\n",
    "---\n",
    "\n",
    "#### Types of Data Leakage\n",
    "\n",
    "##### A. Target Leakage\n",
    "\n",
    "* Features include data that would only be available *after* the prediction is made.\n",
    "* Example:\n",
    "\n",
    "  * Predicting if a patient has diabetes.\n",
    "  * Including ‚Äúinsulin prescribed‚Äù as a feature.\n",
    "  * Problem: prescription decisions depend on knowing the patient has diabetes.\n",
    "\n",
    "---\n",
    "\n",
    "##### B. Train-Test Contamination\n",
    "\n",
    "* Test data information accidentally influences training.\n",
    "* Example:\n",
    "\n",
    "  * Scaling or feature selection done **before splitting** dataset into train/test.\n",
    "  * The test data indirectly shapes the training process.\n",
    "\n",
    "---\n",
    "\n",
    "##### C. Temporal Leakage\n",
    "\n",
    "* In time-series data, using **future information** to predict the past.\n",
    "* Example:\n",
    "\n",
    "  * Predicting stock price at $t$.\n",
    "  * Accidentally including features from $t+1$ or later.\n",
    "\n",
    "---\n",
    "\n",
    "##### D. Indirect / Proxy Leakage\n",
    "\n",
    "* When a feature is a disguised form of the target.\n",
    "* Example:\n",
    "\n",
    "  * Predicting whether a customer churns.\n",
    "  * Including ‚Äúlast month‚Äôs customer support ticket closure‚Äù ‚Üí which directly correlates with churn.\n",
    "\n",
    "---\n",
    "\n",
    "#### Causes of Data Leakage\n",
    "\n",
    "* Preprocessing the entire dataset before splitting.\n",
    "* Poor feature engineering (using outcome-related variables).\n",
    "* Mismanaged cross-validation (e.g., same patient‚Äôs data across train & test).\n",
    "* Temporal misalignment in time-series datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### Real-World Examples\n",
    "\n",
    "* **Healthcare:** Using \"hospital billing code\" as a feature when predicting disease ‚Üí billing code assigned *after* diagnosis.\n",
    "* **Finance:** Predicting loan defaults using ‚Äúlate payment flag‚Äù ‚Üí this flag only appears after default happens.\n",
    "* **E-commerce:** Predicting purchase likelihood using ‚Äúdiscount applied‚Äù ‚Üí but discount decisions happen *after* purchase intent.\n",
    "\n",
    "---\n",
    "\n",
    "#### How to Detect Data Leakage\n",
    "\n",
    "* Too-good-to-be-true model performance.\n",
    "* Validation accuracy much higher than real-world deployment.\n",
    "* Suspicious features that seem too correlated with the target.\n",
    "* Leakage found in **feature importance** analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### How to Prevent Data Leakage\n",
    "\n",
    "* Best Practices:\n",
    "\n",
    "1. **Split first, preprocess later**\n",
    "\n",
    "   * Do train/test split before scaling, imputing, or feature selection.\n",
    "2. **Pipelines**\n",
    "\n",
    "   * Use sklearn `Pipeline` to ensure preprocessing happens separately for train/test.\n",
    "3. **Audit features**\n",
    "\n",
    "   * Check: *Would I have this feature at prediction time?*\n",
    "4. **Careful with time-series**\n",
    "\n",
    "   * Always split chronologically, not randomly.\n",
    "5. **Cross-validation grouping**\n",
    "\n",
    "   * Ensure related samples (same patient, same user) are not split across train/test.\n",
    "6. **Domain expertise**\n",
    "\n",
    "   * Work with subject experts to identify hidden leakage features.\n",
    "\n",
    "---\n",
    "\n",
    "#### Analogy\n",
    "\n",
    "* Training with leakage = **student cheating with leaked exam answers**.\n",
    "* Deployment = **real exam without leaks** ‚Üí the student (model) fails badly.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Data leakage = using future or unavailable information in training.\n",
    "It‚Äôs subtle, dangerous, and often the reason behind ‚Äúamazing models that collapse in production.‚Äù\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Overfitting\n",
    "\n",
    "* **What it is:** Model memorizes noise in training data ‚Üí poor generalization.\n",
    "* **Example:** Deep tree that perfectly fits training but fails on test.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Use **regularization** (L1, L2, dropout).\n",
    "  * Collect more data.\n",
    "  * Use **cross-validation**.\n",
    "  * Prune complexity (e.g., max depth in decision trees).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Underfitting\n",
    "\n",
    "* **What it is:** Model too simple ‚Üí misses important patterns.\n",
    "* **Example:** Using linear regression on complex nonlinear data.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Use more expressive models.\n",
    "  * Add features or polynomial terms.\n",
    "  * Reduce regularization strength.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Class Imbalance\n",
    "\n",
    "* **What it is:** One class dominates (e.g., 99% normal, 1% fraud).\n",
    "* **Example:** Classifier predicts ‚Äúnormal‚Äù always ‚Üí high accuracy but useless.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Resample (oversample minority, undersample majority).\n",
    "  * Use **SMOTE** (synthetic data generation).\n",
    "  * Choose **balanced metrics** (F1, ROC-AUC, Precision-Recall).\n",
    "  * Apply **class weights** in algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Data Drift & Concept Drift\n",
    "\n",
    "* **What it is:** Data or relationships change over time.\n",
    "* **Example:** Customer behavior before vs after COVID.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Monitor model performance regularly.\n",
    "  * Retrain periodically.\n",
    "  * Use **online learning** for streaming data.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Multicollinearity\n",
    "\n",
    "* **What it is:** Features highly correlated ‚Üí unstable coefficients.\n",
    "* **Example:** Predicting salary with both ‚Äúyears of experience‚Äù and ‚Äúmonths of experience‚Äù.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Remove redundant features.\n",
    "  * Use **regularization (Ridge/Lasso)**.\n",
    "  * Apply **PCA** for dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Curse of Dimensionality\n",
    "\n",
    "* **What it is:** As features grow, data becomes sparse ‚Üí distance metrics fail.\n",
    "* **Example:** kNN performs poorly in 1000 dimensions.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Use **feature selection**.\n",
    "  * Apply dimensionality reduction (PCA, t-SNE, UMAP).\n",
    "  * Gather more data.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Sampling Bias\n",
    "\n",
    "* **What it is:** Training data doesn‚Äôt represent real-world distribution.\n",
    "* **Example:** Training only on urban customers ‚Üí fails on rural customers.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Ensure **stratified sampling**.\n",
    "  * Collect **representative datasets**.\n",
    "  * Be cautious with web-scraped or convenience samples.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Scaling & Normalization Issues\n",
    "\n",
    "* **What it is:** Using features with different scales can mislead algorithms.\n",
    "* **Example:** kNN treating ‚Äúincome (\\$)‚Äù as more important than ‚Äúage (years)‚Äù.\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Normalize/standardize features.\n",
    "  * Use pipelines to prevent leakage.\n",
    "  * Choose scale-invariant models if possible (trees).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Evaluation Pitfalls\n",
    "\n",
    "* **What it is:** Using the wrong metric for the problem.\n",
    "* **Example:** Accuracy in fraud detection (useless if data is imbalanced).\n",
    "* ‚úÖ **Prevention:**\n",
    "\n",
    "  * Choose metrics suited to task (F1 for imbalance, RMSE for regression).\n",
    "  * Use **cross-validation**.\n",
    "  * Avoid test set reuse (keep a final hold-out set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operator / Function | Definition | Usage / Intuition | Example |\n",
    "|-------------------|------------|-----------------|---------|\n",
    "| $\\min_x f(x)$ | Minimum value of a function | Find smallest value of objective | $ \\min_x (x-3)^2 = 0 $ |\n",
    "| $ \\max_x f(x) $ | Maximum value of a function | Find largest value of objective | $ \\max_x -(x-3)^2 = 0 $ |\n",
    "| $ \\arg\\min_x f(x) $ | Input where function is minimized | Optimization to find best parameters | $ \\arg\\min_x (x-3)^2 = 3 $ |\n",
    "| $ \\arg\\max_x f(x) $ | Input where function is maximized | Find best parameter location | $ \\arg\\max_x -(x-3)^2 = 3 $ |\n",
    "| $ \\frac{d}{dx} f(x) $ | Derivative w.r.t scalar | Slope / rate of change | $ \\frac{d}{dx} (x^2) = 2x $ |\n",
    "| $ \\frac{\\partial f}{\\partial x_i} $ | Partial derivative | Multivariate rate of change | $ \\frac{\\partial}{\\partial x} (x^2 + y^2) = 2x $ |\n",
    "| $ \\nabla f(x) $ | Gradient vector | Direction of steepest ascent | $ \\nabla (x^2 + y^2) = [2x,2y] $ |\n",
    "| $ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L $ | Gradient Descent update | Iteratively minimize loss | Linear regression update |\n",
    "| $ \\langle u, v \\rangle $ | Dot product / inner product | Similarity / projection | $ \\langle [1,2],[3,4] \\rangle = 11 $ |\n",
    "| $ \\|x\\|_2 $ | L2 norm (Euclidean) | Magnitude of vector | $ \\|[3,4]\\|_2 = 5 $ |\n",
    "| $ \\|x\\|_1 $ | L1 norm (Manhattan) | Sum of absolute values | $ \\|[3,-4]\\|_1 = 7 $ |\n",
    "| $ A^\\top $ | Matrix transpose | Switch rows ‚Üî columns | $ [[1,2],[3,4]]^\\top = [[1,3],[2,4]] $ |\n",
    "| $ \\text{Tr}(A) $ | Trace of a matrix | Sum of diagonal | $ \\text{Tr}([[1,2],[3,4]]) = 5 $ |\n",
    "| $ \\det(A) $ | Determinant | Scaling factor of matrix | $ \\det([[1,2],[3,4]])=-2 $ |\n",
    "| $ \\mathbb{E}[X] $ | Expectation / mean | Average value | $ \\mathbb{E}[X] = \\sum x_i P(x_i) $ |\n",
    "| $ \\text{Var}(X) $ | Variance | Spread of X | $ \\text{Var}([1,2,3]) = 2/3 $ |\n",
    "| $ \\text{Cov}(X,Y) $ | Covariance | Measure of correlation | $ \\text{Cov}(X,Y) = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])] $ |\n",
    "| $ \\mathbb{P}(A) $ | Probability | Chance of event | $ \\mathbb{P}(X>0) $ |\n",
    "| $ L(y,\\hat{y}) $ | Loss function | Measures prediction error | `MSE, Cross-Entropy` |\n",
    "| $ r_{im} = - \\frac{\\partial L}{\\partial F(x_i)} $ | Pseudo-residuals (Boosting) | Direction to reduce loss | Gradient Boosting step |\n",
    "| $ F_m = F_{m-1} + \\nu \\gamma_m h_m(x) $ | Boosted model update | Add tree‚Äôs contribution | Gradient Boosting |\n",
    "| $ \\text{sign}(x) $ | Sign function | Direction of number | $ \\text{sign}(-5)=-1 $ |\n",
    "| $ \\mathbf{1}_{\\{\\text{condition}\\}} $ | Indicator function | 1 if true, 0 if false | $ \\mathbf{1}_{x>0} $ |\n",
    "| $ \\sigma(x) $ | Sigmoid function | Map to probability [0,1] | $ \\sigma(0)=0.5 $ |\n",
    "| $ \\text{softmax}(z_i) $ | Softmax function | Multi-class probability | $ \\text{softmax}([1,2,3])_i $ |\n",
    "| $ \\text{ReLU}(x) $ | Rectified Linear Unit | Nonlinear activation | $ \\text{ReLU}(-2)=0, \\text{ReLU}(3)=3 $ |\n",
    "| $ \\hat{y} = F_M(x) $ | Regression prediction | Final model output | Gradient Boosting Regressor |\n",
    "| $ \\hat{y} = \\mathbf{1}[\\sigma(F_M(x))>0.5] $ | Binary classification prediction | Threshold probability | Gradient Boosting Classifier |\n",
    "| $ \\hat{y}_i = \\text{softmax}(F_M(x))_i $ | Multi-class classification | Probability per class | Gradient Boosting Multi-class |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
