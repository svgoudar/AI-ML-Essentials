{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f374b6f",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "TF-IDF is a **numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus).**\n",
    "\n",
    "It combines two measures:\n",
    "\n",
    "1. **TF (Term Frequency):** How often a word appears in a document.\n",
    "2. **IDF (Inverse Document Frequency):** How unique or rare a word is across all documents in the corpus.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log \\frac{N}{1 + \\text{DF}(t)}\n",
    "$$\n",
    "\n",
    "* $t$ → term (word)\n",
    "* $d$ → document\n",
    "* $N$ → total number of documents\n",
    "* $\\text{DF}(t)$ → number of documents containing the term $t$\n",
    "* Adding 1 in the denominator avoids division by zero\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Intuition**\n",
    "\n",
    "* **High TF:** Word appears frequently in a document → important for that document.\n",
    "* **High IDF:** Word appears in fewer documents → more unique → carries more information.\n",
    "* **High TF-IDF:** Word is frequent in a document **and** rare across other documents → highly significant.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Word \"the\" → high TF but appears in almost all documents → low IDF → low TF-IDF.\n",
    "* Word \"NLP\" → appears multiple times in a specific document but rarely elsewhere → high TF-IDF.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Example**\n",
    "\n",
    "Suppose we have **3 documents**:\n",
    "\n",
    "```\n",
    "Doc1: \"I love NLP\"\n",
    "Doc2: \"NLP is amazing\"\n",
    "Doc3: \"I love Python\"\n",
    "```\n",
    "\n",
    "**Step 1: Calculate TF**\n",
    "\n",
    "* Doc1: I(1), love(1), NLP(1) → total 3 words\n",
    "* TF(\"NLP\", Doc1) = 1 / 3 ≈ 0.333\n",
    "\n",
    "**Step 2: Calculate IDF**\n",
    "\n",
    "* NLP appears in 2 documents → DF(\"NLP\") = 2\n",
    "* Total documents N = 3\n",
    "* IDF(\"NLP\") = log(3 / (1+2)) = log(3/3) = log(1) = 0\n",
    "\n",
    "**Step 3: TF-IDF**\n",
    "\n",
    "* TF-IDF(\"NLP\", Doc1) = TF × IDF = 0.333 × 0 = 0\n",
    "\n",
    "> Note: Words appearing in almost all documents get TF-IDF close to 0.\n",
    "\n",
    "* Word \"Python\" appears only in Doc3 → TF-IDF will be high for Doc3.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02350892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['amazing' 'is' 'love' 'nlp' 'python']\n",
      "TF-IDF matrix:\n",
      " [[0.         0.         0.70710678 0.70710678 0.        ]\n",
      " [0.62276601 0.62276601 0.         0.4736296  0.        ]\n",
      " [0.         0.         0.60534851 0.         0.79596054]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is amazing\",\n",
    "    \"I love Python\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Feature names (words)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# TF-IDF matrix\n",
    "print(\"TF-IDF matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd683b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **5. Key Points**\n",
    "\n",
    "* TF-IDF reduces the weight of common words like \"the\", \"is\", \"and\".\n",
    "* Highlights **words unique to a document**.\n",
    "* Widely used in **text classification, search engines, and recommendation systems**.\n",
    "\n",
    "Absolutely! Let’s break down the **intuition behind TF-IDF** in a simple, clear way.\n",
    "\n",
    "---\n",
    "\n",
    "### **TF-IDF Intuition**\n",
    "\n",
    "TF-IDF stands for **Term Frequency – Inverse Document Frequency**. It’s a way to **weight words based on importance** in a corpus. The key idea is:\n",
    "\n",
    "1. Words that appear **often in a document** are important → TF (Term Frequency).\n",
    "2. Words that appear **in many documents** are less informative → IDF (Inverse Document Frequency).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Term Frequency (TF)**\n",
    "\n",
    "* Measures how often a word occurs in a document.\n",
    "* Intuition: If a word occurs more often in a document, it’s probably important for that document.\n",
    "\n",
    "$$\n",
    "TF(word) = \\frac{\\text{Number of times word appears in doc}}{\\text{Total number of words in doc}}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Document: `\"I love NLP and NLP is amazing\"`\n",
    "\n",
    "* Word `\"NLP\"` occurs 2 times out of 6 words → TF(NLP) = 2/6 = 0.33\n",
    "* Word `\"amazing\"` occurs 1 time out of 6 → TF(amazing) = 1/6 = 0.167\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Inverse Document Frequency (IDF)**\n",
    "\n",
    "* Measures how rare a word is across all documents.\n",
    "* Intuition: Words like `\"the\"`, `\"is\"`, `\"and\"` appear everywhere → not important. Words like `\"NLP\"`, `\"Python\"` appear less → more important.\n",
    "\n",
    "$$\n",
    "IDF(word) = \\log \\frac{\\text{Total number of documents}}{\\text{Number of documents containing the word}}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Corpus:\n",
    "\n",
    "1. `\"I love NLP\"`\n",
    "2. `\"NLP is amazing\"`\n",
    "3. `\"I love Python\"`\n",
    "\n",
    "* `\"NLP\"` appears in 2 documents → IDF(NLP) = log(3/2) ≈ 0.176\n",
    "* `\"Python\"` appears in 1 document → IDF(Python) = log(3/1) ≈ 1.098\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: TF-IDF**\n",
    "\n",
    "Finally, multiply TF and IDF to get **TF-IDF weight**:\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(word, doc) = TF(word, doc) \\times IDF(word)\n",
    "$$\n",
    "\n",
    "* **High TF-IDF → Important word in the document**\n",
    "* **Low TF-IDF → Common or less relevant word**\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "* Words that are **frequent in a document** but **rare in the corpus** get the **highest scores**.\n",
    "* Words that are **common across documents** get **low scores**, even if frequent in one document.\n",
    "\n",
    "---\n",
    "\n",
    "**Analogy**\n",
    "\n",
    "* Imagine reading news articles:\n",
    "\n",
    "  * The word `\"the\"` appears in every article → not useful.\n",
    "  * The word `\"NLP\"` appears in only tech articles → important.\n",
    "* TF-IDF mathematically captures this intuition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed22ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
