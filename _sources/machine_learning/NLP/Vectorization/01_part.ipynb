{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381e4657",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "# Bag of Words (BoW)\n",
    "\n",
    "The **Bag of Words** model is a way to **represent text data numerically** by treating a document as a \"bag\" of its words, **ignoring grammar and word order**, but keeping multiplicity (how many times a word appears).\n",
    "\n",
    "Essentially, BoW **converts text into a vector of numbers**, which can then be used for machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## **How BoW Works: Step-by-Step**\n",
    "\n",
    "1. **Collect the corpus**\n",
    "\n",
    "   * A corpus is the entire collection of text documents you want to analyze.\n",
    "   * Example corpus:\n",
    "\n",
    "     ```\n",
    "     Doc1: I love NLP\n",
    "     Doc2: NLP is amazing\n",
    "     Doc3: I love machine learning\n",
    "     ```\n",
    "\n",
    "2. **Create the vocabulary**\n",
    "\n",
    "   * Extract all **unique words** from the corpus.\n",
    "   * Vocabulary = `[I, love, NLP, is, amazing, machine, learning]`\n",
    "\n",
    "3. **Vectorize the documents**\n",
    "\n",
    "   * For each document, count the occurrence of each word in the vocabulary.\n",
    "   * Represent each document as a **vector** of word counts.\n",
    "\n",
    "**Example Table:**\n",
    "\n",
    "| Vocabulary                    | I | love | NLP | is | amazing | machine | learning |\n",
    "| ----------------------------- | - | ---- | --- | -- | ------- | ------- | -------- |\n",
    "| Doc1: I love NLP              | 1 | 1    | 1   | 0  | 0       | 0       | 0        |\n",
    "| Doc2: NLP is amazing          | 0 | 0    | 1   | 1  | 1       | 0       | 0        |\n",
    "| Doc3: I love machine learning | 1 | 1    | 0   | 0  | 0       | 1       | 1        |\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Features of Bag of Words**\n",
    "\n",
    "1. **Simplicity**\n",
    "\n",
    "   * Very easy to understand and implement.\n",
    "\n",
    "2. **Ignores grammar and word order**\n",
    "\n",
    "   * Only considers **presence and frequency** of words, not sequence.\n",
    "   * “I love NLP” and “NLP love I” are treated the same.\n",
    "\n",
    "3. **Frequency-based representation**\n",
    "\n",
    "   * Each vector entry shows how many times a word occurs in the document.\n",
    "\n",
    "4. **Sparse vectors**\n",
    "\n",
    "   * Many words in the vocabulary may not appear in every document, resulting in zeros.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of BoW**\n",
    "\n",
    "* Simple and intuitive.\n",
    "* Works well for basic text classification problems.\n",
    "* Can be combined with **TF-IDF** to improve performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of BoW**\n",
    "\n",
    "* **Ignores word order** → loses context.\n",
    "* **High dimensionality** → very large vocabulary can lead to large sparse vectors.\n",
    "* **Cannot capture semantics** → “good” and “great” are treated as different words.\n",
    "\n",
    "---\n",
    "\n",
    "## **BoW in Python using scikit-learn**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c33cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['amazing' 'is' 'learning' 'love' 'machine' 'nlp']\n",
      "BoW Vectors:\n",
      " [[0 0 0 1 0 1]\n",
      " [1 1 0 0 0 1]\n",
      " [0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is amazing\",\n",
    "    \"I love machine learning\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Vectors:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9e875",
   "metadata": {},
   "source": [
    "\n",
    "**Key Points**\n",
    "\n",
    "* BoW converts text to numeric vectors.\n",
    "* Represents **word presence and frequency**.\n",
    "* Does **not capture meaning or context**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15236dcf",
   "metadata": {},
   "source": [
    "## **What are N-Grams?**\n",
    "\n",
    "* **Definition:** N-Grams are **continuous sequences of n items** (usually words or characters) from a given text.\n",
    "* They help capture **context and word order** which basic Bag-of-Words ignores.\n",
    "* The “n” in N-Grams refers to the **number of items in the sequence**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Types of N-Grams**\n",
    "\n",
    "1. **Unigram (1-gram)**\n",
    "\n",
    "   * Sequence of **1 word**.\n",
    "   * Captures individual word frequency.\n",
    "   * Example:\n",
    "\n",
    "     ```\n",
    "     Text: \"I love NLP\"\n",
    "     Unigrams: [\"I\", \"love\", \"NLP\"]\n",
    "     ```\n",
    "\n",
    "2. **Bigram (2-gram)**\n",
    "\n",
    "   * Sequence of **2 consecutive words**.\n",
    "   * Captures some local context.\n",
    "   * Example:\n",
    "\n",
    "     ```\n",
    "     Text: \"I love NLP\"\n",
    "     Bigrams: [\"I love\", \"love NLP\"]\n",
    "     ```\n",
    "\n",
    "3. **Trigram (3-gram)**\n",
    "\n",
    "   * Sequence of **3 consecutive words**.\n",
    "   * Captures slightly longer context.\n",
    "   * Example:\n",
    "\n",
    "     ```\n",
    "     Text: \"I love NLP models\"\n",
    "     Trigrams: [\"I love NLP\", \"love NLP models\"]\n",
    "     ```\n",
    "\n",
    "4. **n-gram (general)**\n",
    "\n",
    "   * Sequence of **n consecutive words**.\n",
    "   * Example: 4-gram (quadgram) from \"I love NLP models today\":\n",
    "\n",
    "     ```\n",
    "     [\"I love NLP models\", \"love NLP models today\"]\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Why use N-Grams?**\n",
    "\n",
    "* Helps **capture context** and word order in text.\n",
    "* Useful in:\n",
    "\n",
    "  * Text classification\n",
    "  * Sentiment analysis\n",
    "  * Spam detection\n",
    "  * Predictive text / autocomplete\n",
    "* Can be used for both **words** and **characters**:\n",
    "\n",
    "  * **Character-level n-grams** are useful for spelling correction, language modeling, or handling noisy text.\n",
    "\n",
    "---\n",
    "\n",
    "**Trade-offs**\n",
    "\n",
    "| N-Gram Type | Pros                    | Cons                             |\n",
    "| ----------- | ----------------------- | -------------------------------- |\n",
    "| Unigram     | Simple, less memory     | Ignores word order/context       |\n",
    "| Bigram      | Captures local context  | Increases feature space          |\n",
    "| Trigram     | Captures longer context | Higher dimensionality, sparsity  |\n",
    "| Higher N    | More context            | Exponential increase in features |\n",
    "\n",
    "---\n",
    "\n",
    "N-Grams are a **bridge between simple Bag-of-Words and advanced embeddings**, giving models some sense of word sequences without requiring deep learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cde55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'love', 'natural', 'language', 'processing']\n",
      "\n",
      "Unigrams:\n",
      "('I',)\n",
      "('love',)\n",
      "('natural',)\n",
      "('language',)\n",
      "('processing',)\n",
      "\n",
      "Bigrams:\n",
      "('I', 'love')\n",
      "('love', 'natural')\n",
      "('natural', 'language')\n",
      "('language', 'processing')\n",
      "\n",
      "Trigrams:\n",
      "('I', 'love', 'natural')\n",
      "('love', 'natural', 'language')\n",
      "('natural', 'language', 'processing')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import required library\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"I love natural language processing\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Unigrams (1-gram)\n",
    "unigrams = list(ngrams(tokens, 1))\n",
    "print(\"\\nUnigrams:\")\n",
    "for uni in unigrams:\n",
    "    print(uni)\n",
    "\n",
    "# Bigrams (2-gram)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"\\nBigrams:\")\n",
    "for bi in bigrams:\n",
    "    print(bi)\n",
    "\n",
    "# Trigrams (3-gram)\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print(\"\\nTrigrams:\")\n",
    "for tri in trigrams:\n",
    "    print(tri)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
