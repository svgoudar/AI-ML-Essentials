{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a009f352",
   "metadata": {},
   "source": [
    "\n",
    "# Lemmatization\n",
    "\n",
    "* **Definition**: Lemmatization is the process of reducing a word to its **base form (lemma)**, but unlike stemming, it uses **vocabulary + morphological analysis + POS (Part of Speech) tags**.\n",
    "* It produces **real words** (not chopped forms).\n",
    "* Example:\n",
    "\n",
    "  * **Stemming**: *studies â†’ studi*\n",
    "  * **Lemmatization**: *studies â†’ study*\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Why is Lemmatization Better than Stemming?\n",
    "\n",
    "1. **Valid Words**: Lemmas are dictionary words.\n",
    "2. **POS-Aware**: Lemmatizer needs to know if the word is a *noun, verb, adjective*, etc. Example:\n",
    "\n",
    "   * \"better\" â†’\n",
    "\n",
    "     * As an adjective: *good*\n",
    "     * As a verb: *better*\n",
    "3. **Context-Sensitive**: Uses linguistic rules to avoid incorrect chopping.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Process of Lemmatization\n",
    "\n",
    "1. **Tokenization** â†’ Split text into words/sentences.\n",
    "2. **POS Tagging** â†’ Assign part of speech to each word.\n",
    "3. **Lemmatization** â†’ Use dictionary (WordNet in NLTK or spaCy lexicon) to get lemma.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Example\n",
    "\n",
    "Sentence:\n",
    "ðŸ‘‰ *â€œThe cats are sitting outside, and the children were playing happily.â€*\n",
    "\n",
    "* *cats â†’ cat*\n",
    "* *sitting â†’ sit*\n",
    "* *children â†’ child*\n",
    "* *playing â†’ play*\n",
    "* *happily â†’ happily* (adverbs often remain unchanged)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Tools for Lemmatization\n",
    "\n",
    "* **NLTK WordNet Lemmatizer** (basic, needs POS tags for accuracy).\n",
    "* **spaCy Lemmatizer** (more powerful, uses large linguistic models).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ When to Use Lemmatization\n",
    "\n",
    "âœ… When meaning and grammar matter (chatbots, translation, search engines).\n",
    "âœ… For **semantic NLP tasks**: text classification, QA, summarization.\n",
    "âŒ Stemming may be enough for **speed-focused tasks** like keyword extraction.\n",
    "\n",
    "---\n",
    "\n",
    "In short:\n",
    "\n",
    "* **Stemming** â†’ Fast but crude chopping (*connection â†’ connect*).\n",
    "* **Lemmatization** â†’ Slower but linguistically accurate (*better â†’ good*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79fe5f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 660.6 kB/s eta 0:00:20\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "      --------------------------------------- 0.3/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.6/12.8 MB 2.9 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.8/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.0/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.2/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 2.9 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 2.8 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 2.9 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 3.0 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.3/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.5/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.7/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.8/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.1/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.9/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.1/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.2/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.4/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.4/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.6/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.7/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.9/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 3.2 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.8/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.0/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.2/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 7.0/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.8/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.9/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.3/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.6/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.8/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.6/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.1/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 3.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 3.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 3.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 3.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.5/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sangouda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sangouda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sangouda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sangouda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sangouda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['The',\n",
       "  'cats',\n",
       "  'are',\n",
       "  'sitting',\n",
       "  'outside',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'children',\n",
       "  'were',\n",
       "  'playing',\n",
       "  'happily',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'cat',\n",
       "  'be',\n",
       "  'sit',\n",
       "  'outside',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'child',\n",
       "  'be',\n",
       "  'play',\n",
       "  'happily',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'cat',\n",
       "  'be',\n",
       "  'sit',\n",
       "  'outside',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'child',\n",
       "  'be',\n",
       "  'play',\n",
       "  'happily',\n",
       "  '.'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration of Lemmatization using NLTK and spaCy\n",
    "\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "\n",
    "# Download required resources for NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "# Function to convert POS tags to WordNet format for lemmatization\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The cats are sitting outside, and the children were playing happily.\"\n",
    "\n",
    "# Tokenize and POS tagging\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization with POS tags\n",
    "lemmatized_words_nltk = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "\n",
    "# Now using spaCy for comparison\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "lemmatized_words_spacy = [token.lemma_ for token in doc]\n",
    "\n",
    "tokens, lemmatized_words_nltk, lemmatized_words_spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679cd44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
