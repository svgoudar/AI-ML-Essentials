{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de088a28",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "### 1. **Sentence Tokenization (Sentence Segmentation)**\n",
    "\n",
    "* Breaks a paragraph/document into sentences.\n",
    "* Useful for tasks like summarization, translation, and dialogue systems.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  Text: \"I love NLP. It is amazing!\"\n",
    "  Sentence Tokens: [\"I love NLP.\", \"It is amazing!\"]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Word Tokenization**\n",
    "\n",
    "* Splits sentences into individual words.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  Text: \"I love NLP.\"\n",
    "  Word Tokens: [\"I\", \"love\", \"NLP\", \".\"]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Character Tokenization**\n",
    "\n",
    "* Splits text into individual characters.\n",
    "* Useful for handling misspellings, rare words, and languages like Chinese.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  Text: \"NLP\"\n",
    "  Character Tokens: [\"N\", \"L\", \"P\"]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Subword Tokenization**\n",
    "\n",
    "* Breaks words into meaningful sub-units, instead of whole words or characters.\n",
    "* Handles out-of-vocabulary (OOV) words better.\n",
    "* Used in modern NLP models like **BERT, GPT, T5**.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  Word: \"unhappiness\"\n",
    "  Subword Tokens: [\"un\", \"happi\", \"ness\"]\n",
    "  ```\n",
    "\n",
    "Common algorithms:\n",
    "\n",
    "* **Byte Pair Encoding (BPE)**\n",
    "* **WordPiece (used in BERT)**\n",
    "* **SentencePiece (used in T5, XLNet, GPT-2)**\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Whitespace Tokenization**\n",
    "\n",
    "* Simply splits text by spaces.\n",
    "* Fast but naive: `\"NLP-based tokenization\"` â†’ `[\"NLP-based\", \"tokenization\"]`\n",
    "* Problem: doesnâ€™t handle punctuation well.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Regex Tokenization**\n",
    "\n",
    "* Uses regular expressions to define custom rules.\n",
    "* Example: Split by non-alphabetic characters, keep only words.\n",
    "\n",
    "  ```\n",
    "  Text: \"Email me at abc123@gmail.com!\"\n",
    "  Regex Tokens: [\"Email\", \"me\", \"at\", \"abc123\", \"gmail\", \"com\"]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Morphological Tokenization**\n",
    "\n",
    "* Splits words into **roots, prefixes, suffixes** (morphological units).\n",
    "* Example (English): `\"playing\"` â†’ `[\"play\", \"ing\"]`\n",
    "* Example (Turkish): `\"evlerinizden\"` (from your houses) â†’ `[\"ev\" (house), \"ler\" (plural), \"iniz\" (your), \"den\" (from)]`\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Byte-Level Tokenization**\n",
    "\n",
    "* Works at the raw byte level (instead of characters).\n",
    "* Used in **GPT-2 and GPT-3** models.\n",
    "* Handles any language, emoji, or special character.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Tokenization Type       | Example Input                                 | Example Output                      | Used In                    |\n",
    "| ----------------------- | --------------------------------------------- | ----------------------------------- | -------------------------- |\n",
    "| Sentence Tokenization   | \"I love NLP. It's fun.\"                       | \\[\"I love NLP.\", \"It's fun.\"]       | Summarization, Translation |\n",
    "| Word Tokenization       | \"I love NLP.\"                                 | \\[\"I\", \"love\", \"NLP\", \".\"]          | Most NLP tasks             |\n",
    "| Character Tokenization  | \"NLP\"                                         | \\[\"N\", \"L\", \"P\"]                    | Chinese/Japanese, OCR      |\n",
    "| Subword Tokenization    | \"unhappiness\"                                 | \\[\"un\", \"happi\", \"ness\"]            | BERT, GPT, T5              |\n",
    "| Whitespace Tokenization | \"NLP-based model\"                             | \\[\"NLP-based\", \"model\"]             | Simple tasks               |\n",
    "| Regex Tokenization      | \"[abc123@gmail.com](mailto:abc123@gmail.com)\" | \\[\"abc123\", \"gmail\", \"com\"]         | Custom NLP pipelines       |\n",
    "| Morphological           | \"playing\"                                     | \\[\"play\", \"ing\"]                    | Morphology-heavy languages |\n",
    "| Byte-Level Tokenization | \"ðŸ”¥ NLP\"                                      | \\[bytes representing emoji + \"NLP\"] | GPT-2, GPT-3               |\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ In modern NLP, **subword tokenization (BPE, WordPiece, SentencePiece, Byte-Level)** is the most popular because it balances vocabulary size and handles rare words gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6560a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentence Tokenization': ['I love NLP.',\n",
       "  \"It's amazing!\",\n",
       "  \"Unhappiness can't stop us ðŸ˜Š.\"],\n",
       " 'Word Tokenization': ['I',\n",
       "  'love',\n",
       "  'NLP',\n",
       "  '.',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'amazing',\n",
       "  '!',\n",
       "  'Unhappiness',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'stop',\n",
       "  'us',\n",
       "  'ðŸ˜Š',\n",
       "  '.'],\n",
       " 'Character Tokenization': ['I',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'o',\n",
       "  'v',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'N',\n",
       "  'L',\n",
       "  'P',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'I',\n",
       "  't',\n",
       "  \"'\",\n",
       "  's',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'm',\n",
       "  'a'],\n",
       " 'Whitespace Tokenization': ['I',\n",
       "  'love',\n",
       "  'NLP.',\n",
       "  \"It's\",\n",
       "  'amazing!',\n",
       "  'Unhappiness',\n",
       "  \"can't\",\n",
       "  'stop',\n",
       "  'us',\n",
       "  'ðŸ˜Š.'],\n",
       " 'Regex Tokenization': ['I',\n",
       "  'love',\n",
       "  'NLP',\n",
       "  'It',\n",
       "  's',\n",
       "  'amazing',\n",
       "  'Unhappiness',\n",
       "  'can',\n",
       "  't',\n",
       "  'stop',\n",
       "  'us'],\n",
       " 'Subword Tokenization (example)': ['un', 'happi', 'ness'],\n",
       " 'Byte-Level Tokenization (first 20 bytes)': [73,\n",
       "  32,\n",
       "  108,\n",
       "  111,\n",
       "  118,\n",
       "  101,\n",
       "  32,\n",
       "  78,\n",
       "  76,\n",
       "  80,\n",
       "  46,\n",
       "  32,\n",
       "  73,\n",
       "  116,\n",
       "  39,\n",
       "  115,\n",
       "  32,\n",
       "  97,\n",
       "  109,\n",
       "  97]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration of different types of tokenization\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"I love NLP. It's amazing! Unhappiness can't stop us ðŸ˜Š.\"\n",
    "\n",
    "# 1. Sentence Tokenization\n",
    "sent_tokens = sent_tokenize(text)\n",
    "\n",
    "# 2. Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# 3. Character Tokenization\n",
    "char_tokens = list(text)\n",
    "\n",
    "# 4. Whitespace Tokenization\n",
    "whitespace_tokens = text.split()\n",
    "\n",
    "# 5. Regex Tokenization (keep only words, split on non-alphabetic)\n",
    "regex_tokens = re.findall(r\"[A-Za-z]+\", text)\n",
    "\n",
    "# 6. Subword Tokenization (simple example: split prefixes/suffixes manually)\n",
    "example_word = \"unhappiness\"\n",
    "subword_tokens = [\"un\", \"happi\", \"ness\"]\n",
    "\n",
    "# 7. Byte-Level Tokenization (encode text into bytes)\n",
    "byte_tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "results = {\n",
    "    \"Sentence Tokenization\": sent_tokens,\n",
    "    \"Word Tokenization\": word_tokens,\n",
    "    \"Character Tokenization\": char_tokens[:20],  # show first 20 chars\n",
    "    \"Whitespace Tokenization\": whitespace_tokens,\n",
    "    \"Regex Tokenization\": regex_tokens,\n",
    "    \"Subword Tokenization (example)\": subword_tokens,\n",
    "    \"Byte-Level Tokenization (first 20 bytes)\": byte_tokens[:20],\n",
    "}\n",
    "\n",
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
