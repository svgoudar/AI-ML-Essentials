{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47473d34",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "# Text Preprocessing\n",
    "\n",
    "### **Lowercasing**\n",
    "\n",
    "* Convert all text into lowercase to avoid treating `\"Apple\"` and `\"apple\"` as different words.\n",
    "* Example: `\"Natural Language Processing\"` â†’ `\"natural language processing\"`\n",
    "\n",
    "```python\n",
    "text = \"Natural Language Processing\"\n",
    "text_lower = text.lower()\n",
    "print(text_lower)  # 'natural language processing'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Tokenization**\n",
    "\n",
    "* Breaking text into smaller units (sentences or words).\n",
    "* Example: `\"I love NLP.\"` â†’ `['I', 'love', 'NLP', '.']`\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"I love NLP.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Removing Punctuation / Special Characters**\n",
    "\n",
    "* Punctuation often doesnâ€™t add much meaning in text classification tasks.\n",
    "* Example: `\"Hello!!! How are you??\"` â†’ `\"Hello How are you\"`\n",
    "\n",
    "```python\n",
    "import re\n",
    "text = \"Hello!!! How are you??\"\n",
    "cleaned = re.sub(r'[^\\w\\s]', '', text)  # keep only words and spaces\n",
    "print(cleaned)  # 'Hello How are you'\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "text = \"Hello!!! How are you?? I'm fine... thanks :)\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Stopword Removal**\n",
    "\n",
    "* Stopwords are common words (e.g., \"is\", \"the\", \"and\") that carry little meaning.\n",
    "* Example: `\"This is a good book\"` â†’ `\"good book\"`\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "words = word_tokenize(\"This is a good book\")\n",
    "filtered = [w for w in words if w.lower() not in stopwords.words(\"english\")]\n",
    "print(filtered)  # ['good', 'book']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Stemming**\n",
    "\n",
    "* Reducing words to their **root form** (not always valid words).\n",
    "* Example: `\"playing\" â†’ \"play\", \"studies\" â†’ \"studi\"`\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"playing\"))  # play\n",
    "print(stemmer.stem(\"studies\"))  # studi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Lemmatization**\n",
    "\n",
    "* Similar to stemming but uses vocabulary + grammar â†’ produces valid words.\n",
    "* Example: `\"studies\" â†’ \"study\", \"better\" â†’ \"good\"`\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"studies\"))   # study\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # good\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Numbers**\n",
    "\n",
    "* Numbers may or may not be useful.\n",
    "\n",
    "  * Option 1: Remove numbers â†’ `\"I bought 3 apples\"` â†’ `\"I bought apples\"`\n",
    "  * Option 2: Keep numbers but normalize â†’ `\"3\"` â†’ `\"three\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Emojis / Emoticons (Optional)**\n",
    "\n",
    "* Emojis can carry meaning in sentiment analysis.\n",
    "\n",
    "  * Example: `\"I am happy ðŸ˜Š\"` â†’ `\"happy\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **Spelling Correction**\n",
    "\n",
    "* Example: `\"I lve NLP\"` â†’ `\"I love NLP\"`\n",
    "\n",
    "Libraries like `TextBlob` or `SymSpell` can fix spelling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Text Normalization**\n",
    "\n",
    "* Expanding contractions â†’ `\"don't\"` â†’ `\"do not\"`\n",
    "* Normalizing slang â†’ `\"u\"` â†’ `\"you\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **Vectorization**\n",
    "\n",
    "* Final step: convert words into numerical form.\n",
    "\n",
    "  * **Bag of Words (BoW)**\n",
    "  * **TF-IDF (Term Frequency â€“ Inverse Document Frequency)**\n",
    "  * **Word Embeddings (Word2Vec, GloVe, BERT, etc.)**\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow Summary**\n",
    "\n",
    "1. Lowercasing\n",
    "2. Tokenization (sentence/word level)\n",
    "3. Cleaning (punctuation, numbers, special chars)\n",
    "4. Stopword removal\n",
    "5. Normalization (stemming/lemmatization)\n",
    "6. Spelling correction / Slang normalization (if needed)\n",
    "7. Convert to vectors (BoW, TF-IDF, embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "âš¡ In practice: preprocessing steps **depend on the task**.\n",
    "\n",
    "* For **sentiment analysis**, emojis might matter.\n",
    "* For **legal/medical NLP**, numbers and special terms matter.\n",
    "* For **chatbots**, spelling correction and contraction expansion are crucial.\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
