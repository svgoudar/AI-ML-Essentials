{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390acaad",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "# Terminologies\n",
    "\n",
    "\n",
    "* **Corpus** → A collection of text (paragraphs, sentences). Example: one big paragraph is a corpus.\n",
    "\n",
    "* **Document** → An individual sentence or text unit inside the corpus.\n",
    "\n",
    "* **Words** → Each individual element (tokens) within a sentence.\n",
    "\n",
    "* **Vocabulary** → The set of **unique words** in the corpus.\n",
    "\n",
    "* **Tokenization**:\n",
    "\n",
    "  * Breaking text into smaller units (**tokens**).\n",
    "  * Levels of tokenization:\n",
    "\n",
    "    1. **Paragraph → Sentences** (sentence tokenization).\n",
    "    2. **Sentence → Words** (word tokenization).\n",
    "  * Example:\n",
    "\n",
    "    * Corpus: *“My name is Krish. I am also a YouTuber.”*\n",
    "    * Sentence Tokens:\n",
    "\n",
    "      1. “My name is Krish”\n",
    "      2. “I am also a YouTuber”\n",
    "    * Word Tokens: \\[My, name, is, Krish, I, am, also, a, YouTuber].\n",
    "\n",
    "* **Vocabulary Example**:\n",
    "\n",
    "  * Text: *“I like to drink apple juice. My friend likes mango juice.”*\n",
    "  * Total words = 11, Unique words = 9 (if “like” and “likes” are treated separately, count increases).\n",
    "  * Vocabulary = {I, like, to, drink, apple, juice, my, friend, likes, mango}.\n",
    "\n",
    "* Importance → Tokenization is a **key step in text preprocessing** for NLP tasks because models require numerical representations (vectors) of words.\n",
    "\n",
    "---\n",
    "\n",
    "## Elaboration & Deeper Insights\n",
    "\n",
    "1. **Why Corpus, Document, Vocabulary Matter?**\n",
    "\n",
    "   * Corpus = dataset you are working with (like raw text).\n",
    "   * Document = training instance (like one review in sentiment analysis).\n",
    "   * Vocabulary = the dictionary of your text world → forms the **basis for encoding** words into vectors.\n",
    "   * Example: In text classification, your vocabulary determines the **feature space**.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Types of Tokenization**\n",
    "\n",
    "   * **Sentence Tokenization** → Splits by punctuation (., !, ?).\n",
    "\n",
    "     * Useful in summarization, translation.\n",
    "   * **Word Tokenization** → Splits by spaces, punctuation.\n",
    "\n",
    "     * Useful in bag-of-words, embeddings.\n",
    "   * **Subword Tokenization (modern NLP)** → Breaks words into smaller chunks (e.g., “playing” → “play” + “ing”).\n",
    "\n",
    "     * Used in **BERT, GPT, Hugging Face models** to handle unknown words and reduce vocabulary size.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Practical Importance of Tokenization**\n",
    "\n",
    "   * Models like Naive Bayes, SVM, or Deep Learning require text → numbers.\n",
    "   * Tokenization provides the **units for encoding** into:\n",
    "\n",
    "     * **Count Vectors (Bag of Words)**\n",
    "     * **TF-IDF**\n",
    "     * **Word Embeddings (Word2Vec, GloVe)**\n",
    "     * **Transformers embeddings (BERT, GPT)**\n",
    "\n",
    "---\n",
    "\n",
    "4. **Challenges in Tokenization**\n",
    "\n",
    "   * **Ambiguity**: “New York” should be one token, not two.\n",
    "   * **Languages**: In Chinese/Japanese, words are not separated by spaces.\n",
    "   * **Morphology**: “like” vs “likes” vs “liked” → same base meaning but different tokens.\n",
    "   * Modern approaches use **lemmatization** + **subword tokenization** to solve these.\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway**\n",
    "\n",
    "* **Corpus → Documents → Sentences → Words → Vocabulary** forms the basic hierarchy of NLP.\n",
    "* **Tokenization** is the *gateway* to all NLP tasks: without breaking text into structured units, no ML/DL model can process it.\n",
    "* Future steps after tokenization include: **normalization, stopword removal, stemming, lemmatization, embeddings**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b60a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sangouda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['My name is Krish.',\n",
       "  'I am also a YouTuber.',\n",
       "  'I like to drink apple juice.',\n",
       "  'My friend likes mango juice.'],\n",
       " ['My',\n",
       "  'name',\n",
       "  'is',\n",
       "  'Krish',\n",
       "  '.',\n",
       "  'I',\n",
       "  'am',\n",
       "  'also',\n",
       "  'a',\n",
       "  'YouTuber',\n",
       "  '.',\n",
       "  'I',\n",
       "  'like',\n",
       "  'to',\n",
       "  'drink',\n",
       "  'apple',\n",
       "  'juice',\n",
       "  '.',\n",
       "  'My',\n",
       "  'friend',\n",
       "  'likes',\n",
       "  'mango',\n",
       "  'juice',\n",
       "  '.'],\n",
       " {'.',\n",
       "  'I',\n",
       "  'Krish',\n",
       "  'My',\n",
       "  'YouTuber',\n",
       "  'a',\n",
       "  'also',\n",
       "  'am',\n",
       "  'apple',\n",
       "  'drink',\n",
       "  'friend',\n",
       "  'is',\n",
       "  'juice',\n",
       "  'like',\n",
       "  'likes',\n",
       "  'mango',\n",
       "  'name',\n",
       "  'to'},\n",
       " [('.', 4),\n",
       "  ('My', 2),\n",
       "  ('I', 2),\n",
       "  ('juice', 2),\n",
       "  ('name', 1),\n",
       "  ('is', 1),\n",
       "  ('Krish', 1),\n",
       "  ('am', 1),\n",
       "  ('also', 1),\n",
       "  ('a', 1)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration of NLP basics: Corpus, Documents, Vocabulary, Tokenization\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Example corpus\n",
    "corpus = \"My name is Krish. I am also a YouTuber. I like to drink apple juice. My friend likes mango juice.\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(corpus)\n",
    "\n",
    "# Vocabulary (unique words)\n",
    "vocabulary = set(words)\n",
    "\n",
    "# Word Frequency (to show importance in corpus)\n",
    "word_freq = Counter(words)\n",
    "\n",
    "sentences, words, vocabulary, word_freq.most_common(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
