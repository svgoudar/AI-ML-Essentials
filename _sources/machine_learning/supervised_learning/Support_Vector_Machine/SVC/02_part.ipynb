{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f66dae3",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad13e26",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix (Foundation of Metrics)\n",
    "\n",
    "For a binary classifier, the **confusion matrix** is:\n",
    "\n",
    "|                     | Predicted Positive  | Predicted Negative  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "From this table, all evaluation metrics are derived.\n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "* Measures **overall correctness** of predictions.\n",
    "* Good when classes are **balanced**.\n",
    "* Can be misleading if dataset is **imbalanced**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision, Recall, and F1-Score\n",
    "\n",
    "### ✅ Precision\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "* Out of predicted positives, how many are actually positive?\n",
    "* High precision → few **false alarms**.\n",
    "\n",
    "### Recall (Sensitivity / TPR)\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "* Out of actual positives, how many did we catch?\n",
    "* High recall → few **missed detections**.\n",
    "\n",
    "### F1-Score\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "* Harmonic mean of precision & recall.\n",
    "* Useful when dataset is **imbalanced** and we need a **balance**.\n",
    "\n",
    "---\n",
    "\n",
    "### ROC Curve & AUC\n",
    "\n",
    "* **ROC Curve** plots **Recall (TPR)** vs **False Positive Rate (FPR)**.\n",
    "* **AUC (Area Under Curve):**\n",
    "\n",
    "  * Closer to 1 → better classifier.\n",
    "  * 0.5 → random guessing.\n",
    "\n",
    "For SVC, use `decision_function` or `predict_proba` (`probability=True`) to calculate these.\n",
    "\n",
    "---\n",
    "\n",
    "### Metrics for Multi-class SVC\n",
    "\n",
    "Since SVC uses **One-vs-Rest (OvR)** or **One-vs-One (OvO)**:\n",
    "\n",
    "* **Macro average** → averages metric across all classes equally.\n",
    "* **Weighted average** → averages metric weighted by class frequency.\n",
    "\n",
    "These are shown in `classification_report` in scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "### Other Advanced Metrics\n",
    "\n",
    "* **Balanced Accuracy** → adjusts accuracy for imbalanced datasets.\n",
    "* **Cohen’s Kappa** → measures agreement beyond chance.\n",
    "* **Matthews Correlation Coefficient (MCC):** robust for imbalanced data.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Metric        | Meaning                                                | Best Use                      |\n",
    "| ------------- | ------------------------------------------------------ | ----------------------------- |\n",
    "| **Accuracy**  | Overall correctness                                    | Balanced data                 |\n",
    "| **Precision** | Correct positive predictions / All predicted positives | When false alarms costly      |\n",
    "| **Recall**    | Correct positive predictions / All actual positives    | When missing positives costly |\n",
    "| **F1-Score**  | Balance of precision & recall                          | Imbalanced data               |\n",
    "| **ROC-AUC**   | Ranking ability                                        | Threshold selection           |\n",
    "| **MCC**       | Correlation between predictions & truth                | Imbalanced data               |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
