{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de64233",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87b8f5",
   "metadata": {},
   "source": [
    "# Cost Functions \n",
    "\n",
    "## 1. **Maximum Likelihood Estimation (MLE) â€“ Training Objective**\n",
    "\n",
    "NaÃ¯ve Bayes learns probabilities $P(y)$ and $P(x_i|y)$ from data by maximizing the **likelihood** of the training set:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{j=1}^m P(y^{(j)} \\mid x^{(j)}; \\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $m$ = number of training samples\n",
    "* $y^{(j)}$ = class label of sample $j$\n",
    "* $x^{(j)}$ = features of sample $j$\n",
    "* $\\theta$ = parameters (priors + likelihoods).\n",
    "\n",
    "âš¡ In practice, we maximize the **log-likelihood** (to avoid underflow and simplify multiplication into summation):\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\sum_{j=1}^m \\log P(y^{(j)} \\mid x^{(j)}; \\theta)\n",
    "$$\n",
    "\n",
    "ðŸ‘‰ So the implicit **cost function** is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\ell(\\theta) = - \\sum_{j=1}^m \\log P(y^{(j)} \\mid x^{(j)}; \\theta)\n",
    "$$\n",
    "\n",
    "This is essentially **negative log-likelihood (NLL)**, also called **cross-entropy loss**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Cross-Entropy / Log Loss â€“ Evaluation**\n",
    "\n",
    "When evaluating probabilistic classifiers like NaÃ¯ve Bayes, we often use **log loss**:\n",
    "\n",
    "$$\n",
    "\\text{LogLoss} = -\\frac{1}{m} \\sum_{j=1}^m \\sum_{c=1}^k \\mathbf{1}\\{y^{(j)} = c\\} \\log P(y=c \\mid x^{(j)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $k$ = number of classes\n",
    "* $\\mathbf{1}$ = indicator function (1 if true class = $c$, else 0).\n",
    "\n",
    "ðŸ‘‰ This penalizes wrong predictions more when the model is confident but incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Zero-One Loss â€“ Simpler alternative**\n",
    "\n",
    "Sometimes for classification, we also look at **0-1 loss** (not probabilistic, just accuracy-based):\n",
    "\n",
    "$$\n",
    "\\text{0-1 Loss} = \\frac{1}{m} \\sum_{j=1}^m \\mathbf{1}\\{\\hat{y}^{(j)} \\neq y^{(j)}\\}\n",
    "$$\n",
    "\n",
    "This is basically the **misclassification rate**.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* **Training** â†’ NaÃ¯ve Bayes parameters are estimated via **maximum likelihood**, which implicitly minimizes **negative log-likelihood (NLL)**.\n",
    "* **Evaluation** â†’ Common cost functions:\n",
    "\n",
    "  * **Log Loss (cross-entropy)** â†’ best for probabilistic performance.\n",
    "  * **0-1 Loss (error rate)** â†’ best for accuracy comparison.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
