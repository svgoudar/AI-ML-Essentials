{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3bc3d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de4a2c8",
   "metadata": {},
   "source": [
    "\n",
    "# Performance Metrics\n",
    "\n",
    "## 1. **Accuracy**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total predictions}}\n",
    "$$\n",
    "\n",
    "* Measures overall correctness.\n",
    "* Works well when classes are **balanced**.\n",
    "* Misleading for **imbalanced datasets**.\n",
    "\n",
    "  * Example: If 95% of emails are \"ham\", predicting \"ham\" always gives 95% accuracy but is useless.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Confusion Matrix**\n",
    "\n",
    "A **table** comparing predicted vs actual classes.\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "|                     | Predicted Positive  | Predicted Negative  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "From this, we compute other metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Precision**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "* Of all items predicted positive, how many are truly positive?\n",
    "* Good when **false positives** are costly (e.g., classifying ham as spam).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Recall (Sensitivity, True Positive Rate)**\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "* Of all true positives, how many did we correctly find?\n",
    "* Good when **false negatives** are costly (e.g., missing a cancer diagnosis).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **F1 Score**\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "* Harmonic mean of precision and recall.\n",
    "* Useful for **imbalanced data**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **ROC Curve & AUC**\n",
    "\n",
    "* **ROC Curve** â†’ plots True Positive Rate (Recall) vs False Positive Rate (FP / (FP+TN)) for different probability thresholds.\n",
    "* **AUC (Area Under Curve)** â†’ measures how well the model separates classes.\n",
    "\n",
    "  * AUC = 1 â†’ perfect.\n",
    "  * AUC = 0.5 â†’ random guessing.\n",
    "\n",
    "NaÃ¯ve Bayes outputs probabilities ($P(y|x)$), so you can directly use ROC-AUC.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Log Loss (Cross-Entropy Loss)**\n",
    "\n",
    "$$\n",
    "\\text{LogLoss} = -\\frac{1}{m} \\sum_{j=1}^m \\log P(y^{(j)} | x^{(j)})\n",
    "$$\n",
    "\n",
    "* Evaluates the **probabilistic predictions**, not just labels.\n",
    "* Penalizes confident but wrong predictions.\n",
    "* Useful when probability calibration matters (e.g., medical risk prediction).\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Calibration Metrics**\n",
    "\n",
    "NaÃ¯ve Bayes often produces **poorly calibrated probabilities** (too extreme, close to 0 or 1).\n",
    "\n",
    "* Tools like **calibration curves** or **Brier score** check if predicted probabilities match actual outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "For NaÃ¯ve Bayes classification, use:\n",
    "\n",
    "* **Accuracy** â†’ if classes balanced.\n",
    "* **Precision, Recall, F1** â†’ if data imbalanced.\n",
    "* **ROC-AUC** â†’ for probability-based evaluation.\n",
    "* **Log Loss** â†’ if probability quality matters.\n",
    "* **Calibration** â†’ if decision thresholds rely on well-calibrated probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c0dad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8866666666666667,\n",
       " array([[100,   4],\n",
       "        [ 13,  33]]),\n",
       " '              precision    recall  f1-score   support\\n\\n     Class 0       0.88      0.96      0.92       104\\n     Class 1       0.89      0.72      0.80        46\\n\\n    accuracy                           0.89       150\\n   macro avg       0.89      0.84      0.86       150\\nweighted avg       0.89      0.89      0.88       150\\n',\n",
       " 0.4033059439714829,\n",
       " 0.8760451505016723)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_curve, auc, log_loss\n",
    ")\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500, n_features=10, n_informative=5, n_redundant=2,\n",
    "    n_classes=2, weights=[0.7, 0.3], random_state=42\n",
    ")\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = nb.predict(X_test)\n",
    "y_proba = nb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=[\"Class 0\", \"Class 1\"])\n",
    "logloss_val = log_loss(y_test, y_proba)\n",
    "\n",
    "# ROC-AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "(acc, cm, report, logloss_val, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b526dc",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "* **Accuracy**: `0.887` (\\~89%)\n",
    "\n",
    "* **Confusion Matrix**:\n",
    "\n",
    "  ```\n",
    "  [[100   4]\n",
    "   [ 13  33]]\n",
    "  ```\n",
    "\n",
    "  * True Negatives = 100\n",
    "  * False Positives = 4\n",
    "  * False Negatives = 13\n",
    "  * True Positives = 33\n",
    "\n",
    "* **Classification Report**:\n",
    "\n",
    "  ```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "     Class 0       0.88      0.96      0.92       104\n",
    "     Class 1       0.89      0.72      0.80        46\n",
    "\n",
    "    accuracy                           0.89       150\n",
    "   macro avg       0.89      0.84      0.86       150\n",
    "  ```\n",
    "\n",
    "weighted avg       0.89      0.89      0.88       150\n",
    "\n",
    "```\n",
    "\n",
    "- **Log Loss**: `0.403` (lower is better; penalizes wrong confident predictions)  \n",
    "- **ROC-AUC**: `0.876` (good separation; 1.0 = perfect, 0.5 = random)  \n",
    "\n",
    "---\n",
    "\n",
    "These metrics show:\n",
    "- Model is strong overall (~89% accuracy).  \n",
    "- Slight imbalance in recall â†’ Class 1 (minority) has lower recall (0.72), meaning some positives are missed.  \n",
    "- ROC-AUC confirms good probability separation.  \n",
    "\n",
    "---\n",
    "\n",
    "âš¡ Do you want me to also **plot ROC curve + confusion matrix heatmap** for clearer visualization?\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331daad",
   "metadata": {},
   "source": [
    "### **Macro Average (`macro_avg`)**\n",
    "\n",
    "* **Definition**: Takes the **arithmetic mean** of the metric across all classes **without considering class imbalance**.\n",
    "\n",
    "* Formula for precision (example):\n",
    "\n",
    "  $$\n",
    "  \\text{Precision}_{macro} = \\frac{1}{C} \\sum_{i=1}^{C} \\text{Precision}_i\n",
    "  $$\n",
    "\n",
    "  where $C$ = number of classes.\n",
    "\n",
    "* **Effect**:\n",
    "\n",
    "  * Treats **all classes equally**.\n",
    "  * Useful when you want to evaluate **performance per class fairly**, even if one class has fewer samples.\n",
    "\n",
    "ðŸ‘‰ In your NaÃ¯ve Bayes example:\n",
    "\n",
    "* `macro avg precision = 0.89`\n",
    "* `macro avg recall = 0.84`\n",
    "* Shows average performance across **Class 0 and Class 1**, equally weighted.\n",
    "\n",
    "---\n",
    "\n",
    "### **Weighted Average (`weighted_avg`)**\n",
    "\n",
    "* **Definition**: Takes the **support (number of true samples per class)** into account while averaging.\n",
    "\n",
    "* Formula for precision (example):\n",
    "\n",
    "  $$\n",
    "  \\text{Precision}_{weighted} = \\frac{\\sum_{i=1}^{C} ( \\text{Support}_i \\times \\text{Precision}_i )}{\\sum_{i=1}^{C} \\text{Support}_i}\n",
    "  $$\n",
    "\n",
    "* **Effect**:\n",
    "\n",
    "  * Gives **more importance to larger classes**.\n",
    "  * If dataset is imbalanced, the metric will be skewed toward majority class.\n",
    "\n",
    "ðŸ‘‰ In your NaÃ¯ve Bayes example:\n",
    "\n",
    "* `weighted avg precision = 0.89`\n",
    "* `weighted avg recall = 0.89`\n",
    "* Since **Class 0 has 104 samples vs Class 1 has 46**, Class 0 has more influence on the weighted averages.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**:\n",
    "\n",
    "* **Macro Avg** â†’ Equal weight to each class (good for imbalanced dataset evaluation).\n",
    "* **Weighted Avg** â†’ Weighted by class size (good for overall performance reflection).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
