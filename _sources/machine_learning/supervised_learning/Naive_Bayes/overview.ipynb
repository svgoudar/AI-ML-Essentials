{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e738f34e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes is a **supervised machine learning algorithm** used for **classification**.\n",
    "\n",
    "---\n",
    "\n",
    "### Bayes’ Theorem\n",
    "\n",
    "$$\n",
    "P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "* $P(y|X)$: Probability of class $y$ given features $X$ (*posterior*).\n",
    "* $P(X|y)$: Probability of features given class $y$ (*likelihood*).\n",
    "* $P(y)$: Probability of class $y$ (*prior*).\n",
    "* $P(X)$: Probability of features (*evidence*, same for all classes).\n",
    "\n",
    "---\n",
    "\n",
    "### Naive Assumption\n",
    "\n",
    "It assumes **independence among features**, so:\n",
    "\n",
    "$$\n",
    "P(X|y) = \\prod_{i=1}^n P(x_i | y)\n",
    "$$\n",
    "\n",
    "This makes computation fast and simple.\n",
    "\n",
    "---\n",
    "\n",
    "### Types\n",
    "\n",
    "* **Gaussian Naive Bayes** → continuous features, assumes normal distribution.\n",
    "* **Multinomial Naive Bayes** → discrete counts (e.g., word counts in text).\n",
    "* **Bernoulli Naive Bayes** → binary features (e.g., presence/absence of a word).\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. Compute prior probabilities $P(y)$ from data.\n",
    "2. Estimate conditional probabilities $P(x_i|y)$ for each feature.\n",
    "3. Apply Bayes’ theorem to classify a new instance into the class with the highest posterior.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Email classification:\n",
    "\n",
    "* Features: words like *lottery*, *win*, *money*.\n",
    "* If these words appear often in spam, $P(\\text{spam} | X)$ becomes high.\n",
    "* Classify as **spam** if posterior for spam > ham.\n",
    "\n",
    "---\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Very fast, works on large datasets.\n",
    "* Performs well in **text/NLP tasks** (spam, sentiment).\n",
    "* Easy to implement.\n",
    "\n",
    "### Cons\n",
    "\n",
    "* Independence assumption rarely holds.\n",
    "* Fails with highly correlated features.\n",
    "* Zero-frequency problem (fixed by **Laplace smoothing**).\n",
    "\n",
    "---\n",
    "\n",
    "**Naive Bayes = Bayes’ theorem + independence assumption, used for fast probabilistic classification.**\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659adf4e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
