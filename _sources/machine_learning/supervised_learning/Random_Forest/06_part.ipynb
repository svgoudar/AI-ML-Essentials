{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7604faf0",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faec32e",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "* A **cost function** (also called **splitting criterion**) measures **how “good” a split is at a node** in a decision tree.\n",
    "* Random Forest builds **many decision trees**, and each tree uses a cost function to decide **where to split** the data.\n",
    "* The goal is to **minimize prediction error** in the leaves.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Cost Functions for Regression**\n",
    "\n",
    "For **Random Forest Regressor**, the commonly used criteria are:\n",
    "\n",
    "| Criterion                 | What it Measures         | Formula / Intuition                                                                                                                    |\n",
    "| ------------------------- | ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `squared_error` (default) | Variance reduction       | Measures **how much splitting reduces variance of target values** in child nodes. The split that reduces variance the most is chosen.  |\n",
    "| `absolute_error`          | Mean Absolute Error (L1) | Measures **sum of absolute differences** between target values and mean in child nodes. Less sensitive to outliers than squared error. |\n",
    "| `poisson`                 | Poisson deviance         | Used for **count data**, assumes a Poisson distribution of targets.                                                                    |\n",
    "\n",
    "**Variance Reduction Intuition:**\n",
    "\n",
    "* Suppose a node contains target values `[10, 12, 15, 14]`.\n",
    "* Splitting them into `[10, 12]` and `[15, 14]` reduces variance in child nodes.\n",
    "* The split that **minimizes the average variance across children** is chosen.\n",
    "\n",
    "**Mathematically:**\n",
    "\n",
    "$$\n",
    "\\text{Cost (variance)} = \\text{Var(parent)} - \\frac{n_\\text{left}}{n_\\text{parent}} \\text{Var(left)} - \\frac{n_\\text{right}}{n_\\text{parent}} \\text{Var(right)}\n",
    "$$\n",
    "\n",
    "* The **larger the reduction**, the better the split.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Cost Functions for Classification**\n",
    "\n",
    "For **Random Forest Classifier**, the commonly used criteria are:\n",
    "\n",
    "| Criterion  | What it Measures | Formula / Intuition                                                                                                                            |\n",
    "| ---------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `gini`     | Gini Impurity    | Measures **how often a randomly chosen sample would be misclassified** if labeled according to the node’s class distribution. Lower is better. |\n",
    "| `entropy`  | Information Gain | Measures **uncertainty of class labels**. Split that maximizes **information gain** (reduces entropy) is chosen.                               |\n",
    "| `log_loss` | Logarithmic loss | Measures **probability-based error**. More precise for probabilistic classification.                                                           |\n",
    "\n",
    "**Gini Impurity Formula:**\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "* $p_i$ = proportion of class $i$ in the node\n",
    "* Gini = 0 → node is pure (all samples same class)\n",
    "* Gini = max → node is mixed evenly\n",
    "\n",
    "**Entropy Formula:**\n",
    "\n",
    "$$\n",
    "Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "* Measures **uncertainty**\n",
    "* Split that reduces entropy the most → preferred\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Intuition Behind Cost Functions**\n",
    "\n",
    "1. **Regression (Variance Reduction):**\n",
    "\n",
    "   * Split the node so that children are **as homogeneous as possible** in target values.\n",
    "\n",
    "2. **Classification (Gini/Entropy):**\n",
    "\n",
    "   * Split the node so that children are **as pure as possible**, meaning samples in a child node mostly belong to one class.\n",
    "\n",
    "3. **Random Forest Aggregates Trees:**\n",
    "\n",
    "   * Even if one tree chooses a suboptimal split (high cost), averaging across many trees **reduces the impact of bad splits**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Points**\n",
    "\n",
    "* Random Forest **does not have a global cost function**; each tree **optimizes locally at each node**.\n",
    "* Aggregation of predictions across trees handles **variance and bias**, making RF robust.\n",
    "* Choice of criterion affects:\n",
    "\n",
    "  * Model performance (sometimes small differences)\n",
    "  * Sensitivity to outliers (`squared_error` vs `absolute_error`)\n",
    "  * Training speed (`gini` is faster than `entropy`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125c725",
   "metadata": {},
   "source": [
    "## Classification Example (Gini vs Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2946b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification with Gini: 1.0\n",
      "Classification with Entropy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest with Gini\n",
    "rf_gini = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=42)\n",
    "rf_gini.fit(X_train, y_train)\n",
    "y_pred_gini = rf_gini.predict(X_test)\n",
    "print(\"Classification with Gini:\", accuracy_score(y_test, y_pred_gini))\n",
    "\n",
    "# Random Forest with Entropy\n",
    "rf_entropy = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)\n",
    "rf_entropy.fit(X_train, y_train)\n",
    "y_pred_entropy = rf_entropy.predict(X_test)\n",
    "print(\"Classification with Entropy:\", accuracy_score(y_test, y_pred_entropy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6bb5d",
   "metadata": {},
   "source": [
    "## Regression Example (Squared Error vs Absolute Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79985fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared Error -> R²: 0.78, RMSE: 0.31\n",
      "Absolute Error -> R²: 0.79, RMSE: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Create Sample Regression Data\n",
    "np.random.seed(42)\n",
    "X = np.sort(np.random.rand(100,1) * 10, axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.3, X.shape[0])\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest with Squared Error (Variance Reduction)\n",
    "rf_squared = RandomForestRegressor(n_estimators=100, criterion='squared_error', random_state=42)\n",
    "rf_squared.fit(X_train, y_train)\n",
    "y_pred_squared = rf_squared.predict(X_test)\n",
    "\n",
    "# Random Forest with Absolute Error (L1)\n",
    "rf_absolute = RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=42)\n",
    "rf_absolute.fit(X_train, y_train)\n",
    "y_pred_absolute = rf_absolute.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "def print_metrics(name, y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name} -> R²: {r2:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "print_metrics(\"Squared Error\", y_test, y_pred_squared)\n",
    "print_metrics(\"Absolute Error\", y_test, y_pred_absolute)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
