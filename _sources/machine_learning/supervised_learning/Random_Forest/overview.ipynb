{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baef6390",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04dca3",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "* **Random Forest (RF)** is an **ensemble learning method** for regression and classification.\n",
    "* It builds **multiple Decision Trees** and combines their predictions.\n",
    "* Essentially, it’s “**a forest of decision trees**” where each tree votes (for classification) or averages (for regression).\n",
    "\n",
    "**Key Idea:** Combining many weak learners (trees) reduces overfitting and improves generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How Random Forest Works (Intuition)**\n",
    "\n",
    "1. **Bootstrap Sampling (Bagging):**\n",
    "\n",
    "   * Randomly sample data **with replacement** for each tree.\n",
    "   * Each tree gets a slightly different dataset → introduces diversity.\n",
    "\n",
    "2. **Random Feature Selection:**\n",
    "\n",
    "   * At each split in a tree, **only a random subset of features** is considered.\n",
    "   * Prevents trees from being too correlated (e.g., one strong feature dominating all trees).\n",
    "\n",
    "3. **Tree Training:**\n",
    "\n",
    "   * Each tree is trained independently using its bootstrapped dataset and random features.\n",
    "\n",
    "4. **Prediction Aggregation:**\n",
    "\n",
    "   * **Regression:** Average the predictions of all trees.\n",
    "   * **Classification:** Majority vote among trees.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Why Random Forest Works**\n",
    "\n",
    "* **Reduces overfitting:** Individual trees may overfit, but averaging predictions smooths errors.\n",
    "* **Handles high-dimensional data:** Random feature selection prevents a single feature from dominating splits.\n",
    "* **Robust to noise:** Noise in training data affects individual trees, but not the ensemble.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Key Hyperparameters in Random Forest**\n",
    "\n",
    "| Hyperparameter      | Description                                    | Effect                                           |\n",
    "| ------------------- | ---------------------------------------------- | ------------------------------------------------ |\n",
    "| `n_estimators`      | Number of trees in the forest                  | More trees → better performance, slower training |\n",
    "| `max_depth`         | Maximum depth of each tree                     | Controls overfitting                             |\n",
    "| `min_samples_split` | Min samples required to split a node           | Higher → simpler trees                           |\n",
    "| `min_samples_leaf`  | Min samples required at a leaf                 | Prevents leaves with very few samples            |\n",
    "| `max_features`      | Number of features to consider at each split   | Lower → more diversity, higher bias              |\n",
    "| `bootstrap`         | Whether to use bootstrap sampling              | Usually True (bagging)                           |\n",
    "| `criterion`         | Split quality (`squared_error` for regression) | How splits are chosen                            |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Advantages of Random Forest**\n",
    "\n",
    "1. Handles **both regression and classification**.\n",
    "2. Works well on **nonlinear data** without much feature engineering.\n",
    "3. Less prone to **overfitting** than a single Decision Tree.\n",
    "4. Can compute **feature importance**.\n",
    "5. Robust to **outliers and noise**.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Disadvantages / Considerations**\n",
    "\n",
    "1. **Slower than a single Decision Tree** (more trees → more computation).\n",
    "2. Less interpretable than a single Decision Tree (ensemble is a “black box”).\n",
    "3. Needs careful tuning of `n_estimators`, `max_depth`, `max_features` for optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Random Forest Intuition (Visual)**\n",
    "\n",
    "Imagine predicting house prices:\n",
    "\n",
    "* Each tree learns different patterns from random subsets of houses and features.\n",
    "* One tree might overpredict some areas, another underpredicts others.\n",
    "* **Averaging all trees** gives a more accurate and stable prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c512a49",
   "metadata": {},
   "source": [
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
