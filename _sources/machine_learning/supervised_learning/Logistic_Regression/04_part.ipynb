{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb009672",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70962cc4",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Unlike linear regression, we **cannot use Mean Squared Error (MSE)** directly because of the non-linear sigmoid output—it would lead to a **non-convex cost function**, which is hard to optimize.\n",
    "\n",
    "Here’s a breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "First, logistic regression outputs probabilities using the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\hat{y}$ = predicted probability that $y = 1$\n",
    "* $\\theta$ = model parameters\n",
    "* $x$ = input features\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "Logistic regression is based on **Maximum Likelihood Estimation (MLE)**.\n",
    "\n",
    "The **likelihood** is the probability of observing the given data with parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{m} P(y^{(i)} | x^{(i)}; \\theta)\n",
    "$$\n",
    "\n",
    "For binary labels $y \\in \\{0,1\\}$:\n",
    "\n",
    "$$\n",
    "P(y|x; \\theta) = (\\hat{y})^y (1-\\hat{y})^{1-y}\n",
    "$$\n",
    "\n",
    "So the likelihood becomes:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{m} (\\hat{y}^{(i)})^{y^{(i)}} (1-\\hat{y}^{(i)})^{1-y^{(i)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Log-Likelihood\n",
    "\n",
    "We usually take the **log** of the likelihood to simplify calculations:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Cost Function (Negative Log-Likelihood / Log Loss)\n",
    "\n",
    "To **minimize** a function, we take **negative log-likelihood**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big]\n",
    "$$\n",
    "\n",
    "* This is the **primary cost function** used in logistic regression.\n",
    "* Intuition:\n",
    "\n",
    "  * If the model predicts correctly, $\\hat{y}$ is close to $y$, so log loss is small.\n",
    "  * If the model is confident but wrong, log loss is **very large**.\n",
    "\n",
    "---\n",
    "\n",
    "### Variants / Regularized Cost Functions\n",
    "\n",
    "To prevent **overfitting**, we add **regularization**:\n",
    "\n",
    "1. **L2 Regularization (Ridge)**\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
    "$$\n",
    "\n",
    "2. **L1 Regularization (Lasso)**\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big] + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\theta_j|\n",
    "$$\n",
    "\n",
    "* $\\lambda$ = regularization parameter\n",
    "* L2 penalizes large weights\n",
    "* L1 encourages sparsity (many weights become 0)\n",
    "\n",
    "---\n",
    "\n",
    "### Alternative (Less Common) Cost Functions\n",
    "\n",
    "* **Mean Squared Error (MSE):** Sometimes used, but not preferred because it makes the cost function non-convex for logistic regression.\n",
    "* **Hinge Loss:** Used in SVMs, not typical for logistic regression.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary Table**\n",
    "\n",
    "| Cost Function         | Formula                                                       | Notes                                           |   |                          |\n",
    "| --------------------- | ------------------------------------------------------------- | ----------------------------------------------- | - | ------------------------ |\n",
    "| Log Loss (Binary)     | $-\\frac{1}{m} \\sum [y \\log \\hat{y} + (1-y) \\log (1-\\hat{y})]$ | Standard cost for logistic regression           |   |                          |\n",
    "| L2 Regularized        | Log Loss + $\\frac{\\lambda}{2m} \\sum \\theta_j^2$               | Penalizes large weights, prevents overfitting   |   |                          |\n",
    "| L1 Regularized        | Log Loss + ( \\frac{\\lambda}{m} \\sum                           | \\theta\\_j                                       | ) | Encourages sparse models |\n",
    "| MSE (Not Recommended) | $\\frac{1}{2m} \\sum (\\hat{y}-y)^2$                             | Non-convex for logistic regression, rarely used |   |                          |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
