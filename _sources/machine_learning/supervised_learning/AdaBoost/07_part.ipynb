{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695e1cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a782ad6",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Whether you use **AdaBoostClassifier** or **AdaBoostRegressor**, the core hyperparameters are mostly the same:\n",
    "\n",
    "### 1. `n_estimators` (Number of weak learners)\n",
    "\n",
    "* Defines how many weak learners (stumps/trees) to use.\n",
    "* **Too small** â†’ underfitting (model too simple).\n",
    "* **Too large** â†’ may overfit, slower training.\n",
    "* Default = 50 (often needs tuning, e.g., 50â€“500).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `learning_rate` (Shrinkage factor)\n",
    "\n",
    "* Controls how much each weak learner contributes.\n",
    "* Acts as a **regularization parameter**:\n",
    "\n",
    "  * **High value (e.g., 1.0)** â†’ faster learning, risk of overfitting.\n",
    "  * **Low value (e.g., 0.01â€“0.1)** â†’ slower, more robust learning, but may require higher `n_estimators`.\n",
    "* Tradeoff between `n_estimators` and `learning_rate`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `estimator` (Base learner)\n",
    "\n",
    "* By default, **DecisionTreeClassifier(max\\_depth=1)** for classification (decision stumps).\n",
    "* For regression â†’ **DecisionTreeRegressor(max\\_depth=3)** is common.\n",
    "* You can tune:\n",
    "\n",
    "  * **`max_depth`** â†’ deeper trees capture more complex patterns but may overfit.\n",
    "  * **`min_samples_split`, `min_samples_leaf`** â†’ regularization to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `loss` (for AdaBoost Regressor only)\n",
    "\n",
    "* Options: `\"linear\"`, `\"square\"`, `\"exponential\"`.\n",
    "* **Linear** â†’ penalizes errors proportionally.\n",
    "* **Square** â†’ penalizes large errors more heavily.\n",
    "* **Exponential** â†’ increases weight on hard-to-predict points aggressively (can overfit).\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning Strategy\n",
    "\n",
    "### Step 1: Start with Defaults\n",
    "\n",
    "* `n_estimators=50`, `learning_rate=1.0`, `base_estimator=DecisionTree(max_depth=1 or 3)`.\n",
    "\n",
    "### Step 2: Tune `n_estimators` and `learning_rate`\n",
    "\n",
    "* Grid search over ranges like:\n",
    "\n",
    "  ```python\n",
    "  n_estimators = [50, 100, 200, 500]\n",
    "  learning_rate = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "  ```\n",
    "* Find best tradeoff: high enough to capture complexity, but not overfitting.\n",
    "\n",
    "### Step 3: Tune `base_estimator` depth\n",
    "\n",
    "* Try `max_depth = [1, 2, 3, 5]`.\n",
    "* For simple datasets, shallow trees (depth=1). For complex, deeper trees.\n",
    "\n",
    "### Step 4: For regression, tune `loss`\n",
    "\n",
    "* Compare `\"linear\"`, `\"square\"`, `\"exponential\"` based on metrics like MSE or RÂ².\n",
    "\n",
    "### Step 5: Use Cross-Validation\n",
    "\n",
    "* Perform **GridSearchCV** or **RandomizedSearchCV** with cross-validation to avoid overfitting to a single train-test split.\n",
    "\n",
    "---\n",
    "\n",
    "## Handling Overfitting & Underfitting\n",
    "\n",
    "### Overfitting (model too complex)\n",
    "\n",
    "* Reduce `n_estimators`.\n",
    "* Reduce `max_depth` of base learner.\n",
    "* Lower `learning_rate`.\n",
    "* Use `\"linear\"` loss for regression.\n",
    "\n",
    "### Underfitting (model too simple)\n",
    "\n",
    "* Increase `n_estimators`.\n",
    "* Increase `max_depth` of base learner.\n",
    "* Increase `learning_rate`.\n",
    "* Try `\"square\"` or `\"exponential\"` loss for regression.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**In short**:\n",
    "\n",
    "* **Tune `n_estimators` + `learning_rate` first** (they control learning strength).\n",
    "* **Tune base learner complexity (`max_depth`)**.\n",
    "* For regression, also **experiment with `loss` function**.\n",
    "* Use **cross-validation** to avoid overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96d0b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ AdaBoost Classifier Best Params: {'estimator': DecisionTreeClassifier(max_depth=2), 'learning_rate': 1.0, 'n_estimators': 200}\n",
      "ðŸ”¹ Best CV Accuracy: 0.9200000000000002\n",
      "ðŸ”¹ Test Accuracy: 0.9066666666666666\n",
      "\n",
      "ðŸ”¹ AdaBoost Regressor Best Params: {'estimator': DecisionTreeRegressor(max_depth=3), 'learning_rate': 1.0, 'loss': 'square', 'n_estimators': 200}\n",
      "ðŸ”¹ Best CV RÂ²: 0.7771367499875095\n",
      "ðŸ”¹ Test RÂ²: 0.7601366118098305\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# PART 1: AdaBoost Classifier\n",
    "# -------------------------------\n",
    "# Create classification dataset\n",
    "X_cls, y_cls = make_classification(\n",
    "    n_samples=500, n_features=10, n_informative=5, n_redundant=2,\n",
    "    random_state=42\n",
    ")\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_cls, y_cls, test_size=0.3, random_state=42)\n",
    "\n",
    "# Grid search for classifier\n",
    "param_grid_cls = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.5, 1.0],\n",
    "    \"estimator\": [DecisionTreeClassifier(max_depth=1),\n",
    "                  DecisionTreeClassifier(max_depth=2)]\n",
    "}\n",
    "\n",
    "grid_cls = GridSearchCV(\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    param_grid_cls,\n",
    "    cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "grid_cls.fit(Xc_train, yc_train)\n",
    "\n",
    "print(\"ðŸ”¹ AdaBoost Classifier Best Params:\", grid_cls.best_params_)\n",
    "print(\"ðŸ”¹ Best CV Accuracy:\", grid_cls.best_score_)\n",
    "print(\"ðŸ”¹ Test Accuracy:\", grid_cls.score(Xc_test, yc_test))\n",
    "\n",
    "# -------------------------------\n",
    "# PART 2: AdaBoost Regressor\n",
    "# -------------------------------\n",
    "# Create regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=500, n_features=10, noise=15.0, random_state=42\n",
    ")\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "# Grid search for regressor\n",
    "param_grid_reg = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.5, 1.0],\n",
    "    \"estimator\": [DecisionTreeRegressor(max_depth=2),\n",
    "                  DecisionTreeRegressor(max_depth=3)],\n",
    "    \"loss\": [\"linear\", \"square\", \"exponential\"]\n",
    "}\n",
    "\n",
    "grid_reg = GridSearchCV(\n",
    "    AdaBoostRegressor(random_state=42),\n",
    "    param_grid_reg,\n",
    "    cv=5, scoring=\"r2\"\n",
    ")\n",
    "grid_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "print(\"\\nðŸ”¹ AdaBoost Regressor Best Params:\", grid_reg.best_params_)\n",
    "print(\"ðŸ”¹ Best CV RÂ²:\", grid_reg.best_score_)\n",
    "print(\"ðŸ”¹ Test RÂ²:\", grid_reg.score(Xr_test, yr_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
