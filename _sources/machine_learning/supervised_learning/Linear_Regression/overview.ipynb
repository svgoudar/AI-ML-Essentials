{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddd5adb",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Regression\n",
    "\n",
    "### What is Linear Regression\n",
    "\n",
    "Linear Regression is one of the most fundamental and widely used **supervised learning algorithms** in machine learning.\n",
    "\n",
    "* It is used when the **target variable (Y)** is **continuous** (e.g., predicting salary, house price, temperature).\n",
    "* The goal is to model the **relationship** between one or more **independent variables (X)** and the **dependent variable (Y)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Equation of Linear Regression\n",
    "\n",
    "* **Simple Linear Regression (one feature):**\n",
    "\n",
    "$$\n",
    "Y = \\theta_0 + \\theta_1 X + \\epsilon\n",
    "$$\n",
    "\n",
    "* **Multiple Linear Regression (multiple features):**\n",
    "\n",
    "$$\n",
    "Y = \\theta_0 + \\theta_1X_1 + \\theta_2X_2 + ... + \\theta_nX_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\theta_0$ → Intercept (bias term)\n",
    "* $\\theta_1, \\theta_2, ..., \\theta_n$ → Coefficients (slopes/weights)\n",
    "* $\\epsilon$ → Error term (captures noise not explained by model)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts**\n",
    "\n",
    "* **Hypothesis Function** → Predicts Y from X\n",
    "* **Cost Function (MSE)** → Measures error between predictions and actual values\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "* **Optimization (Gradient Descent or OLS)** → Finds the best $\\theta$ that minimizes cost\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Linear Regression\n",
    "\n",
    "* **Simple Linear Regression** → One input feature\n",
    "* **Multiple Linear Regression** → More than one input feature\n",
    "* **Polynomial Regression** → Features transformed into polynomial terms (non-linear relationship handled with linear model)\n",
    "* **Regularized Regression** → Ridge (L2), Lasso (L1), ElasticNet (combination)\n",
    "\n",
    "---\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "Linear regression relies on assumptions:\n",
    "\n",
    "1. Linearity → Relationship between X and Y is linear\n",
    "2. Independence of errors\n",
    "3. Homoscedasticity (constant variance of errors)\n",
    "4. Normal distribution of errors\n",
    "5. No multicollinearity (in multiple regression)\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "* **MSE (Mean Squared Error)**\n",
    "* **RMSE (Root Mean Squared Error)**\n",
    "* **MAE (Mean Absolute Error)**\n",
    "* **R² (Coefficient of Determination)**\n",
    "\n",
    "---\n",
    "\n",
    "### Applications\n",
    "\n",
    "* Predicting **house prices**\n",
    "* Estimating **sales vs. advertising budget**\n",
    "* Predicting **student marks vs. study hours**\n",
    "* Forecasting **demand or stock prices** (basic approach)\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In summary:**\n",
    "Linear Regression is the foundation of regression algorithms. It tries to fit the \"best fit line (or plane)\" that minimizes errors between actual values and predicted values, under certain assumptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af27d9",
   "metadata": {},
   "source": [
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
