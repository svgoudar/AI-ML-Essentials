{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534b36cd",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acbf70",
   "metadata": {},
   "source": [
    "\n",
    "# Hyperparameter Tuning methods\n",
    "\n",
    "There are several ways to search for the **best hyperparameters** when tuning machine learning models. Here are the main types:\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Hyperparameter Search\n",
    "\n",
    "### 1. **Manual Search**\n",
    "\n",
    "* Try parameters by hand based on intuition or domain knowledge.\n",
    "* Example: test `Œ± = 0.1, 1, 10` for Ridge.\n",
    "* ‚úÖ Simple, but ‚ùå inefficient and may miss optimal values.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Grid Search**\n",
    "\n",
    "* Define a grid of hyperparameter values.\n",
    "* Try all combinations exhaustively with **cross-validation**.\n",
    "* Example:\n",
    "\n",
    "  ```python\n",
    "  alpha = [0.01, 0.1, 1, 10, 100]\n",
    "  l1_ratio = [0.1, 0.5, 0.9]\n",
    "  ```\n",
    "\n",
    "  ‚Üí Tests 5 √ó 3 = 15 combinations.\n",
    "* ‚úÖ Systematic, guarantees best within grid.\n",
    "* ‚ùå Expensive if grid is large.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Random Search**\n",
    "\n",
    "* Instead of testing all values, randomly sample combinations from given distributions.\n",
    "* Example: `alpha ‚àº Uniform(0.001, 100)`\n",
    "* ‚úÖ More efficient than grid, can cover large spaces.\n",
    "* ‚ùå May miss exact optimal if unlucky.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Bayesian Optimization**\n",
    "\n",
    "* Uses past evaluation results to model performance as a probability distribution.\n",
    "* Chooses new hyperparameters that are most promising.\n",
    "* ‚úÖ Finds optimal faster than grid/random.\n",
    "* ‚ùå More complex, needs specialized libraries (`optuna`, `scikit-optimize`, `hyperopt`).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Gradient-Based Optimization** (advanced)\n",
    "\n",
    "* Uses gradients of the loss with respect to hyperparameters.\n",
    "* Works mainly for continuous hyperparameters.\n",
    "* Rare in practice because many hyperparameters (like `max_depth`) are discrete.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Evolutionary / Genetic Algorithms**\n",
    "\n",
    "* Treat hyperparameters like genes.\n",
    "* Randomly mutate and crossover values across generations.\n",
    "* ‚úÖ Can escape local optima.\n",
    "* ‚ùå Slower, harder to tune.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Successive Halving / Hyperband**\n",
    "\n",
    "* Start with many random hyperparameter sets.\n",
    "* Train each briefly.\n",
    "* Discard poorly performing ones early, keep only the best for longer training.\n",
    "* ‚úÖ Efficient, reduces wasted computation.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Method                         | Strategy                          | Pros                        | Cons                |\n",
    "| ------------------------------ | --------------------------------- | --------------------------- | ------------------- |\n",
    "| Manual Search                  | Trial-and-error                   | Simple                      | Not systematic      |\n",
    "| Grid Search                    | Exhaustive combinations           | Guaranteed best in grid     | Expensive           |\n",
    "| Random Search                  | Random sampling                   | Efficient, scalable         | No guarantee        |\n",
    "| Bayesian Optimization          | Probabilistic model-guided search | Fast convergence            | Complex             |\n",
    "| Gradient-Based                 | Gradient descent on hyperparams   | Precise for continuous vars | Rarely practical    |\n",
    "| Evolutionary Algorithms        | Mutation + crossover              | Escapes local optima        | Slow                |\n",
    "| Hyperband / Successive Halving | Early stopping bad configs        | Saves compute               | Needs careful setup |\n",
    "\n",
    "---\n",
    "\n",
    "üëâ In practice:\n",
    "\n",
    "* For **small problems** ‚Üí Grid Search.\n",
    "* For **large spaces** ‚Üí Random Search or Hyperband.\n",
    "* For **serious optimization** ‚Üí Bayesian Optimization (e.g., `Optuna`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5837d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV best params: {'alpha': 0.1}\n",
      "GridSearchCV best CV score: 0.9907142472562647\n",
      "GridSearchCV test R2: 0.9934316711441261\n",
      "\n",
      "RandomizedSearchCV best params: {'alpha': 0.021544346900318846}\n",
      "RandomizedSearchCV best CV score: 0.9907141636217599\n",
      "RandomizedSearchCV test R2: 0.9934566226827182\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=200, n_features=10, noise=15, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------- Grid Search ----------------\n",
    "ridge = Ridge()\n",
    "\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100, 1000]}  # exhaustive list\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"GridSearchCV best params:\", grid_search.best_params_)\n",
    "print(\"GridSearchCV best CV score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred_grid = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"GridSearchCV test R2:\", r2_score(y_test, y_pred_grid))\n",
    "\n",
    "# ---------------- Random Search ----------------\n",
    "param_dist = {'alpha': np.logspace(-3, 3, 100)}  # random sampling from wide range\n",
    "random_search = RandomizedSearchCV(ridge, param_dist, n_iter=10, cv=5, scoring='r2', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nRandomizedSearchCV best params:\", random_search.best_params_)\n",
    "print(\"RandomizedSearchCV best CV score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred_rand = random_search.best_estimator_.predict(X_test)\n",
    "print(\"RandomizedSearchCV test R2:\", r2_score(y_test, y_pred_rand))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
