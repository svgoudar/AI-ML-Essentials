{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888b410f",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6016bebc",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize a cost function by updating model parameters (θ). The types mainly differ in **how much data** they use at each update step.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Gradient Descent\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "* **Definition**: Uses the **entire training dataset** to compute the gradient of the cost function in each iteration.\n",
    "* **Formula**:\n",
    "\n",
    "  $$\n",
    "  \\theta := \\theta - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})\n",
    "  $$\n",
    "\n",
    "  where $m$ = number of training examples.\n",
    "* **Pros**:\n",
    "\n",
    "  * Converges to the **global minimum** for convex functions (like linear regression).\n",
    "  * Stable updates.\n",
    "* **Cons**:\n",
    "\n",
    "  * Very slow for large datasets.\n",
    "  * Requires huge memory since it must load all data at once.\n",
    "* ✅ Best suited for **small to medium datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "* **Definition**: Updates parameters **for each training example** one at a time.\n",
    "* **Formula**:\n",
    "\n",
    "  $$\n",
    "  \\theta := \\theta - \\alpha \\cdot \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})\n",
    "  $$\n",
    "* **Pros**:\n",
    "\n",
    "  * Much faster (frequent updates).\n",
    "  * Can escape **local minima** due to noisy updates.\n",
    "* **Cons**:\n",
    "\n",
    "  * Updates are noisy → cost function fluctuates rather than smoothly converging.\n",
    "  * Harder to reach exact global minimum (oscillates around it).\n",
    "* ✅ Best for **very large datasets** or **online learning**.\n",
    "\n",
    "---\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "* **Definition**: A compromise between Batch and SGD. Uses **small random subsets (mini-batches)** of the data to update parameters.\n",
    "* **Formula**:\n",
    "\n",
    "  $$\n",
    "  \\theta := \\theta - \\alpha \\cdot \\frac{1}{b} \\sum_{i=1}^{b} \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})\n",
    "  $$\n",
    "\n",
    "  where $b$ = mini-batch size (e.g., 32, 64, 128).\n",
    "* **Pros**:\n",
    "\n",
    "  * Faster and more efficient than pure Batch.\n",
    "  * Less noisy than SGD.\n",
    "  * Can leverage **vectorization** (parallel processing on GPUs).\n",
    "* **Cons**:\n",
    "\n",
    "  * Choosing batch size is tricky (too small → noisy, too large → slow).\n",
    "* ✅ Best for **deep learning and neural networks**.\n",
    "\n",
    "### Comparison Summary\n",
    "\n",
    "| Type              | Update Frequency      | Speed            | Stability   | Use Case                        |\n",
    "| ----------------- | --------------------- | ---------------- | ----------- | ------------------------------- |\n",
    "| **Batch GD**      | After full dataset    | Slow             | Very stable | Small datasets                  |\n",
    "| **Stochastic GD** | After each data point | Fast per step    | Noisy       | Large datasets, online learning |\n",
    "| **Mini-Batch GD** | After subset (batch)  | Fast + efficient | Balanced    | Deep Learning                   |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
