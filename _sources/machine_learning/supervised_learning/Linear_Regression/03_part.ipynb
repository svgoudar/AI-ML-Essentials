{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8806ad50",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff7ca8",
   "metadata": {},
   "source": [
    "\n",
    "## Convergence Algorithm\n",
    "\n",
    "The **convergence algorithm** is an iterative process used in **gradient descent** to update model parameters ($\\theta_0, \\theta_1, \\dots$) until the **cost function** (error) reaches its **minimum value**.\n",
    "\n",
    "In **linear regression**, convergence means reaching the **global minimum** of the cost function (Mean Squared Error), which gives us the **best-fit line**.\n",
    "\n",
    "---\n",
    "\n",
    "### The Update Rule (Core Formula)\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\theta_j$ â†’ parameter (intercept, slope, etc.)\n",
    "* $\\alpha$ â†’ learning rate (step size)\n",
    "* $\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$ â†’ derivative of cost function w\\.r.t parameter (i.e., slope of the curve)\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition Behind It\n",
    "\n",
    "1. **Cost Function Curve** â†’ looks like a bowl (U-shape).\n",
    "2. **Derivative (Slope) tells direction**:\n",
    "\n",
    "   * If slope is **positive** â†’ decrease parameter.\n",
    "   * If slope is **negative** â†’ increase parameter.\n",
    "3. By repeatedly adjusting parameters in the opposite direction of the slope, the algorithm **slides down the bowl** toward the **global minimum**.\n",
    "\n",
    "---\n",
    "\n",
    "### Role of Learning Rate ($\\alpha$)\n",
    "\n",
    "* Small $\\alpha$ â†’ tiny steps â†’ slow convergence.\n",
    "* Large $\\alpha$ â†’ big jumps â†’ may overshoot, never converge.\n",
    "* Balanced $\\alpha$ â†’ smooth, efficient convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Stop (Convergence Criteria)\n",
    "\n",
    "We say the algorithm has **converged** when:\n",
    "\n",
    "* The **change in cost function** between iterations is negligible (e.g., < 0.0001).\n",
    "* Or, after a fixed number of iterations.\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Intuition\n",
    "\n",
    "* Imagine standing on a hill (cost function surface).\n",
    "* Each step, you check the slope of the ground.\n",
    "* Walk in the **downhill direction** with small, controlled steps.\n",
    "* Eventually, you reach the **lowest valley** (global minimum).\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "The convergence algorithm in gradient descent **optimizes parameters** by repeatedly moving them in the direction that reduces error, until the cost function cannot be minimized further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create synthetic data\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([1.2, 1.9, 3.0, 3.9, 5.1])\n",
    "m = len(X)\n",
    "\n",
    "# Initialize theta values\n",
    "theta0, theta1 = 0.0, 0.0\n",
    "alpha = 0.01\n",
    "iterations = 100\n",
    "\n",
    "# Store history\n",
    "theta0_hist = []\n",
    "theta1_hist = []\n",
    "cost_hist = []\n",
    "\n",
    "# Hypothesis function\n",
    "def hypothesis(X, theta0, theta1):\n",
    "    return theta0 + theta1 * X\n",
    "\n",
    "# Cost function (MSE)\n",
    "def compute_cost(X, y, theta0, theta1):\n",
    "    predictions = hypothesis(X, theta0, theta1)\n",
    "    errors = predictions - y\n",
    "    return (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "\n",
    "# Gradient Descent\n",
    "for _ in range(iterations):\n",
    "    predictions = hypothesis(X, theta0, theta1)\n",
    "    errors = predictions - y\n",
    "\n",
    "    # Compute gradients\n",
    "    d_theta0 = (1 / m) * np.sum(errors)\n",
    "    d_theta1 = (1 / m) * np.sum(errors * X)\n",
    "\n",
    "    # Update parameters\n",
    "    theta0 -= alpha * d_theta0\n",
    "    theta1 -= alpha * d_theta1\n",
    "\n",
    "    # Record history\n",
    "    theta0_hist.append(theta0)\n",
    "    theta1_hist.append(theta1)\n",
    "    cost_hist.append(compute_cost(X, y, theta0, theta1))\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot cost over iterations (convergence)\n",
    "axs[0].plot(range(iterations), cost_hist, color='blue')\n",
    "axs[0].set_title(\"Convergence of Cost Function\")\n",
    "axs[0].set_xlabel(\"Iterations\")\n",
    "axs[0].set_ylabel(\"Cost (MSE)\")\n",
    "\n",
    "# Plot gradient descent path in parameter space\n",
    "axs[1].plot(theta0_hist, theta1_hist, marker='o', color='red')\n",
    "axs[1].set_title(\"Gradient Descent Path in Parameter Space\")\n",
    "axs[1].set_xlabel(\"theta0\")\n",
    "axs[1].set_ylabel(\"theta1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfbd36",
   "metadata": {},
   "source": [
    "## Global Minima vs Local Minima\n",
    "\n",
    "### Global Minima\n",
    "\n",
    "* The **lowest possible point** of a cost function across the entire function domain.\n",
    "* In optimization (like regression), reaching the **global minima** means weâ€™ve found the **best solution**:\n",
    "\n",
    "  * The cost (error) is the smallest possible.\n",
    "  * Our model is optimally fitted to the data.\n",
    "\n",
    "ðŸ‘‰ Example: In linear regression, the cost function (MSE) is convex (U-shaped parabola or bowl).\n",
    "\n",
    "* A convex function has **only one global minima**.\n",
    "* Thatâ€™s why gradient descent in linear regression always converges to the global minimum (if the learning rate is good).\n",
    "\n",
    "---\n",
    "\n",
    "### Local Minima\n",
    "\n",
    "* Points where the function value is lower than nearby points, but **not the lowest overall**.\n",
    "* Think of a â€œvalleyâ€ in a mountain range â€” itâ€™s lower than surrounding hills but not the deepest valley overall.\n",
    "\n",
    "ðŸ‘‰ Example: In more complex models (like neural networks), the cost surface may have **multiple local minima**.\n",
    "\n",
    "* Gradient descent might get â€œstuckâ€ in one of these valleys instead of reaching the global minimum.\n",
    "* Thatâ€™s why techniques like **momentum, Adam optimizer, random restarts** are used in deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Intuition\n",
    "\n",
    "* Imagine a ball rolling downhill on a curve (cost function).\n",
    "* If the curve is convex (parabola), it always reaches the global minima.\n",
    "* If the curve has multiple dips, the ball may stop at a local minima instead of reaching the lowest global point.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the loss function: Loss(Î¸) = Î¸^4 - 4Î¸^2 + 0.5Î¸ + 5\n",
    "def loss_function(theta):\n",
    "    return theta**4 - 4*theta**2 + 0.5*theta + 5\n",
    "\n",
    "# Generate theta values\n",
    "theta = np.arange(-2.5, 2.51, 0.01)\n",
    "loss = loss_function(theta)\n",
    "\n",
    "# Find approximate minima for annotation\n",
    "global_min_theta = -1.414\n",
    "global_min_loss = loss_function(global_min_theta)  # â‰ˆ 2\n",
    "local_min_theta = 1.414\n",
    "local_min_loss = loss_function(local_min_theta)  # â‰ˆ 4\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(theta, loss, label='Loss Function', color='indigo', linewidth=2)\n",
    "\n",
    "# Mark global minimum\n",
    "plt.scatter([global_min_theta], [global_min_loss], color='green', s=100, label='Global Minimum')\n",
    "plt.annotate('Global Minimum', xy=(global_min_theta, global_min_loss), \n",
    "             xytext=(global_min_theta - 1, global_min_loss + 5),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.05))\n",
    "\n",
    "# Mark local minimum\n",
    "plt.scatter([local_min_theta], [local_min_loss], color='orange', s=100, label='Local Minimum')\n",
    "plt.annotate('Local Minimum', xy=(local_min_theta, local_min_loss), \n",
    "             xytext=(local_min_theta + 0.5, local_min_loss + 5),\n",
    "             arrowprops=dict(facecolor='orange', shrink=0.05))\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Loss Function with Global and Local Minima', fontsize=14)\n",
    "plt.xlabel('Parameter (Î¸)', fontsize=12)\n",
    "plt.ylabel('Loss (Error)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
