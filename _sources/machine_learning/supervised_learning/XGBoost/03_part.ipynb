{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90fbcba",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0615b1",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Input Preparation**\n",
    "\n",
    "* **Data Formatting**\n",
    "\n",
    "  * Input data is converted into **DMatrix** (optimized internal structure).\n",
    "  * Handles sparse data efficiently (compressed column storage).\n",
    "* **Labels & Weights**\n",
    "\n",
    "  * Target variable $y$, instance weights, and optional missing values are stored.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Initialization**\n",
    "\n",
    "* Start with a **base prediction**:\n",
    "\n",
    "  * For regression → mean of targets.\n",
    "  * For classification → log-odds of positive class.\n",
    "* This becomes the initial model $f_0(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Iterative Boosting Rounds**\n",
    "\n",
    "For each boosting round $t = 1, 2, \\dots, T$:\n",
    "\n",
    "### (a) **Compute Gradients & Hessians**\n",
    "\n",
    "* For each data point $i$, compute:\n",
    "\n",
    "  $$\n",
    "  g_i = \\frac{\\partial L(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}, \\quad\n",
    "  h_i = \\frac{\\partial^2 L(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}\n",
    "  $$\n",
    "* $g_i$ = gradient (direction of steepest loss decrease).\n",
    "* $h_i$ = Hessian (curvature, helps with step size).\n",
    "\n",
    "---\n",
    "\n",
    "### (b) **Build a Decision Tree**\n",
    "\n",
    "* At each node, evaluate possible splits using **Gain**:\n",
    "\n",
    "  $$\n",
    "  \\text{Gain} = \\frac{1}{2}\\left(\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right) - \\gamma\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "\n",
    "  * $G, H$ = sums of gradients & Hessians.\n",
    "  * $\\lambda$ = L2 regularization.\n",
    "  * $\\gamma$ = minimum loss reduction required.\n",
    "* If Gain > 0 (or > gamma), the split is made.\n",
    "\n",
    "---\n",
    "\n",
    "### (c) **Pruning / Stopping Splits**\n",
    "\n",
    "* Stop splitting if:\n",
    "\n",
    "  * Depth > `max_depth`.\n",
    "  * Gain < `gamma`.\n",
    "  * Node samples too few (`min_child_weight`).\n",
    "\n",
    "---\n",
    "\n",
    "### (d) **Assign Leaf Weights**\n",
    "\n",
    "* For each leaf, compute optimal weight:\n",
    "\n",
    "  $$\n",
    "  w^* = -\\frac{\\sum_i g_i}{\\sum_i h_i + \\lambda}\n",
    "  $$\n",
    "* This weight minimizes the loss locally.\n",
    "\n",
    "---\n",
    "\n",
    "### (e) **Update Model**\n",
    "\n",
    "* Update predictions:\n",
    "\n",
    "  $$\n",
    "  \\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + \\eta f_t(x_i)\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "\n",
    "  * $f_t(x)$ = new tree’s output.\n",
    "  * $\\eta$ = learning rate (shrinkage factor).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Regularization**\n",
    "\n",
    "* Built-in penalties:\n",
    "\n",
    "  * **L1 (α)** → sparsity, feature selection.\n",
    "  * **L2 (λ)** → weight shrinkage.\n",
    "* Prevents overfitting by discouraging overly complex trees.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Prediction**\n",
    "\n",
    "* After T boosting rounds:\n",
    "\n",
    "  $$\n",
    "  \\hat{y}_i = \\sum_{t=1}^T \\eta f_t(x_i)\n",
    "  $$\n",
    "* Apply sigmoid (classification) or leave as-is (regression).\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Model Evaluation**\n",
    "\n",
    "* Use evaluation metrics (`logloss`, `rmse`, `auc`, etc.) on **train** and **validation** sets.\n",
    "* Can apply **early stopping** if validation loss stops improving.\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow Summary (Simplified Pipeline)**\n",
    "\n",
    "1. **Input data → DMatrix**\n",
    "2. **Initialize model** (baseline prediction)\n",
    "3. **For each boosting round**:\n",
    "\n",
    "   * Compute gradients & Hessians\n",
    "   * Build tree using Gain formula\n",
    "   * Stop splitting if conditions met\n",
    "   * Assign leaf weights\n",
    "   * Update model with learning rate\n",
    "4. **Apply regularization**\n",
    "5. **Repeat until max\\_rounds or early stopping**\n",
    "6. **Final model = sum of all trees**\n",
    "\n",
    "---\n",
    "\n",
    "**Key Intuition**\n",
    "XGBoost is **gradient descent on trees** — it learns **residual patterns** iteratively, but with second-order accuracy (gradients + Hessians), making it faster and more precise than AdaBoost or Gradient Boosting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
