{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8058bd",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909c213",
   "metadata": {},
   "source": [
    "# Cost Functions\n",
    "\n",
    "XGBoost is **not a single model** but a **framework** that supports different cost functions (a.k.a. loss functions) depending on whether youâ€™re solving regression or classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Functions in **XGBRegressor**\n",
    "\n",
    "In regression, the task is to predict a continuous value. XGBRegressor uses **differentiable loss functions** that measure prediction error.\n",
    "\n",
    "### Common loss functions:\n",
    "\n",
    "1. **Squared Error (default)**\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2\n",
    "   $$\n",
    "\n",
    "   * Penalizes larger errors more heavily.\n",
    "   * Smooth and differentiable.\n",
    "   * Works well when errors are normally distributed.\n",
    "\n",
    "2. **Absolute Error (MAE)**\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{y}) = |y - \\hat{y}|\n",
    "   $$\n",
    "\n",
    "   * Robust to outliers (less penalty for extreme values).\n",
    "   * Slower optimization since gradient is less smooth.\n",
    "\n",
    "3. **Huber Loss** (mix between MSE & MAE)\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{y}) =\n",
    "   \\begin{cases}\n",
    "   \\frac{1}{2}(y - \\hat{y})^2 & |y - \\hat{y}| \\leq \\delta \\\\\n",
    "   \\delta |y - \\hat{y}| - \\frac{1}{2}\\delta^2 & |y - \\hat{y}| > \\delta\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "   * Balances robustness and sensitivity.\n",
    "\n",
    "4. **Quantile Loss**\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{y}) = \\max(\\alpha(y - \\hat{y}), (1-\\alpha)(\\hat{y} - y))\n",
    "   $$\n",
    "\n",
    "   * Useful for prediction intervals (not just mean).\n",
    "\n",
    "ðŸ”‘ In practice, XGBRegressor defaults to **squared error loss** unless specified (`objective=\"reg:squarederror\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Functions in **XGBClassifier**\n",
    "\n",
    "For classification, the task is to predict probabilities (then assign classes).\n",
    "The cost functions measure **probability calibration** (how close predicted probabilities are to true labels).\n",
    "\n",
    "### Common loss functions:\n",
    "\n",
    "1. **Logistic Loss (Binary Classification)**\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{p}) = - \\big( y \\log(\\hat{p}) + (1-y) \\log(1 - \\hat{p}) \\big)\n",
    "   $$\n",
    "\n",
    "   where $\\hat{p} = \\sigma(\\hat{y}) = \\frac{1}{1+e^{-\\hat{y}}}$.\n",
    "\n",
    "   * Penalizes wrong confident predictions heavily.\n",
    "   * Optimized with Newtonâ€™s method (second-order gradients).\n",
    "   * Default for `objective=\"binary:logistic\"`.\n",
    "\n",
    "2. **Softmax Loss (Multiclass Classification)**\n",
    "   For $K$ classes:\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{p}) = - \\sum_{k=1}^{K} \\mathbf{1}_{y=k} \\log(\\hat{p}_k)\n",
    "   $$\n",
    "\n",
    "   where\n",
    "\n",
    "   $$\n",
    "   \\hat{p}_k = \\frac{e^{\\hat{y}_k}}{\\sum_{j=1}^{K} e^{\\hat{y}_j}}\n",
    "   $$\n",
    "\n",
    "   * Standard cross-entropy loss.\n",
    "   * Used when `objective=\"multi:softprob\"` or `\"multi:softmax\"`.\n",
    "\n",
    "3. **Hinge Loss (SVM-style, optional)**\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{y}) = \\max(0, 1 - y\\hat{y})\n",
    "   $$\n",
    "\n",
    "   * Focuses on the **margin** between classes.\n",
    "   * Less probabilistic, more decision-boundary focused.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition: Why these losses?\n",
    "\n",
    "* Regression losses â†’ measure **distance** between prediction & actual values.\n",
    "* Classification losses â†’ measure **probability calibration** (confidence in correct class).\n",
    "* All are **differentiable**, allowing **gradient boosting** with 1st & 2nd order derivatives.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* **XGBRegressor** â†’ squared error, MAE, Huber, quantile.\n",
    "* **XGBClassifier** â†’ logistic loss (binary), softmax loss (multiclass), hinge loss (SVM-like).\n",
    "* Loss choice depends on whether you want robustness to outliers, probability calibration, or hard-margin separation.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e4fea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
