{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017c852d",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20cd3b",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "\n",
    "XGBoost does not have strict **statistical assumptions** like linear regression, but it inherits assumptions from **decision trees** and **boosting frameworks**. These assumptions explain when and why XGBoost works well.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Additive Model Assumption\n",
    "\n",
    "* XGBoost assumes the true function can be **approximated as a sum of weak learners (trees)**:\n",
    "\n",
    "  $$\n",
    "  \\hat{y}_i = \\sum_{m=1}^M f_m(x_i)\n",
    "  $$\n",
    "* Each new tree corrects residual errors of the previous ensemble.\n",
    "* Works best if the relationship between features and target is **non-linear and can be captured by recursive splits**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Differentiable Loss Function\n",
    "\n",
    "* XGBoost assumes the **loss function is differentiable**, so it can compute **gradients** and **Hessians** for optimization.\n",
    "* Example:\n",
    "\n",
    "  * Regression → squared error, MAE, Huber.\n",
    "  * Classification → logistic loss, cross-entropy.\n",
    "* If the loss cannot be differentiated, XGBoost cannot optimize it.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Independent and Identically Distributed (i.i.d.) Data\n",
    "\n",
    "* Assumes training examples are **independent and drawn from the same distribution** as test data.\n",
    "* Violations (like time-series without ordering, or domain shift) can reduce performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Weak Learner Assumption\n",
    "\n",
    "* Each individual tree is a **weak learner** (shallow, high-bias).\n",
    "* XGBoost assumes boosting many weak learners will create a **strong learner** (low-bias, low-variance).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. No Multicollinearity Requirement\n",
    "\n",
    "* Unlike linear regression, XGBoost does **not assume features are independent**.\n",
    "* But highly correlated features may reduce interpretability (e.g., feature importance becomes diluted).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Handling Missing Data\n",
    "\n",
    "* Assumes missing values can be assigned to a **default split direction** in trees.\n",
    "* XGBoost automatically learns the best default branch, so missingness is not problematic.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Complexity vs. Generalization\n",
    "\n",
    "* Assumes a trade-off between tree complexity and generalization.\n",
    "* That’s why regularization terms ($\\lambda, \\alpha, \\gamma$) are included in the objective function.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. Target can be modeled as additive trees.\n",
    "2. Loss function must be differentiable.\n",
    "3. Data is i.i.d.\n",
    "4. Weak learners combined form a strong model.\n",
    "5. No strict assumptions on feature independence.\n",
    "6. Missing values can be handled by default splits.\n",
    "7. Model complexity must be controlled to prevent overfitting.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
