{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4fb32b",
   "metadata": {},
   "source": [
    " ```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6e0f4",
   "metadata": {},
   "source": [
    "## Intiution\n",
    "\n",
    "### 1. Theoretical Intuition of XGBoost\n",
    "\n",
    "At its core, **XGBoost = Gradient Boosting + Regularization + System Optimizations**.\n",
    "\n",
    "* It builds trees sequentially, where each new tree corrects the **residual errors** of the previous ensemble.\n",
    "* It optimizes a **custom loss function** using **gradient descent in function space**.\n",
    "* Unlike plain Gradient Boosting, XGBoost adds **regularization**, **second-order optimization**, and **clever system-level tricks** (like sparsity awareness, cache optimization, parallelism).\n",
    "\n",
    "So, XGBoost is like a **regularized, optimized version of Gradient Boosting**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Mathematical Intuition\n",
    "\n",
    "Suppose we have a dataset:\n",
    "\n",
    "$$\n",
    "D = \\{(x_i, y_i)\\}, \\quad i=1,2,\\dots,n\n",
    "$$\n",
    "\n",
    "where $x_i \\in \\mathbb{R}^d$, and $y_i$ is the target.\n",
    "\n",
    "Our goal: learn a function $\\hat{y}_i = \\sum_{m=1}^M f_m(x_i)$ where each $f_m$ is a regression tree.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Define Objective Function\n",
    "\n",
    "The general objective is:\n",
    "\n",
    "$$\n",
    "\\text{Obj} = \\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{m=1}^M \\Omega(f_m)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $l(y_i, \\hat{y}_i)$ → differentiable loss function (MSE, log-loss, etc.)\n",
    "* $\\Omega(f)$ → regularization to control complexity:\n",
    "\n",
    "  $$\n",
    "  \\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2\n",
    "  $$\n",
    "\n",
    "  * $T$ = number of leaves in the tree\n",
    "  * $w_j$ = score (weight) of leaf $j$\n",
    "  * $\\gamma, \\lambda$ = regularization parameters\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Boosting Iteration\n",
    "\n",
    "Suppose after $t-1$ iterations, the model is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t-1)} = \\sum_{k=1}^{t-1} f_k(x_i)\n",
    "$$\n",
    "\n",
    "Now we add a new function $f_t(x)$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "The objective becomes:\n",
    "\n",
    "$$\n",
    "\\text{Obj}^{(t)} = \\sum_{i=1}^n l\\big(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)\\big) + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Taylor Expansion (2nd Order Approximation)\n",
    "\n",
    "Since $l$ can be complex, XGBoost uses **second-order Taylor expansion**:\n",
    "\n",
    "$$\n",
    "l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) \\approx l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\tfrac{1}{2} h_i f_t(x_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial \\hat{y}_i^{(t-1)}}$ (first derivative → gradient)\n",
    "* $h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{\\partial (\\hat{y}_i^{(t-1)})^2}$ (second derivative → curvature/Hessian)\n",
    "\n",
    "So, new objective becomes:\n",
    "\n",
    "$$\n",
    "\\text{Obj}^{(t)} \\approx \\sum_{i=1}^n \\Big[ g_i f_t(x_i) + \\tfrac{1}{2} h_i f_t(x_i)^2 \\Big] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Tree Structure Optimization\n",
    "\n",
    "For tree $f_t(x)$, prediction for leaf $j$ is constant $w_j$.\n",
    "If sample $i$ belongs to leaf $j$, then $f_t(x_i) = w_j$.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\text{Obj}^{(t)} = \\sum_{j=1}^T \\left[ G_j w_j + \\tfrac{1}{2}(H_j + \\lambda) w_j^2 \\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $G_j = \\sum_{i \\in I_j} g_i$ (sum of gradients in leaf $j$)\n",
    "* $H_j = \\sum_{i \\in I_j} h_i$ (sum of Hessians in leaf $j$)\n",
    "* $I_j$ = set of data points in leaf $j$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: Optimal Leaf Weight\n",
    "\n",
    "To minimize objective, take derivative wrt $w_j$:\n",
    "\n",
    "$$\n",
    "w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 6: Optimal Split (Gain)\n",
    "\n",
    "The score of a tree:\n",
    "\n",
    "$$\n",
    "\\text{Obj}^{(t)} = -\\tfrac{1}{2} \\sum_{j=1}^T \\frac{G_j^2}{H_j + \\lambda} + \\gamma T\n",
    "$$\n",
    "\n",
    "When splitting a node into left/right child, the **Gain** is:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\tfrac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L+G_R)^2}{H_L + H_R + \\lambda} \\right] - \\gamma\n",
    "$$\n",
    "\n",
    "* Higher gain → better split.\n",
    "* $\\gamma$ acts like a **minimum split gain threshold**, avoiding overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Intuition Recap\n",
    "\n",
    "1. Start with a weak model (tree).\n",
    "2. Compute gradients ($g_i$) and Hessians ($h_i$) of loss wrt predictions.\n",
    "3. Fit a tree to minimize these residuals.\n",
    "4. Update predictions with optimal leaf weights $w_j^*$.\n",
    "5. Repeat until convergence or stopping criteria.\n",
    "\n",
    "---\n",
    "\n",
    "**Why is XGBoost powerful?**\n",
    "\n",
    "* Uses **2nd order optimization (Newton step)** for faster convergence.\n",
    "* Has **built-in regularization** ($\\lambda, \\gamma$) → controls overfitting.\n",
    "* Handles **sparse data** and **missing values** gracefully.\n",
    "* Efficient (parallelism, cache awareness, pruning).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559ef1a",
   "metadata": {},
   "source": [
    "\n",
    "## When Do We Stop Splitting in XGBoost?\n",
    "\n",
    "XGBoost uses several **stopping conditions** for splitting nodes:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Maximum Depth (max\\_depth)**\n",
    "\n",
    "* Once a node reaches the maximum allowed depth, it cannot be split further.\n",
    "* Example: `max_depth=3` → tree stops growing after 3 levels.\n",
    "* Intuition: Prevents trees from growing too deep and memorizing training data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Minimum Child Weight (min\\_child\\_weight)**\n",
    "\n",
    "* A split is stopped if the **sum of instance Hessians** (approx measure of data + confidence) in a leaf is smaller than this threshold.\n",
    "* Ensures that a leaf has enough \"data weight\" to justify a split.\n",
    "* Helps avoid creating leaves with very few samples.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Minimum Loss Reduction (gamma)**\n",
    "\n",
    "* XGBoost computes the **Gain** for each potential split:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L+G_R)^2}{H_L + H_R + \\lambda}\\right] - \\gamma\n",
    "$$\n",
    "\n",
    "* If `Gain < gamma`, the split is **not performed**.\n",
    "* Intuition: If splitting doesn’t improve the objective enough, stop.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Maximum Leaves (max\\_leaves)**\n",
    "\n",
    "* If you use `max_leaves`, tree growth is stopped once the number of leaves hits this limit.\n",
    "* Useful for controlling tree size directly (instead of depth).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Sample Constraints**\n",
    "\n",
    "* If a node has too few samples, splitting stops:\n",
    "\n",
    "  * Controlled by parameters like `min_samples_split` (in sklearn API) or `min_child_weight`.\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition Summary**\n",
    "\n",
    "XGBoost stops splitting when **further splitting is not worth it**:\n",
    "\n",
    "* Tree is too deep (`max_depth`).\n",
    "* Too few samples in a node (`min_child_weight`).\n",
    "* Split doesn’t improve loss enough (`gamma`).\n",
    "* Reached max number of leaves (`max_leaves`).\n",
    "\n",
    "This balances:\n",
    "\n",
    "* **Overfitting** (tree grows too much → memorizes training set).\n",
    "* **Underfitting** (tree stops too early → misses patterns).\n",
    "\n",
    "---\n",
    "\n",
    "⚖️ So, in practice:\n",
    "\n",
    "* Use **`max_depth`** and **`gamma`** to control model complexity.\n",
    "* Use **`min_child_weight`** to avoid noisy splits.\n",
    "* Monitor **validation loss** and use **early\\_stopping\\_rounds** at the boosting level.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
