{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290d1ec3",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f445270",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "### 1. **Recursive Partitioning Can Capture the True Pattern**\n",
    "\n",
    "* The algorithm assumes the data can be separated into **subgroups** that are relatively **homogeneous** in terms of the target class.\n",
    "* Example: splitting on \"Weather = Sunny\" should meaningfully separate classes like \"Play Tennis\" vs \"Donâ€™t Play\".\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Features Have Predictive Power**\n",
    "\n",
    "* At least some features must **carry information about the target**.\n",
    "* Otherwise, splits wonâ€™t reduce impurity, and the tree wonâ€™t learn meaningful patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **No Linear/Distributional Assumptions**\n",
    "\n",
    "* Unlike regression models, decision trees **donâ€™t assume linearity** between features and target.\n",
    "* They donâ€™t assume **normality of features** or **equal variance** across classes.\n",
    "  âœ… This makes them non-parametric and flexible.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Features Are Independent for Splitting**\n",
    "\n",
    "* At each split, the algorithm treats features independently and chooses the \"best\" one.\n",
    "* It does **not assume feature independence globally**, but locally at a split it ignores feature interactions unless they show up in deeper splits.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Sufficient Data for Each Split**\n",
    "\n",
    "* Assumes thereâ€™s **enough data in each node** to compute reliable impurity measures (Gini/Entropy).\n",
    "* Small datasets can make trees unstable (high variance).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Target Variable is Well-Defined**\n",
    "\n",
    "* Assumes that the target classes are **mutually exclusive and exhaustive**.\n",
    "* Example: A loan application is either \"Approved\" or \"Rejected\", not both.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* **What Trees DONâ€™T assume:** linearity, normality, equal variance, feature scaling.\n",
    "* **What Trees DO assume:**\n",
    "\n",
    "  * Recursive partitioning can separate data meaningfully.\n",
    "  * Some features are predictive.\n",
    "  * Enough samples exist per node to make good splits.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ This low number of assumptions is why **Decision Trees work well in practice**, especially when extended into ensembles (Random Forests, Gradient Boosting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b317c05a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
