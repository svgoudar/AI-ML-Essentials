{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee0f825",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21714968",
   "metadata": {},
   "source": [
    "# Purity Measures\n",
    "\n",
    "* A node is said to be **pure** if all samples inside it belong to the same class.\n",
    "* A node is **impure** if it contains mixed classes.\n",
    "* Decision trees aim to split data so that each branch leads to nodes with higher purity.\n",
    "\n",
    "To measure purity, we use **Entropy** and **Gini Impurity**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Entropy**\n",
    "\n",
    "* Formula (for binary classification):\n",
    "\n",
    "$$\n",
    "H(S) = - p_+ \\log_2(p_+) - p_- \\log_2(p_-)\n",
    "$$\n",
    "\n",
    "* Where:\n",
    "\n",
    "  * $p_+$ = probability of positive class (say \"Yes\")\n",
    "  * $p_-$ = probability of negative class (say \"No\")\n",
    "\n",
    "* **Multi-class extension:**\n",
    "\n",
    "$$\n",
    "H(S) = - \\sum_{i=1}^k p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * $H(S) = 0$: node is pure (all one class).\n",
    "  * $H(S) = 1$: maximum impurity (equal distribution of classes, e.g. 50/50 in binary case).\n",
    "\n",
    "* **Entropy graph:**\n",
    "\n",
    "  * Ranges between **0 and 1**.\n",
    "  * Maximum impurity occurs when classes are equally likely.\n",
    "\n",
    "---\n",
    "\n",
    "## **Gini Impurity**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "$$\n",
    "Gini(S) = 1 - \\sum_{i=1}^k p_i^2\n",
    "$$\n",
    "\n",
    "* For binary case:\n",
    "\n",
    "$$\n",
    "Gini(S) = 1 - (p_+^2 + p_-^2)\n",
    "$$\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * $Gini = 0$: pure node.\n",
    "  * Maximum impurity in binary case: $Gini = 0.5$ (when $p_+ = p_- = 0.5$).\n",
    "\n",
    "* **Range:** 0 ‚Üí 0.5 (binary), slightly different for multi-class.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example:**\n",
    "\n",
    "Suppose 6 samples ‚Üí 3 \"Yes\", 3 \"No\".\n",
    "\n",
    "* **Entropy**:\n",
    "\n",
    "$$\n",
    "H = - \\frac{3}{6} \\log_2 \\frac{3}{6} - \\frac{3}{6} \\log_2 \\frac{3}{6} = 1\n",
    "$$\n",
    "\n",
    "(complete impurity)\n",
    "\n",
    "* **Gini**:\n",
    "\n",
    "$$\n",
    "1 - \\Big(\\frac{3}{6}\\Big)^2 - \\Big(\\frac{3}{6}\\Big)^2 = 1 - 0.25 - 0.25 = 0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison: Entropy vs Gini**\n",
    "\n",
    "| Aspect         | Entropy                               | Gini Impurity                        |\n",
    "| -------------- | ------------------------------------- | ------------------------------------ |\n",
    "| Formula        | $-\\sum p_i \\log_2 p_i$                | $1 - \\sum p_i^2$                     |\n",
    "| Range (binary) | 0 ‚Üí 1                                 | 0 ‚Üí 0.5                              |\n",
    "| Complexity     | Slightly more expensive (log terms)   | Faster to compute (squares only)     |\n",
    "| Behavior       | Both give similar results in practice | Sometimes Gini prefers larger splits |\n",
    "\n",
    "---\n",
    "\n",
    "## **Information Gain (IG)** ‚Äî Feature Selection Criterion\n",
    "\n",
    "Entropy & Gini tell us purity of a node. But to decide **which feature to split on**, we use **Information Gain**:\n",
    "\n",
    "$$\n",
    "IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
    "$$\n",
    "\n",
    "* $H(S)$: entropy before split\n",
    "* Weighted average entropy after split is subtracted.\n",
    "* The feature with **highest IG** is chosen to split.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "1. **Entropy** and **Gini Impurity** are purity measures.\n",
    "2. **Entropy ‚Üí range 0‚Äì1**, max impurity = 1.\n",
    "3. **Gini ‚Üí range 0‚Äì0.5** in binary classification.\n",
    "4. Both are used in practice; Gini is slightly faster.\n",
    "5. **Information Gain** helps choose which feature gives the best split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da7e5f",
   "metadata": {},
   "source": [
    "## **What is purity in decision trees?**\n",
    "\n",
    "* A node is said to be **pure** if all samples inside it belong to the same class.\n",
    "* A node is **impure** if it contains mixed classes.\n",
    "* Decision trees aim to split data so that each branch leads to nodes with higher purity.\n",
    "\n",
    "To measure purity, we use **Entropy** and **Gini Impurity**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Entropy**\n",
    "\n",
    "* Formula (for binary classification):\n",
    "\n",
    "$$\n",
    "H(S) = - p_+ \\log_2(p_+) - p_- \\log_2(p_-)\n",
    "$$\n",
    "\n",
    "* Where:\n",
    "\n",
    "  * $p_+$ = probability of positive class (say \"Yes\")\n",
    "  * $p_-$ = probability of negative class (say \"No\")\n",
    "\n",
    "* **Multi-class extension:**\n",
    "\n",
    "$$\n",
    "H(S) = - \\sum_{i=1}^k p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * $H(S) = 0$: node is pure (all one class).\n",
    "  * $H(S) = 1$: maximum impurity (equal distribution of classes, e.g. 50/50 in binary case).\n",
    "\n",
    "* **Entropy graph:**\n",
    "\n",
    "  * Ranges between **0 and 1**.\n",
    "  * Maximum impurity occurs when classes are equally likely.\n",
    "\n",
    "---\n",
    "\n",
    "## **Gini Impurity**\n",
    "\n",
    "* Formula:\n",
    "\n",
    "$$\n",
    "Gini(S) = 1 - \\sum_{i=1}^k p_i^2\n",
    "$$\n",
    "\n",
    "* For binary case:\n",
    "\n",
    "$$\n",
    "Gini(S) = 1 - (p_+^2 + p_-^2)\n",
    "$$\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * $Gini = 0$: pure node.\n",
    "  * Maximum impurity in binary case: $Gini = 0.5$ (when $p_+ = p_- = 0.5$).\n",
    "\n",
    "* **Range:** 0 ‚Üí 0.5 (binary), slightly different for multi-class.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example:**\n",
    "\n",
    "Suppose 6 samples ‚Üí 3 \"Yes\", 3 \"No\".\n",
    "\n",
    "* **Entropy**:\n",
    "\n",
    "$$\n",
    "H = - \\frac{3}{6} \\log_2 \\frac{3}{6} - \\frac{3}{6} \\log_2 \\frac{3}{6} = 1\n",
    "$$\n",
    "\n",
    "(complete impurity)\n",
    "\n",
    "* **Gini**:\n",
    "\n",
    "$$\n",
    "1 - \\Big(\\frac{3}{6}\\Big)^2 - \\Big(\\frac{3}{6}\\Big)^2 = 1 - 0.25 - 0.25 = 0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison: Entropy vs Gini**\n",
    "\n",
    "| Aspect         | Entropy                               | Gini Impurity                        |\n",
    "| -------------- | ------------------------------------- | ------------------------------------ |\n",
    "| Formula        | $-\\sum p_i \\log_2 p_i$                | $1 - \\sum p_i^2$                     |\n",
    "| Range (binary) | 0 ‚Üí 1                                 | 0 ‚Üí 0.5                              |\n",
    "| Complexity     | Slightly more expensive (log terms)   | Faster to compute (squares only)     |\n",
    "| Behavior       | Both give similar results in practice | Sometimes Gini prefers larger splits |\n",
    "\n",
    "---\n",
    "\n",
    "## **Information Gain (IG)** ‚Äî Feature Selection Criterion\n",
    "\n",
    "Entropy & Gini tell us purity of a node. But to decide **which feature to split on**, we use **Information Gain**:\n",
    "\n",
    "$$\n",
    "IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
    "$$\n",
    "\n",
    "* $H(S)$: entropy before split\n",
    "* Weighted average entropy after split is subtracted.\n",
    "* The feature with **highest IG** is chosen to split.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "1. **Entropy** and **Gini Impurity** are purity measures.\n",
    "2. **Entropy ‚Üí range 0‚Äì1**, max impurity = 1.\n",
    "3. **Gini ‚Üí range 0‚Äì0.5** in binary classification.\n",
    "4. Both are used in practice; Gini is slightly faster.\n",
    "5. **Information Gain** helps choose which feature gives the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afabaa0",
   "metadata": {},
   "source": [
    "\n",
    "## Information Gain\n",
    "\n",
    "We already know:\n",
    "\n",
    "* **Entropy** and **Gini impurity** ‚Üí measure how pure or impure a node is.\n",
    "* But a decision tree needs to **choose the best feature** to split at each step.\n",
    "  üëâ This is where **Information Gain (IG)** comes in.\n",
    "\n",
    "---\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Information Gain tells us **how much ‚Äúuncertainty‚Äù is reduced** in the target after splitting on a feature.\n",
    "\n",
    "$$\n",
    "IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\; H(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $H(S)$ = entropy of the root node (before split)\n",
    "* $S_v$ = subset of data after splitting on value $v$ of feature $A$\n",
    "* $H(S_v)$ = entropy of that subset\n",
    "* $\\frac{|S_v|}{|S|}$ = weighted contribution of that subset\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Process\n",
    "\n",
    "1. Compute entropy of the root node (**before any split**).\n",
    "2. Choose a candidate feature (say F1).\n",
    "3. Split dataset into subsets based on values of F1.\n",
    "4. Compute entropy of each subset.\n",
    "5. Take a weighted average of these entropies.\n",
    "6. Subtract this weighted entropy from the root entropy ‚Üí **Information Gain** for F1.\n",
    "7. Repeat for all features (F1, F2, F3, ‚Ä¶).\n",
    "8. Pick the feature with **highest Information Gain** ‚Üí best split.\n",
    "\n",
    "---\n",
    "\n",
    "### Worked Example\n",
    "\n",
    "Suppose dataset has 14 samples: **9 Yes, 5 No**.\n",
    "\n",
    "* **Root Entropy**:\n",
    "\n",
    "$$\n",
    "H(S) = -\\frac{9}{14}\\log_2\\frac{9}{14} - \\frac{5}{14}\\log_2\\frac{5}{14} \\approx 0.94\n",
    "$$\n",
    "\n",
    "* **Split on F1**:\n",
    "\n",
    "  * Subset C1: 6 Yes, 2 No ‚Üí $H(C1) \\approx 0.81$\n",
    "  * Subset C2: 3 Yes, 3 No ‚Üí $H(C2) = 1$\n",
    "\n",
    "  Weighted Entropy = $\\frac{8}{14}\\times0.81 + \\frac{6}{14}\\times1 \\approx 0.89$\n",
    "\n",
    "  So:\n",
    "\n",
    "$$\n",
    "IG(S, F1) = 0.94 - 0.89 = 0.049\n",
    "$$\n",
    "\n",
    "* **Split on F2**: Suppose IG(S, F2) > 0.049\n",
    "  üëâ Then F2 is chosen for the root split.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insights**\n",
    "\n",
    "* **Higher Information Gain = Better Feature for Split**.\n",
    "* Decision tree keeps splitting until:\n",
    "\n",
    "  * IG ‚âà 0 (no improvement possible), OR\n",
    "  * stopping criteria (max depth, min samples, etc.) is reached.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Entropy vs Gini**\n",
    "\n",
    "* **Entropy**:\n",
    "\n",
    "  * More theoretical, based on information theory.\n",
    "  * Can be slightly slower (logarithm calculation).\n",
    "  * Preferred if you want a ‚Äúpurer‚Äù measure of uncertainty.\n",
    "\n",
    "* **Gini Impurity**:\n",
    "\n",
    "  * Computationally faster (uses squares, no logs).\n",
    "  * Works very similarly in practice.\n",
    "  * Often the default choice (e.g., in Scikit-learn).\n",
    "\n",
    "üëâ In practice, **both give similar results**. Gini is often preferred for speed, Entropy for interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaways**\n",
    "\n",
    "1. **Entropy / Gini** = measure of node impurity.\n",
    "2. **Information Gain** = reduction in impurity ‚Üí helps decide **which feature to split on**.\n",
    "3. Decision Trees always pick the **feature with highest IG** at each step.\n",
    "4. Gini and Entropy are very similar in behavior ‚Üí choice depends on speed vs interpretability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dcebff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Entropy v/s Gini Impurity\n",
    "\n",
    "* **Entropy**\n",
    "\n",
    "  * Formula:\n",
    "\n",
    "    $$\n",
    "    H(S) = -\\sum_{i=1}^{k} p_i \\log_2 p_i\n",
    "    $$\n",
    "\n",
    "    where $k$ = number of classes, $p_i$ = probability of class $i$.\n",
    "  * Measures **disorder/uncertainty** in the dataset.\n",
    "  * More **theoretical foundation** (from information theory).\n",
    "\n",
    "* **Gini Impurity**\n",
    "\n",
    "  * Formula:\n",
    "\n",
    "    $$\n",
    "    G(S) = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "    $$\n",
    "  * Measures **how often a randomly chosen element would be misclassified** if labeled according to class distribution.\n",
    "  * More **computationally efficient** (no logarithms).\n",
    "\n",
    "---\n",
    "\n",
    "### Behavior When Classes Increase\n",
    "\n",
    "* If the number of output categories increases:\n",
    "\n",
    "  * **Entropy** expands into more terms (one for each class).\n",
    "  * **Gini Impurity** also expands, but only with squared probabilities (no logs).\n",
    "* Both handle **multi-class problems**, but entropy involves heavier computation due to logarithms.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "### Use **Entropy** when:\n",
    "\n",
    "* You want a **pure information-theoretic interpretation**.\n",
    "* Dataset is **small or moderately sized** (e.g., ‚â§ 10,000 records).\n",
    "* You want to **emphasize purity more strongly** ‚Äî entropy tends to be more sensitive to class probabilities close to 0 or 1.\n",
    "\n",
    "### Use **Gini Impurity** when:\n",
    "\n",
    "* You‚Äôre working with **large datasets** (millions of records).\n",
    "* You care about **speed and efficiency** (fewer computations since no log).\n",
    "* You‚Äôre using scikit-learn‚Äôs `DecisionTreeClassifier` ‚Äî by default it uses **Gini**.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Notes\n",
    "\n",
    "* **Performance Difference**:\n",
    "  In most real-world problems, **Gini and Entropy give very similar results** in terms of accuracy and splits.\n",
    "* **Default Choice**:\n",
    "\n",
    "  * **Gini** is used more often (fast, default in scikit-learn).\n",
    "  * **Entropy** can be used for interpretability or teaching purposes.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary (One-Liner)**\n",
    "\n",
    "* **Entropy** = more theoretical, uses log, slower, suitable for **small datasets**.\n",
    "* **Gini** = faster, simpler, default in most implementations, better for **large datasets**.\n",
    "* In practice, **both lead to similar decision trees**, so the choice is often pragmatic (speed vs. interpretability).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de601d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('|--- petal length (cm) <= 2.45\\n|   |--- class: 0\\n|--- petal length (cm) >  2.45\\n|   |--- petal width (cm) <= 1.75\\n|   |   |--- petal length (cm) <= 4.95\\n|   |   |   |--- class: 1\\n|   |   |--- petal length (cm) >  4.95\\n|   |   |   |--- class: 2\\n|   |--- petal width (cm) >  1.75\\n|   |   |--- petal length (cm) <= 4.85\\n|   |   |   |--- class: 2\\n|   |   |--- petal length (cm) >  4.85\\n|   |   |   |--- class: 2\\n',\n",
       " '|--- petal length (cm) <= 2.45\\n|   |--- class: 0\\n|--- petal length (cm) >  2.45\\n|   |--- petal width (cm) <= 1.75\\n|   |   |--- petal length (cm) <= 4.95\\n|   |   |   |--- class: 1\\n|   |   |--- petal length (cm) >  4.95\\n|   |   |   |--- class: 2\\n|   |--- petal width (cm) >  1.75\\n|   |   |--- petal length (cm) <= 4.85\\n|   |   |   |--- class: 2\\n|   |   |--- petal length (cm) >  4.85\\n|   |   |   |--- class: 2\\n')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train two decision trees: one with Gini, one with Entropy\n",
    "tree_gini = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
    "tree_entropy = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=42)\n",
    "\n",
    "tree_gini.fit(X, y)\n",
    "tree_entropy.fit(X, y)\n",
    "\n",
    "# Extract decision rules for comparison\n",
    "rules_gini = export_text(tree_gini, feature_names=iris.feature_names)\n",
    "rules_entropy = export_text(tree_entropy, feature_names=iris.feature_names)\n",
    "\n",
    "rules_gini, rules_entropy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
