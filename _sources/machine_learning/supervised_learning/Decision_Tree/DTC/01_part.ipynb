{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b166e4e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10b28f6d",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "## üå≥ What is the ‚ÄúCost‚Äù in Decision Trees?\n",
    "\n",
    "When training a decision tree, the model‚Äôs goal is to find **splits** that **minimize impurity (error)** at each node.\n",
    "So the **cost function** is what the tree tries to minimize while splitting the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Cost Functions in DTC\n",
    "\n",
    "1. **Gini Impurity (default in scikit-learn)**\n",
    "\n",
    "   * Measures how often a randomly chosen sample from the node would be misclassified.\n",
    "   * Formula:\n",
    "\n",
    "     $$\n",
    "     Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "     $$\n",
    "\n",
    "     where $p_i$ = probability of class $i$, $C$ = number of classes.\n",
    "   * Gini = 0 ‚Üí perfectly pure node (all samples same class).\n",
    "   * Gini is faster to compute (no log).\n",
    "\n",
    "---\n",
    "\n",
    "2. **Entropy (Information Gain)**\n",
    "\n",
    "   * Comes from information theory.\n",
    "   * Formula:\n",
    "\n",
    "     $$\n",
    "     Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "     $$\n",
    "   * Pure nodes ‚Üí Entropy = 0.\n",
    "   * Decision tree chooses the feature that **maximizes information gain** (reduces entropy most).\n",
    "\n",
    "---\n",
    "\n",
    "3. **Misclassification Error** (less common, mostly theoretical)\n",
    "\n",
    "   * Simply measures fraction of incorrectly classified samples at a node.\n",
    "   * Formula:\n",
    "\n",
    "     $$\n",
    "     Error = 1 - \\max(p_i)\n",
    "     $$\n",
    "   * It‚Äôs not sensitive enough for splitting (often used for pruning instead).\n",
    "\n",
    "---\n",
    "\n",
    "## The Cost Function for a Split\n",
    "\n",
    "For a split at node $t$:\n",
    "\n",
    "$$\n",
    "Cost(t) = \\sum_{k=1}^{K} \\frac{N_k}{N} \\cdot Impurity(t_k)\n",
    "$$\n",
    "\n",
    "* $N$ = total samples in parent node.\n",
    "* $N_k$ = samples in child node $k$.\n",
    "* $Impurity$ = Gini or Entropy.\n",
    "\n",
    "üëâ The best split is the one that **minimizes this weighted cost**.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "If a node has:\n",
    "\n",
    "* 8 samples ‚Üí 6 ‚ÄúYes‚Äù, 2 ‚ÄúNo‚Äù\n",
    "* Gini = $1 - [(6/8)^2 + (2/8)^2] = 0.375$\n",
    "\n",
    "If a split reduces this Gini from 0.375 to (say) 0.1, then it‚Äôs a **good split**.\n",
    "\n",
    "---\n",
    "\n",
    "**In short**\n",
    "\n",
    "* The **cost function = impurity measure** (Gini, Entropy, or Error).\n",
    "* The decision tree chooses splits that **minimize cost** (i.e., maximize purity/information gain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e84ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Criterion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Root Impurity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Child Impurities",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "bff691e4-3871-4691-ada9-72b45a1206e4",
       "rows": [
        [
         "0",
         "Gini",
         "0.46875",
         "[0.32       0.44444444]"
        ],
        [
         "1",
         "Entropy",
         "0.9544340029249649",
         "[1. 0.]"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criterion</th>\n",
       "      <th>Root Impurity</th>\n",
       "      <th>Child Impurities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gini</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>[0.31999999999999995, 0.4444444444444444]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Entropy</td>\n",
       "      <td>0.954434</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Criterion  Root Impurity                           Child Impurities\n",
       "0      Gini       0.468750  [0.31999999999999995, 0.4444444444444444]\n",
       "1   Entropy       0.954434                                 [1.0, 0.0]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Small toy dataset\n",
    "data = {\n",
    "    \"Outlook\": [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\"],\n",
    "    \"Play\":    [\"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert categorical to numerical for simplicity\n",
    "df_encoded = pd.get_dummies(df[[\"Outlook\"]])\n",
    "y = df[\"Play\"].map({\"No\":0, \"Yes\":1})\n",
    "\n",
    "# Train two trees: one with Gini, one with Entropy\n",
    "clf_gini = DecisionTreeClassifier(criterion=\"gini\", max_depth=1, random_state=42)\n",
    "clf_entropy = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1, random_state=42)\n",
    "\n",
    "clf_gini.fit(df_encoded, y)\n",
    "clf_entropy.fit(df_encoded, y)\n",
    "\n",
    "# Extract impurity values from root node (before split) and child nodes (after split)\n",
    "root_gini = clf_gini.tree_.impurity[0]\n",
    "child_ginis = clf_gini.tree_.impurity[1:3]\n",
    "\n",
    "root_entropy = clf_entropy.tree_.impurity[0]\n",
    "child_entropies = clf_entropy.tree_.impurity[1:3]\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Criterion\": [\"Gini\", \"Entropy\"],\n",
    "    \"Root Impurity\": [root_gini, root_entropy],\n",
    "    \"Child Impurities\": [child_ginis, child_entropies]\n",
    "})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990df144",
   "metadata": {},
   "source": [
    "### Results on the toy dataset (Outlook ‚Üí Play)\n",
    "\n",
    "| Criterion   | Root Impurity | Child Impurities |\n",
    "| ----------- | ------------- | ---------------- |\n",
    "| **Gini**    | 0.469         | \\[0.320, 0.444]  |\n",
    "| **Entropy** | 0.954         | \\[1.0, 0.0]      |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "* **Root Node (before splitting):**\n",
    "\n",
    "  * Gini impurity = 0.469\n",
    "  * Entropy = 0.954 (higher because log-based measure is more sensitive).\n",
    "\n",
    "* **After Split (Child Nodes):**\n",
    "\n",
    "  * Using **Gini**, both children still have some impurity (0.32 and 0.44).\n",
    "  * Using **Entropy**, one child became perfectly pure (0.0), the other is fully impure (1.0).\n",
    "\n",
    "üëâ The tree compares these values, computing the **weighted average impurity**.\n",
    "\n",
    "* The split that **reduces impurity the most** (maximizes Information Gain or minimizes Gini cost) is chosen.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
