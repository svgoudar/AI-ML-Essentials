{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fd71c0",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "\n",
    "* A **lazy learning**, **non-parametric** algorithm.\n",
    "* Can solve **classification** ✅ and **regression** ✅ problems.\n",
    "* Makes predictions based on the **“closeness” (distance)** of data points.\n",
    "* No explicit training (model just stores data). Prediction happens at test time.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow of KNN\n",
    "\n",
    "1. **Choose K (the number of neighbors)**\n",
    "\n",
    "   * Example: k=3 → look at the 3 closest points to the test instance.\n",
    "   * K is a **hyperparameter** → chosen via cross-validation.\n",
    "\n",
    "2. **Calculate Distance**\n",
    "\n",
    "   * To determine which points are closest to the test instance.\n",
    "   * Common distance metrics:\n",
    "\n",
    "     * **Euclidean Distance** (straight-line distance):\n",
    "\n",
    "       $$\n",
    "       d(p, q) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
    "       $$\n",
    "     * **Manhattan Distance** (block or city-block distance):\n",
    "\n",
    "       $$\n",
    "       d(p, q) = |x_2 - x_1| + |y_2 - y_1|\n",
    "       $$\n",
    "\n",
    "3. **Find the K Nearest Neighbors**\n",
    "\n",
    "   * Select the K points in the training data that are closest to the test point.\n",
    "\n",
    "4. **Vote (Classification) or Average (Regression)**\n",
    "\n",
    "   * **Classification**: majority vote among the neighbors’ labels.\n",
    "   * **Regression**: take the **average (or median)** of neighbors’ values.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Intuition\n",
    "\n",
    "* Dataset: features F1, F2, output Y (binary 0/1).\n",
    "* New test point comes in → find the 5 nearest points.\n",
    "* If **3 belong to class 1** and **2 belong to class 0**, predicted class = **1**.\n",
    "\n",
    "For regression:\n",
    "\n",
    "* Predict house price → take the **average price of 5 nearest houses**.\n",
    "\n",
    "---\n",
    "\n",
    "## Variants for Speed (because KNN is slow)\n",
    "\n",
    "* **Naive KNN complexity** = $O(n)$ per query (must compute distance to every point).\n",
    "* Optimizations:\n",
    "\n",
    "  * **k-d Tree** → partitions space into a binary tree for faster lookup.\n",
    "  * **Ball Tree** → groups points into hyperspheres.\n",
    "* These reduce the number of distance computations.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insights**\n",
    "\n",
    "* **K choice matters**:\n",
    "\n",
    "  * Low K (e.g., 1) → very sensitive to noise (high variance).\n",
    "  * High K (e.g., 20) → oversmoothing, may miss local patterns (high bias).\n",
    "\n",
    "* **Distance metric matters**:\n",
    "\n",
    "  * Euclidean for continuous features.\n",
    "  * Manhattan if data has grid-like structure.\n",
    "  * Can even use cosine similarity for text.\n",
    "\n",
    "* **Feature scaling is crucial**:\n",
    "\n",
    "  * Since distances are sensitive to scale, normalize/standardize features first.\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Intuition (Good to Show Students)\n",
    "\n",
    "1. **Classification**: draw two sets of points (red & blue). Add a new test point, show how its nearest neighbors decide its label.\n",
    "2. **Regression**: plot house size vs price, show how nearest neighbors’ average gives prediction.\n",
    "3. **Distance metrics**: show Euclidean (diagonal straight line) vs Manhattan (L-shaped path).\n",
    "4. **Effect of K**: with K=1 → decision boundary jagged; with K=15 → smooth.\n",
    "\n",
    "---\n",
    "\n",
    "**In Short**\n",
    "\n",
    "* KNN = predict based on **neighbors**.\n",
    "* Works for classification and regression.\n",
    "* Needs careful choice of **K** and **distance metric**.\n",
    "* **Pros**: simple, interpretable, no training time.\n",
    "* **Cons**: slow with large datasets, sensitive to irrelevant features, requires feature scaling.\n",
    "* **Optimized by**: k-d trees, ball trees, approximate nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052fe52e",
   "metadata": {},
   "source": [
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
