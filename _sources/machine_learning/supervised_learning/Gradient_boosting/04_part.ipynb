{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56dbd347",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1172ee5a",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier\n",
    "\n",
    "**Gradient Boosting Classifier (GBC)** is the classification version of Gradient Boosting. It builds an ensemble of weak learners (usually decision trees) in a **stage-wise fashion**, where each learner corrects the mistakes of the previous one.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Objective\n",
    "\n",
    "* Given training data $(x_i, y_i)$ with $y_i \\in \\{0,1\\}$ or $\\{-1,+1\\}$, the goal is to minimize a **classification loss**.\n",
    "* Common choice: **Logistic loss**\n",
    "\n",
    "$$\n",
    "L(y, F(x)) = \\log\\big(1 + e^{-y F(x)}\\big), \\quad y \\in \\{-1,+1\\}\n",
    "$$\n",
    "\n",
    "where $F(x)$ is the additive model.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Initialization\n",
    "\n",
    "* Start with a constant model:\n",
    "\n",
    "$$\n",
    "F_0(x) = \\frac{1}{2} \\ln \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "where $p$ is the proportion of positive samples.\n",
    "\n",
    "* This is the **log-odds of the positive class**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Iterative boosting process\n",
    "\n",
    "At each iteration $m$:\n",
    "\n",
    "#### a) Compute pseudo-residuals\n",
    "\n",
    "* Derivative of logistic loss wrt predictions:\n",
    "\n",
    "$$\n",
    "r_{im} = y_i - p_{i}^{(m-1)}\n",
    "$$\n",
    "\n",
    "where $p_i^{(m-1)} = \\frac{1}{1+e^{-F_{m-1}(x_i)}}$ is the predicted probability.\n",
    "\n",
    "* Intuition: residuals = *true label â€“ predicted probability*.\n",
    "\n",
    "---\n",
    "\n",
    "#### b) Fit weak learner\n",
    "\n",
    "* Train a small decision tree $h_m(x)$ on pseudo-residuals.\n",
    "* The tree tries to capture patterns in the misclassified points.\n",
    "\n",
    "---\n",
    "\n",
    "#### c) Compute multiplier\n",
    "\n",
    "* Find best $\\gamma_m$ (step size) via line search:\n",
    "\n",
    "$$\n",
    "\\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^n L\\big(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### d) Update model\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\nu \\cdot \\gamma_m h_m(x)\n",
    "$$\n",
    "\n",
    "* $\\nu$ = learning rate (shrinkage).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Final prediction\n",
    "\n",
    "* After $M$ rounds, we have:\n",
    "\n",
    "$$\n",
    "F_M(x) = F_0(x) + \\nu \\sum_{m=1}^M \\gamma_m h_m(x)\n",
    "$$\n",
    "\n",
    "* Convert to probability with sigmoid:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{1 + e^{-F_M(x)}}\n",
    "$$\n",
    "\n",
    "* Predict class:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases}1 & p(x) \\geq 0.5 \\\\ 0 & p(x) < 0.5\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Intuition\n",
    "\n",
    "* Each tree is trained on the **errors of the previous ensemble**.\n",
    "* Predictions are updated in small steps (learning rate).\n",
    "* Over many iterations, the model improves classification boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Features**\n",
    "\n",
    "* Handles binary and multiclass classification (one-vs-rest or multinomial loss).\n",
    "* Can use different loss functions (log-loss, exponential loss, deviance).\n",
    "* Sensitive to learning rate and number of trees (requires tuning).\n",
    "* More robust than AdaBoost because it uses gradient descent rather than weighting errors exponentially.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738b1409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Data\n",
    "X, y = make_classification(n_samples=200, n_features=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gbc.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
