{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebaea4c3",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# Intiution\n",
    "\n",
    "Hierarchical clustering is about **grouping similar points into clusters in a nested, tree-like structure**.\n",
    "\n",
    "* Instead of pre-defining the number of clusters (like K-Means), HC **creates a hierarchy of clusters**.\n",
    "* The hierarchy is often visualized as a **dendrogram**, which shows how clusters merge (or split) step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Intuition: Agglomerative (Bottom-Up) HC**\n",
    "\n",
    "1. **Start with each point as its own cluster**\n",
    "\n",
    "   * Imagine every data point is a **leaf on a tree**.\n",
    "\n",
    "2. **Merge the closest clusters** iteratively\n",
    "\n",
    "   * “Closest” is determined by a **distance metric** (Euclidean, Manhattan, etc.) and **linkage method** (single, complete, average, Ward).\n",
    "   * Merge these points to form a **branch of the tree**.\n",
    "\n",
    "3. **Repeat until all points are merged into one cluster**\n",
    "\n",
    "   * The final cluster is the **root of the tree**.\n",
    "\n",
    "4. **Dendrogram shows the process**\n",
    "\n",
    "   * Height of a merge represents the distance between clusters.\n",
    "   * Cutting the dendrogram at a certain height gives a specific number of clusters.\n",
    "\n",
    "**Analogy:**\n",
    "\n",
    "* Imagine clustering friends based on how close they are:\n",
    "\n",
    "  * First, best friends stick together (small clusters).\n",
    "  * Then, groups of friends merge into larger social circles.\n",
    "  * Finally, everyone forms one big network.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Intuition: Divisive (Top-Down) HC**\n",
    "\n",
    "1. Start with **all points in one cluster**.\n",
    "2. Recursively **split clusters** based on distance or variance.\n",
    "3. Continue splitting until each point is its own cluster.\n",
    "\n",
    "> This is less common in practice because it is computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Key Points in HC Intuition**\n",
    "\n",
    "| Concept                | Intuition                                                                       |\n",
    "| ---------------------- | ------------------------------------------------------------------------------- |\n",
    "| Distance metric        | Measures “closeness” between points or clusters                                 |\n",
    "| Linkage method         | Decides how clusters are merged (min distance, max distance, average, variance) |\n",
    "| Dendrogram             | Tree showing the merging/splitting process                                      |\n",
    "| Cutting the dendrogram | Choosing the number of clusters visually based on desired similarity            |\n",
    "| Nested structure       | HC naturally captures sub-clusters within larger clusters                       |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Visual Example (Conceptual)**\n",
    "\n",
    "* Imagine 10 points on a line:\n",
    "\n",
    "```\n",
    "Points:  A   B   C       D   E   F       G   H   I   J\n",
    "```\n",
    "\n",
    "* Agglomerative HC merges closest points:\n",
    "\n",
    "  1. Merge A & B, D & E, G & H … → small clusters\n",
    "  2. Merge clusters based on distance → bigger clusters\n",
    "  3. Merge all → root cluster\n",
    "\n",
    "* Dendrogram height shows **distance at which clusters merge**.\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway**\n",
    "\n",
    "Hierarchical clustering is intuitive because it’s like **building a tree of relationships**:\n",
    "\n",
    "* Closest points merge first → small branches\n",
    "* Similar branches merge → larger branches\n",
    "* Final root contains all points\n",
    "\n",
    "It gives a **visual, interpretable view of cluster structure**, especially useful for exploring nested relationships.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
