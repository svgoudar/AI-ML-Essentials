{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737c145c",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intiution\n",
    "\n",
    "## **1. Core Idea**\n",
    "\n",
    "t-SNE is all about **preserving local neighborhoods**.\n",
    "\n",
    "* Imagine you have a high-dimensional dataset.\n",
    "* You want a 2D or 3D plot that reflects **how points relate to each other locally**.\n",
    "* t-SNE does this by modeling the **probability that two points are neighbors** and trying to preserve that in the low-dimensional embedding.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Step-by-Step Intuition**\n",
    "\n",
    "### **Step 1: Compute Similarities in High-Dimensional Space**\n",
    "\n",
    "* For each point, t-SNE computes how similar it is to every other point using **conditional probabilities**:\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\n",
    "$$\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * If two points are close in high-D space → probability $p_{ij}$ is high.\n",
    "  * If far apart → $p_{ij}$ is low.\n",
    "* This captures **local neighborhood structure**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Map to Low-Dimensional Space**\n",
    "\n",
    "* t-SNE places points in 2D/3D randomly at first.\n",
    "* Then, it defines **low-dimensional similarities** using a **heavy-tailed Student-t distribution**:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n",
    "$$\n",
    "\n",
    "* **Why t-distribution?**\n",
    "\n",
    "  * Avoids “crowding problem.”\n",
    "  * Faraway points in high-D space can be pushed apart in 2D.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Minimize Difference Between High-D and Low-D**\n",
    "\n",
    "* t-SNE minimizes the **Kullback-Leibler (KL) divergence** between high-D and low-D similarities:\n",
    "\n",
    "$$\n",
    "\\text{KL}(P || Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * Low KL → points that were close remain close.\n",
    "  * Points that were distant in high-D space may be pushed farther apart in 2D.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Visualization Analogy**\n",
    "\n",
    "* Imagine **a sheet of paper representing low-D space**.\n",
    "* High-dimensional points are **connected by springs** to neighbors with strengths proportional to similarity.\n",
    "* t-SNE moves the points around so that the **spring system is relaxed**, preserving local neighborhoods while allowing distant points to spread out.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Key Takeaways**\n",
    "\n",
    "| Concept                   | Intuition                                              |\n",
    "| ------------------------- | ------------------------------------------------------ |\n",
    "| High-D similarity         | “Which points are neighbors?”                          |\n",
    "| Low-D similarity          | “Place neighbors close, others far”                    |\n",
    "| KL divergence             | “Minimize mismatch between high-D and low-D neighbors” |\n",
    "| Heavy-tailed distribution | “Prevent crowding, let distant points stretch”         |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* t-SNE **does not preserve global distances**.\n",
    "* It **highlights clusters** and local relationships.\n",
    "* Best used for **visualizing patterns** rather than downstream modeling.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
