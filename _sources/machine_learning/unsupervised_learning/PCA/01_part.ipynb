{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3bd059",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa4b5670",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "### **1. Linearity**\n",
    "\n",
    "* **Assumption:** PCA assumes that the relationships between variables are **linear**.\n",
    "* **Implication:** PCA finds a linear combination of features that maximizes variance. Non-linear relationships may not be captured effectively.\n",
    "* **Example:** If your data lies on a curved manifold (like a circle), PCA will try to fit straight axes and fail to capture the true structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Large Variance = Important Structure**\n",
    "\n",
    "* **Assumption:** Features with larger variance are more “informative”.\n",
    "* **Implication:** PCA identifies directions (principal components) that **maximize variance**. It assumes that the most important underlying structure corresponds to directions of high variance.\n",
    "* **Caveat:** Noisy features with high variance might be misleadingly considered important.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Mean-Centered Data**\n",
    "\n",
    "* **Assumption:** PCA assumes the data is **centered around zero**.\n",
    "* **Implication:** Before computing the covariance matrix, you subtract the mean of each feature. If you skip this step, the first principal component might just point to the mean instead of the direction of maximal variance.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Orthogonality of Principal Components**\n",
    "\n",
    "* **Assumption:** The principal components are **uncorrelated (orthogonal)**.\n",
    "* **Implication:** Each component captures a new direction of variance not explained by previous components. PCA cannot capture correlated patterns along non-orthogonal directions beyond linear correlation.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Scale Matters**\n",
    "\n",
    "* **Assumption:** The scale of variables affects PCA.\n",
    "* **Implication:** PCA is sensitive to the relative scaling of features. Standardizing features (e.g., z-score normalization) is usually recommended, especially if features have different units or ranges.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Noise is Homoscedastic (Optional)**\n",
    "\n",
    "* **Assumption (ideal case):** The noise in the data is isotropic (same in all directions).\n",
    "* **Implication:** PCA works best when the variance due to noise is roughly the same across all dimensions. If one feature has very high noise variance, PCA might consider it “important” incorrectly.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| Assumption                     | Why It Matters                                    |\n",
    "| ------------------------------ | ------------------------------------------------- |\n",
    "| Linearity                      | PCA captures linear patterns only                 |\n",
    "| Large variance = important     | High-variance directions are assumed informative  |\n",
    "| Mean-centered data             | Ensures components point along variance, not mean |\n",
    "| Orthogonality of components    | Each PC captures unique, uncorrelated variance    |\n",
    "| Scale sensitivity              | Features need normalization if scales differ      |\n",
    "| Homoscedastic noise (optional) | Unequal noise can distort principal components    |\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition:** \n",
    "PCA is essentially like finding the “best-fit axes” through your data cloud. The assumptions above are what make that “best-fit” meaningful. If your data violates these, PCA may give misleading directions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
